## GAN
### 思想  
  Gan可以学习到loss。实际上本质上判别网络做的事情就是loss做的事情。因为一些情况，我们设置的loss并不是最贴切的情况，仍需要投入大量人力来设计有效的loss函数，所以希望网络自己取寻找最好的损失。
###  局限性  
 - 没有用户控制能力    
   - cGAN 和 pix2pix 通过添加条件，保证输入和输出有一定的对应关系，加强了用户控制能力。

 - 低分辨率（Low resolution）和低质量（Low quality）问题    
   - pix2pixHD解决图像质量问题
	   GAN生成的图片看起来很不错，但如果你放大看，就会发现**细节相当模糊**。

- 原始GAN形式训练中的问题   
  - **判别器越好，生成器梯度消失越严重**  
  - **最小化第二种生成器loss函数，会等价于最小化一个不合理的距离衡量，导致两个问题，一是梯度不稳定，二是collapse mode即多样性不足**  
    
     第二种生成器损失指的是$E_{x \sim g}[-log(D(x))]$  ,其中g为生成器分布
 
  - **小结**：在原始GAN的（近似）最优判别器下，第一种生成器loss面临梯度消失问题，第二种生成器loss面临优化目标荒谬、梯度不稳定、对多样性与准确性惩罚不平衡导致mode collapse这几个问题。
### 损失  
  - 第一种  
	   - 判别器损失：  $E_{x \sim P_{r}}log(D(x))+E_{x \sim P_{g}}log(1-D(x)$  
	   - 生成器损失： $E_{x \sim P_{g}}log(D(x))$
 - 第二种   
     
## WGAN  
- **原始wgan网络**     
   - **改进条件**  
     1. 判别器最后一层去掉sigmoid  
     2. 生成器和判别器的loss不取log  
     3. 每次更新判别器的参数之后把它们的绝对值截断到不超过一个固定常数c  
     4. 不要用基于动量的优化算法（包括momentum和Adam），推荐RMSProp，SGD也行
   - **损失**  
     $L_D=E_{x \sim P_g}[D(x)]-E_{x \sim P_r}[D(x)]$    
     $L_G = -E_{x \sim P_g}[D(x)]$  
     $W_D\leftarrow \text {clip-by-value}(W_D,-0.01,0.01)$   
    - **理论**  
         - 在理论分析中D(x)是一个Lipschitz连续（导数存在上界）的函数，这个函数是通过神经网络，通过改变权重w，学习这个函数的近似。  
         -  对于以前的分类问题，判别器最后用的是sigmoid激活，但是现在变成回归问题去拟合Wasserstein距离,所以要取走sigmoid和log

**在原始论文中，作者是通过Weight clipping,也就是把网络权重限制在一定范围内，去限制导数的上界。但是这个在后面的改进的论文也被指出，这样做的结果会使权重的分布集中在范围的左右两个端点处。因此，后来提出的改进方案gradient penalty，就是用来替代这种方法，这种网络被称为WGAN-GP**  
- WGAN-GP   
  - **gradient penalty思想**  
    Gradient penalty的思想其实很简单：在Loss function中加入一个梯度的正则化项，惩罚导数值大于上界的权重。
  - **loss**  
    而为了让样本更有多样性，让梯度尽量的大，惩罚项可改为让其固定在上界值(Lipschitz常数)K附近，最后得到的Loss function如下（这里把K值简单地设定成了1）：    
    $$L_D=E_{x \sim P_g}[D(x)]-E_{x \sim P_r}[D(x)]+\lambda E_{x \sim P_{\hat x}}[||\nabla_xD(x)||_p-1]^2$$     
    - 惩罚项本来应该是在整个样本空间内采样，但是这在很多问题中不现实，于是作者就采用了一点技巧，改成在训练数据和生成数据附近的范围采样,即分别在真实数据r和生成数据g中采样，并加入一个随机噪声：  
      $x_r \sim P_r,x_g \sim P_g,\epsilon \sim Uniform[0,1]$    
      $\hat x = \epsilon x_r +(1-\epsilon)x_g$  
      这便是惩罚空间，即上式loss中的$P_{\hat x}$中的$\hat x$
    
      
    
    
   将损失问题优化为$D(G(X))-D(Y)$问题，并对真实样本和虚假样本进行加权抽样，进行梯度loss计算，从而限制。  
- **wgan-gp loss tensorflow实现**  
   ```
   epsilon = tf.random_uniform(shape = [self.batch_size, 1, 1, 1], minval = 0.0, maxval = 1.0)    
   interpolated_input = epsilon * label + (1 - epsilon) *self.gene_img    
   gradient = tf.gradients(self.discriminator(interpolated_input, reuse = True), [interpolated_input])[0]   
   GP_loss = tf.reduce_mean(tf.square(tf.sqrt(tf.reduce_mean(tf.square(gradient), axis = [1, 2, 3])) - 1))  
   d_loss_real = - tf.reduce_mean(self.real_prob)  
   d_loss_fake = tf.reduce_mean(self.fake_prob)  
   self.D_loss = d_loss_real + d_loss_fake + 10.0 * GP_loss
   ```  

## CGAN  
- 原因  
 GAN这种不需要预先建模的方法缺点是太过自由了，对于较大的图片，较多的 pixel的情形，基于简单 GAN 的方式就不太可控了。为了解决GAN太过自由这个问题，一个很自然的想法是给GAN加一些约束   
### 主要内容  
  将原始GAN中的概率全改成了条件概率，  
  $\displaystyle\min_G \max _{D} \left \{  E_{x \sim P_{r}}logD(x|c)+E_{x \sim P_{g}}log(1-D(x|c))\right \}$   
   
  这个条件可以是图片，标注等等，结构如下图：  
  
  ![enter image description here](https://lh3.googleusercontent.com/r6xOlLOR4SfCLUS4nvCX_vAtKdEI_FqrKvvW03C7-YSc1SGErEVjjqrGMJv6mw3lvRcbFChronQ)
  
  - 缺点  
    网络容易忽略输入，直接将条件作为唯一输入。 
## patch GAN(pix2pix)  
   用cGAN做image translation     
   基于条件gan   
###  **方法**    
   该方法使用的是**配对数据**。
  判别器判别时，除了输入生成器输出外，**加入条件**输入(这篇文章是判别器输入加入生成器输入)，对判别器进行一个约束，从而达到对生成器的约束。使**得生成器生成的结果与输入有一定的对应性**。提高了用户控制能力。  
   其中z为输入，x为条件输入。
   $L_{cGAN}(G,D)=E_{x,y}[logD(x,y)]+E_{x,z}[log(1−D(x,G(x,z)))]$
    
### 网络结构  
网络结构如下图  

![enter image description here](https://lh3.googleusercontent.com/cmG00Mtbg11mrMbGwwlKGuv6HOcp5kNCfFHEM-OG7r1WGnao62XQSW2bwr5W6rBToOhU_YmIVTU)

 - **生成器结构**  
   使用Unet作为生成器  
   - **使用Unet原因**  
     输入和输出图像的外表面(surface appearance)应该不同而潜在的结构(underlying structure)应该相似，对于image translation的任务来说，输入和输出应该共享一些底层的信息，因此使用Unet这种跳层连接(skip connection)的方法，较好的保留的边界，角点等高频的内容，U-Net对提升细节的效果非常明显- **损失**  
   用L1距离而不是L2距离，因为L1鼓励更少的模糊

   - **原因**  
     用L1和L2 loss重建的图像很模糊，也就是说L1和L2并不能很好的恢复图像的高频部分(图像中的边缘等)，但能较好地恢复图像的低频部分(图像中的色块)。为了能更好得对图像的局部做判断，作者提出patchGAN的结构，也就是说把图像等分成patch，分别判断每个Patch的真假，最后再取平均！文章提出的这个PatchGAN可以看成所以另一种形式的纹理损失或样式损失  
 - **判别器结构**  
   判别器结构使用全卷积网络，运用了patch的思想。  
   - **patch gan 内容**  
     **用来判别感受野是N×N的局部patch是真是假**。这个感受野举个例子，网络1x1大小的输出，他的感受野表示的是原图，含义是原图是真是假，这就是常规的判别器所用的。本文用的30x30大小的输出，他的每个元素感受野是70x70的patch,含义是原图中30个70x70的patch是真是假。最后一层每个像素过sigmoid输出为真的概率，然后用BCEloss计算得到最终loss。
   - **好处**  
       这样做的好处是因为输入的维度大大降低，所以参数量少，运算速度也比直接输入一张快，并且可以计算任意大小的图。一些研究表明对于要求高分辨率、高清细节的图像领域中，普通GAN判别器并不适合，由此引入了PatchGAN，它的感受域对于与输入中的一小块区域，也就是说， 对应了判别器对输入图像的一小块的判别输出，这样训练使模型更能关注图像细节。  
   - **效果对比**   
      论文对比了不同大小patch的结果，对于256x256的输入，patch大小在70x70的时候，从视觉上看结果就和直接把整张图片作为判别器输入没有多大区别了，但70x70速度更快。
    
### **损失**  
   用L1距离而不是L2距离，因为L1鼓励更少的模糊
   。  
      该网络对比了L1,GANloss,L1+GANloss损失效果，L1+GANloss效果最好。    判别器损失为交叉熵。  
   - **object function为**：  
     
      $L_{cGAN}(G,D)=E_{x,y}[logD(x,y)]+E_{x,z}[log(1−D(x,G(x,z)))]$    
      
      加上一个与ground truth的L1 loss:    
      $L_{L1}(G)=E_{x,y,z}[||y-G(x,z)||_1]$
    - **小结**  
      GAN其实是一种相对于L1 loss更好的判别准则或者loss。有时候单独使用GAN loss效果好，有时候与L1 loss配合起来效果好。在pix2pix中，作者就是把L1 loss 和GAN loss相结合使用，因为作者认为L1 loss 可以恢复图像的低频部分，由于L1强制低频的正确性，从而促使GAN loss只关注高频部分，更准确的恢复图像的高频部分。对于模拟高频，只要将注意力限制在局部图像块中结构就可以了。因此，文章设计了一个判别器结构—PatchGAN。  
      只用L1 会模糊，只用GAN会shape，两个加起来正好
## cycleGAN  
   该网络针对非成对数据。   
   直接使用不成对的数据是不奏效的。网络会直接**忽略输入，随机产生输出**！所以，我们还得对网络增加**限制（constraint）**才行。所以该文章思想
## pix2pixHD  
   为了区分高分辨率真实图像和合成的图像，判别器需要有一个非常大的感受野。（为什么区分高分辨真实和合成需要很大的感受野）  
### **主要内容**  
   在pix2pix的基础上，使用了由粗到细的生成器，多尺度的判别器，一个鲁棒的对抗性学习目标函数。
   - 由粗到细的生成器   
     将生成器分为两部分：G1和G2，将G1称为全局生成网络，G2为局部增强网络，如下图。为了生成更高分辨率输出，可以额外自己增加局部增强网络。   
     ![enter image description here](https://lh3.googleusercontent.com/dCWMoSbvfY9Q6K4ARoAHfyIzdob1F-E3X2Awt-edoOb_Srn2gsm4JzL1GQjIo4RcQgXSZ_uMbYxS)    
     首先训练全局生成器，之后按分辨率顺序训练局部增强网络。再一起微调所有的网络。这样有效的聚合了全局和局部信息。
   - 
## DeblurGAN  
- **本文贡献**  
  - 提出了损失和架构  
  - 提出了一种生成运动模糊数据的方法，并证明将其与现有的数据相结合可以获得更好的效果  
  - 提出了一种评估性能的方法
- **背景知识**  
  模糊问题分为两类，盲模糊和非盲模糊。非盲模糊为模糊核是已知的，盲模糊的模糊核是未知的。  
  
- **损失**  
  对抗损失和MSE感知损失，MSE感知损失关注一般的内容，而对抗损失关注恢复纹理信息。    
  生成器损失为：VGG的MSE感知损失，判别器GAN损失  
  判别器损失为：WGAN损失(patch GAN判别器)
- **网络结构**  
	- 生成器结构 
	  包含一个卷积核大小为7的卷积层，两个步长为2的卷积块，9个残差块和两个步长为2的反卷积块。每个残差块包含卷积层和IN,ReLU。每个残差块中第一个卷积层后面设置0.5的dropout。  
	  ![](https://github.com/sfxz035/DL-Learning/raw/master/picture/DeburGAN.png)    
	- 判别器结构  
	  运用patch gan的判别器结构 
- 训练细节  
训练数据是裁剪的256x256的1000张图片。  
在$D_θ$上执行了5次梯度下降，在$G_θ$上执行了1次    
- 相关特点  
  GAN能够保留图像中丰富的纹理结构、创造出和真实图像十分相近的图像。故文章选用了GAN网络形式。
 - 问题  
   文章说此网络为条件GAN，条件体现在哪里？体现label上？


<!--stackedit_data:
eyJoaXN0b3J5IjpbLTE1OTk1ODk0OTEsLTE4NDcyNjI1NTYsNz
YxMjI5MzE5LC0xNTgxMzc0NjIyLDg3ODIyNzYwMywtNzYxMTU1
NTYzLDU3NTQwMzk3LDExMDIxNjYxOSwxNjUyODY5NTUsLTE4Mj
IxMTcyNjEsMTM1NDM4MDkxNiwtNTUyMTc1Mzg3LDY0MzAxMTA4
MSwyMTMwNDk2ODQ0LC0yMTQ0NzQ0MDAwLDg5NzM0NDc3NywtMj
Y5ODY0MDIyLDY4NTc0ODc4OSwxMDk2ODM2MzQwLDcwNDUxMDcx
Ml19
-->