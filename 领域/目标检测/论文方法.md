### RCNN   
- 流程简介   
  - 给定一张输入图片，从图片中提取 2000 个类别独立的候选区域   
  - 对于每个区域利用 CNN 抽取一个固定长度的特征向量。   
  - 再对每个区域利用 SVM 进行目标分类。    
   ![enter image description here](https://lh3.googleusercontent.com/qt3IvSYPhKZl7WWq9YMIFu43AMfocxOh8D6lkv_Q9u9HT9fd-CopnhkN7Qvtu-LRHEcFxrBXFSxc)
- 提取候选区域的方法   
  -   objectness  
  - selective search
  - category-independen object proposals
  - constrained parametric min-cuts(CPMC)
  -   multi-scale combinatorial grouping
  -   Ciresan   
  RCNN使用的使selective search   
- 特征提取   
  R-CNN 抽取了一个 4096 维的特征向量，采用的是 Alexnet。   
  Alexnet的输入使227x227，RCNN做法是直接将候选区域大小变换到227x227输入227x227。  
- SVM分类   
  对CNN输出的特征向量进行SVM分类，并产生分数。   
- 训练阶段   
  利用迁移学习，提取在 ILSVRC 2012 的模型和权重，然后在 VOC 上进行 fine-tune。解决VOC标注数据不足的问题
- 测试阶段   
   2000个候选区域，有分别对应的类别分数。针对同一类进行非极大性抑制(NMS)，避免重复。   
- bbox 回归   
  bbox 的值其实就是物体方框的位置，预测它就是回归问题，而不是分类问题。   
  
  受 DPM 的启发，作者训练了一个线性的回归模型，这个模型能够针对候选区域的 pool5 数据预测一个新的 box 位置。具体细节，作者放在补充材料当中

### sppnet   
针对RCNN，sppnet进行了改进。   
sppnet总体流程还是 Selective Search得到候选区域->CNN提取ROI特征->类别判断->位置精修。   
- RCNN与sppnet的总体流程相同，不同之处是：
	- RCNN进行候选区域挑选，再从原图中将候选区域剪切下来，分别送进CNN进行特征向量计算(2000个候选区进行2000次卷积网络的操作)。
	- SPPnet进行候选区域挑选，然后将整张图送进CNN，在feature map上针对候选区域位置提取候选区域的jian
- spp层去掉了原始图像上的crop/warp等操作。  
   主要原因是CNN的全连接层要求输入图片是大小一致的，而实际中的输入图片往往大小不一，如果直接缩放到同一尺寸，很可能有的物体会充满整个图片，而有的物体可能只能占到图片的一角。传统的解决方案是进行不同位置的裁剪，但是这些裁剪技术都可能会导致一些问题出现，比如图7中的crop会导致物体不全，warp导致物体被拉伸后形变严重，SPP就是为了解决这种问题的。  
-   **空间金字塔池化**
   ![enter image description here](https://lh3.googleusercontent.com/tI_Y8__AYZB7r_1g1nstk0WIlctLNjJAQrdQtYnRdOtkUwKGk5hgwE5EHY02D4aQyyQc4fx7zHoK)     
   蓝色的图1——我们把一张完整的图片，分成了16个块，也就是每个块的大小就是(w/4,h/4);     
   
	绿色的图2，划分了4个块，每个块的大小就是(w/2,h/2);    
   
    黑色的图3，把整张图片作为了一个块，也就是块的大小为(w,h)       
    
    空间金字塔最大池化的过程，其实就是从这21个图片块中，分别计算每个块的**最大值**（局部max-pooling）。通过SPP，我们就把一张任意大小的图片转换成了一个**固定大小的21维特征**
### Fast RCNN
- **流程简介**  
  输入图像和多个感兴趣区域（RoI）被输入到完全卷积网络中。网络首先使用几个卷积层（conv）和最大池化层来处理整个图像，以产生卷积特征图, 每个RoI汇集到一个固定大小的特征映射中(RoI池化层从特征图中提取固定长度的特征向量)，然后通过完全连接的层（FC）映射到特征向量, 其最终分支成两个同级输出层。
- RCNN缺点
- 损失函数  
  多损失融合（分类损失和回归损失融合）  
  - 分类损失  
    分类经过softmax层，分类损失采用负log函数计算损失 
  - **回归损失层**
    回归经过smoothL1层（**不太懂**）
- **SVD改进网络层**  
  - 采用**SVD**对Fast R-CNN网络末尾并行的全连接层进行分解，减少计算复杂度，加快检测速度  
  - **由于卷积层计算针对的是一整张图片，而全连接层需要对每一个region proposal都作用一次**，所以全连接层的计算占网络计算的将近一半。作者采用SVD来简化全连接层计算。  
  - 层的权重矩阵通过SVD被近似分解  
  - 详见[https://blog.csdn.net/P_LarT/article/details/84647113](https://blog.csdn.net/P_LarT/article/details/84647113)
- 输入数据  
  输入是224x224图片，训练过程中每个mini-batch包含2张图像和128个region proposal（即ROI，64个ROI/张）。**region proposal的提取仍然采用image selective search**。整个检测流程时间大多消耗在这上面  
  **region proposal是怎么输入网络的**？？？？region proposal经过ROIpooling后怎么映射到特征图书上。
  - 各个RoI在特征图上的投影.参考  
    [https://blog.csdn.net/P_LarT/article/details/84647113](https://blog.csdn.net/P_LarT/article/details/84647113)
 - **单尺度VS多尺度**  
   多尺度表示输入图像采用多种尺度输入，在测试的时候发现多尺度虽然能在mAP上得到些许提升但也增加了时间开销（作者给出原因：深度卷积网络可以学习尺度不变性）？？？？文章中描述没看懂。


### faster RCNN
- RPN  
  RPN(Region Propose Network) 对提取的卷积特征图进行处理. RPN 用于寻找可能包含 objects 的**预定义数量的区域(regions，边界框)**.
- 特征提取  
  在 depth 上，卷积特征图对图片的所有信息进行了编码，同时保持相对于原始图片所编码“things”的位置. 例如，如果在图片的左上角存在一个红色正方形，而且卷积层有激活响应，那么该红色正方形的信息被卷积层编码后，仍在卷积特征图的左上角.


## Yolo v1
### 理论  
- 网络结构
- **输出**：  
  输出为:**S×S×(B×5+C)**个预测值，即**7×7×(2×5+20)**，最终的输出为**7×7×30**的张量
  - **分成单元格**  
    首先会把原始图片**resize到448×448**，放缩到这个尺寸是为了后面整除来的方便。再把整个图片**分成S×S**(例:**7×7**)个单元格，对应了输出的7x7。此后以每个单元格为单位进行预测分析。
  - **单元格内**  
    - 如果一个object的中心落在某个单元格上，那么这个单元格负责预测这个物体。（即生成的label中，该网格置信度为1）
    - 每个单元格需要预测B个bbox值(**bbox值包括坐标和宽高**，原文中B=2)，同时为每个bbox值预测一个**置信度**(confidence scores)。也就是每个单元格需要预测B×(4+1)个值。
    - 每个单元格需要**预测C(物体种类个数**，原文C=20，这个与使用的数据库有关)个条件概率值.
  - **输出含义**  
    - **框的长宽及中心位置**  
      每个bounding box**输出4个数值**来表示其位置(Center_x,Center_y,width,height)，即(bounding box的中心点的x坐标，y坐标，bounding box的宽度，高度)，2个bounding box共需要8个数值来表示其位置  
      - 输出框的中心位置代表的是归一化的结果  
        输出其中的(x,y)代表的是框的相对于单元格的中心坐标，即bbox的中心相对于单元格的offset。  
        $x=\frac{x_c}{w_i}{S}−x_{col} , y=\frac{y_c}{h_i}{S}−y_{row}$
	        其中wi和hi代表整张图片的宽和高。x,y为网络输出，xc和yc为绝对坐标。
	  - 输出框的宽和高代表的是归一化结果  
	      $w=\frac{w_b}{w_i} , h=\frac{h_b}{h_i}$
	      > 代码中网络输出的宽和高可以是归一化结果的开方值，为了之后的loss计算。  
	      宽高开方的原因，是为了解决不同大小框，针对同样大小的边框差别，计算loss时影响相同的问题。[具体参考](https://www.jianshu.com/p/fefeb37a125a)
  
    - **置信度**   
      输出**1个数值**
    bounding box的置信度 = 该bounding box内存在对象的概率 * 该bounding box与该对象
    $P_r(Object)∗IOU^{truth}_{pred}$    
	    - 如果格子内有物体，则Pr(Object)=1，此时置信度等于IoU
	    - 如果格子内没有物体，则Pr(Object)=0，此时置信度为0
    - **分类**  
      输出是**20个值**，分别代表**网格**内物体是这20个类别的概率。  

	综合之上的输出为4+1+20=25，因为每个网格预测了两个框，故为7x7x(4+1+4+1+20)=7x7x30
- loss  
  ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/12486617-b4c64351c82653cc.png)
- 测试输出  
  因为测试文件无label框，所以用到NMS方法得到最终物体检测框输出。
  - 计算方法：  
    1. 每个框的输出置信度分别乘以20个类别的概率  
       $Score_{ij} = P(C_i|Object) * Confidence_j$  
       49个单元格，每个单元格两个检测框，所以共98个检测框。故可以得到98x20的得分值。
     2. 根据这98x20个得分，分别对每一类的98个得分值运用NMS方法。
     3. 最后输出的框即为最终检测框
    
### 代码总结

- 测试  
  测试中运用了NMS方法。  soft-NMS
- 问题  
    1. 论文中不是应该针对每一个类分别运用NMS求iou去掉重叠大的框，为什么这个代码里是直接对所有框进行NMS。
    2. 为什么将输入图像归一化到[-1,1]，而不是[0，1]
    3. object_delta = object_mask * (predict_scales - iou_predict_truth) 减去iou_predict_truth这个检测框与真实框的iou可以么？论文中是怎么讲述的？
<!--stackedit_data:
eyJoaXN0b3J5IjpbMTY3NTc4NTg4MiwtMTkzNzk0OTE2OCw0OD
YwODIwNCw0Mzc5OTQxNTEsLTE1Njg1NzA2MzIsMTYwMDc1NzUy
LC05MDIxNjM2MTUsMTIxNjk5MjUzLDIxNDY1MjIyNzgsLTgyNz
U4NTUzOSw5MDQ0MTI3ODIsMTg5MTM0NDQ0MCwyMDU1Nzk0NDgx
LC04MzA5NzMzMjMsMTcwODgzMjUxOSwtNzk5NDI2ODIxLC0xOT
YzMTA2MDE1LDE2NzI3MjExMzYsLTEwMjkxMDMwNiwzNzY3OTEz
MzJdfQ==
-->