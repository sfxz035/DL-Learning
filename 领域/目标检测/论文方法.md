### RCNN   
- 流程简介   
  - 给定一张输入图片，从图片中提取 2000 个类别独立的候选区域   
  - 对于每个区域利用 CNN 抽取一个固定长度的特征向量(使用fc7的4096长度特征向量)。   
  - 再对每个区域利用 SVM 进行目标分类。    
  - 位置精修： 使用回归器精细修正候选框位置
   ![enter image description here](https://lh3.googleusercontent.com/qt3IvSYPhKZl7WWq9YMIFu43AMfocxOh8D6lkv_Q9u9HT9fd-CopnhkN7Qvtu-LRHEcFxrBXFSxc)
- 提取候选区域的方法   
  -   objectness  
  - selective search
  - category-independen object proposals
  - constrained parametric min-cuts(CPMC)
  -   multi-scale combinatorial grouping
  -   Ciresan   
  RCNN使用的使selective search   
- 特征提取   
  R-CNN 抽取了一个 4096 维的特征向量(fc7输出)，采用的是 Alexnet。   
  Alexnet的输入使227x227，RCNN做法是直接将候选区域大小变换到227x227输入227x227。  
- SVM分类   
  对CNN输出的特征向量进行SVM分类，并产生分数。   
- 训练阶段   
  利用迁移学习，提取在 ILSVRC 2012 的模型和权重，然后在 VOC 上进行 fine-tune。解决VOC标注数据不足的问题
- 测试阶段   
   2000个候选区域，有分别对应的类别分数。针对同一类进行非极大性抑制(NMS)，避免重复。   
- bbox 回归   
  bbox 的值其实就是物体方框的位置，预测它就是回归问题，而不是分类问题。   
  
  受 DPM 的启发，作者训练了一个线性的回归模型，这个模型能够针对候选区域的 pool5 数据预测一个新的 box 位置。具体细节，作者放在补充材料当中

### sppnet   
针对RCNN，sppnet进行了改进。   
sppnet总体流程还是 Selective Search得到候选区域->CNN提取ROI特征->类别判断->位置精修。   
- RCNN与sppnet的总体流程相同，不同之处是：
	- RCNN进行候选区域挑选，再从原图中将候选区域剪切下来，分别送进CNN进行特征向量计算(2000个候选区进行2000次卷积网络的操作)。
	- SPPnet进行候选区域挑选，然后将整张图送进CNN，在feature map上针对候选区域位置提取ROI特征，之后进行类别判断。   
	![enter image description here](https://lh3.googleusercontent.com/1v5JkXW5z2SXQrtjm_2tZZB0DCppZS3x-GlVdOWKvuBTSsROtFUgFkSIj8jPi9wighL8Nhg2cYjH)    
	![enter image description here](https://lh3.googleusercontent.com/f-jSXOK42IDK2uVJ1X2XEd3WNwvcd4y7A7tZqeqg1GVdza-_aTU0-m0T-GnV_NkHibnaFf2smSS5)    
- 针对sppnet上述流程存在两个问题：  
   - 原始图像的ROI如何映射到特征图（一系列卷积层的最后输出）  
   - ROI的在特征图上的对应的特征区域的维度不满足全连接层的输入要求怎么办
-   解决从特征图上选区的ROI不满足全链接输入    
    - **空间金字塔池化**
   ![enter image description here](https://lh3.googleusercontent.com/tI_Y8__AYZB7r_1g1nstk0WIlctLNjJAQrdQtYnRdOtkUwKGk5hgwE5EHY02D4aQyyQc4fx7zHoK)     
       蓝色的图1——我们把一张完整的图片，分成了16个块，也就是每个块的大小就是(w/4,h/4);     
   
       绿色的图2，划分了4个块，每个块的大小就是(w/2,h/2);    
   
       黑色的图3，把整张图片作为了一个块，也就是块的大小为(w,h)       
    
       空间金字塔最大池化的过程，其实就是从这21个图片块中，分别计算每个块的**最大值**（局部max-pooling）。通过SPP，我们就把一张任意大小的图片转换成了一个**固定大小的21维特征**   
- ROI位置映射到feature map      
   映射的是ROI的两个角点，左上角和右下角，这两个角点就可以唯一确定ROI的位置了。
   参考：[https://zhuanlan.zhihu.com/p/24780433]   
- 训练   
   fine-tuning:只训练全连接层, 一个全连接层fc6,fc7, 一个有N+1个分支的分类层fc8。  
   采用多尺度训练，将不同尺寸大小的图片进行训练。
### Fast RCNN
- **RCNN的不足**   
   - 训练过程是多级流水线。R-CNN首先使用目标候选框对卷积神经网络使用log损失进行微调。然后，它将卷积神经网络得到的特征送入SVM。 这些SVM作为目标检测器，替代通过微调学习的softmax分类器。 在第三个训练阶段，学习检测框回归。   
   - 训练在时间和空间上是的开销很大。对于SVM和检测框回归训练，从每个图像中的每个目标候选框提取特征，并写入磁盘。在测试时，从每个测试图像中的每个目标候选框提取特征。  
   - 目标检测速度很慢。   
   - R-CNN网络需要对候选框进行形变操作后   
- **流程简介**     
   相比原网络做了以下调整：   
   -   最后一个最大值池化层用RoI池化层代替，该池化层可将不同大小的输入池化为统一大小输出。
   -   最后一层全连接层使用两个分裂的全连接层代替，一个用于计算分类，一个用于计算候选框的调整因子   
   -  输入改为两个，分别为原图和Selective Search产生的候选框坐标。相比RCNN不再对每一个候选框卷积，而是整张图卷积后从feature map上选取候选框。     
   - 总流程概括：该系统对于待识别图片，首先将其使用Selective Search处理获得一系列候选框，随后将其归一化到固定大小，送入CNN网络中提取特征。对于提取出的特征张量，假设其保留了原图片的空间位置信息，将候选框做对应变换后映射到特征张量上，提取出大小不同的候选区域的特征张量。对于每个候选区域的特征张量，使用RoI pooling层将其大小归一化，随后使用全连接层提取固定长度的特征向量。对于该特征向量，分别使用全连接层+softmax和全连接层+回归判断类别并计算原候选框的调整因子。
   RCNN:    
   ![enter image description here](https://lh3.googleusercontent.com/LrlbOD42UG-oEURS4L5yNB6qK6mTRnZivrf6jnsz9V9AJRCYJQXBAU_wp1PbHUFrZsCYKOEVfPJJ)    
   Fast RCNN:   
   ![enter image description here](https://lh3.googleusercontent.com/1g2uCqk3X5V1XtnCGotAvJGW-t9rgJPHLFiPlGWV8p6lKyU-K-EhbDuyWn3OIjQjjdOrbkQBbqzp)    
   - 贡献   
      - 实现大部分end-to-end训练(提proposal阶段除外)： 所有的特征都暂存在显存中，就不需要额外的磁盘空。  
      
        - joint training （SVM分类，bbox回归 联合起来在CNN阶段训练）把vgg最后一层的Softmax换成两个，一个是对区域的分类Softmax（包括背景），另一个是对bounding box的微调,代替了RCNN中的SVM分类，bbox回归。这个网络有两个输入，一个是整张图片，另一个是候选proposals算法产生的可能proposals的坐标。（对于SVM和Softmax，论文在SVM和Softmax的对比实验中说明，SVM的优势并不明显，故直接用Softmax将整个网络整合训练更好。对于联合训练： 同时利用了分类的监督信息和回归的监督信息，使得网络训练的更加鲁棒，效果更好。这两种信息是可以有效联合的。）
     - 提出了一个RoI层，算是SPP的变种，SPP是pooling成多个固定尺度，RoI只pooling到单个固定的尺度 （论文通过实验得到的结论是多尺度学习能提高一点点mAP，不过计算量成倍的增加，故单尺度训练的效果更好。）   
     
  
- 损失函数  
  多损失融合（分类损失和回归损失融合）    
  - 分类损失  
    分类经过softmax层，分类损失采用负log函数计算损失 
  - **回归损失层**
     回归经过smoothL1层（**不太懂**）论文在回归问题上并没有用很常见的2范数作为回归，而是使用所谓的鲁棒L1范数作为损失函数
- **SVD改进网络层**  
  - 采用**SVD**对Fast R-CNN网络末尾并行的全连接层进行分解，减少计算复杂度，加快检测速度  
  - **由于卷积层计算针对的是一整张图片，而全连接层需要对每一个region proposal都作用一次**，所以全连接层的计算占网络计算的将近一半。作者采用SVD来简化全连接层计算。  
  - 层的权重矩阵通过SVD被近似分解  
  - 详见[https://blog.csdn.net/P_LarT/article/details/84647113](https://blog.csdn.net/P_LarT/article/details/84647113)
- 输入数据  
  输入是224x224图片，训练过程中每个mini-batch包含2张图像和128个region proposal（即ROI，64个ROI/张）。**region proposal的提取仍然采用image selective search**。整个检测流程时间大多消耗在这上面  
  **region proposal是怎么输入网络的**？？？？region proposal经过ROIpooling后怎么映射到特征图书上。
 - 如何处理尺度不变性问题？即如何使24×24和1080×720的车辆同时在一个训练好的网络中都能正确识别？   
    单一尺度直接在训练和测试阶段将image定死为某种scale，直接输入网络训练就好，然后期望网络自己能够学习到scale-invariance的表达；多尺度在训练阶段随机从图像金字塔【缩放图片的scale得到，相当于扩充数据集】中采样训练，测试阶段将图像缩放为金字塔中最为相似的尺寸进行测试  
   多尺度表示输入图像采用多种尺度输入，在测试的时候发现多尺度虽然能在mAP上得到些许提升但也增加了时间开销（作者给出原因：深度卷积网络可以学习尺度不变性）


### faster RCNN   
fast rcnn已经实现了大部分的端到端，除了提取候选区这个步骤。Faster Rcnn则是把proposal也放入CNN实现（GPU中），实现完全的端到端。最终把region proposal提取和Fast-RCNN部分融合进了一个网络模型 (区域生成网络 RPN层)      
Faster RCNN可以大致看做“区域生成网络+fast RCNN“的系统，用区域生成网络代替fast RCNN中的Selective Search方法    
![enter image description here](https://lh3.googleusercontent.com/MDEzr-jL9Ak7uloFu09ai1XOVuk7TsACJHv4jQeXMmOMrxQ9Co8f2x5O_DilPpNKrvde_7kiKmcV)
- RPN  
  RPN(Region Propose Network) 对提取的卷积特征图进行处理. RPN 用于寻找可能包含objects 的**预定义数量的区域(regions，边界框)**.    
  ![enter image description here](https://lh3.googleusercontent.com/S0vfSl1x0npzklmhWyzNV7QxSa-IKRkyS00-MDitI_gvZ-ViI66ACZMO9JmgQXfVYwVsEKdxY-lg)   
  - 计算Anchors： 在卷积网络提取的feature map上的每个特征点预测多个region proposals。具体作法是：把每个特征点映射回映射回原图的感受野的中心点当成一个基准点，然后围绕这个基准点选取k个不同scale、aspect ratio的anchor。论文中3个scale（三种面积$128^2,256^2,521^2$），3个aspect ratio( {1:1,1:2,2:1} )，共9个框（可以自己设置任意数为k）。   
     ![enter image description here](https://lh3.googleusercontent.com/EKOjY3ja8JjvAlTvjEsMU0kn8pSebxLu9nFfjfCjmD0P-TjQ8Sk6Ag37h8hfuLBUHFTOGfjqUOQk)    
    - 网络针对每个框计算前景背景概率，进行二分类   
       对于每个anchor，首先在后面接上一个二分类softmax，有2个score 输出用以表示其是一个物体的概率与不是一个物体的概率。是物体的框则为候选框，配合原feature map进行ROI pooling。所以cls=2k scores。  
       
    -  框的回归位置预测   
       对于每个anchor都有[x, y, w, h]对应4个偏移量，所以reg=4k coordinates    
     ![enter image description here](https://lh3.googleusercontent.com/VUwAibf4Cg9bTx5jMZ4Ke4k7j1ckuJ9HcKL_hezoEtLoBYZujsaMWCkALBfAAp2-j7qFaPSUqifB)    
     > 卷积网络输出feature map大小为51*39*256，两支分别进行1x1卷积，分类的一支经过1x1卷积后变为51*39*18(2*9)，意味着一个特征点有九个框，每个框有正负两个概率，即2*9=18通道数。另一支框的预测回归则是51*49*36(4*9)，每个特征点有九个框，每个框有4个(x,y,w,h)的信息，即4*9=36通道数   
     
- 正负样本划分   
	-   对每个标定的ground true box区域，与其重叠比例最大的anchor记为 正样本 (保证每个ground true 至少对应一个正样本anchor)；
	-   对1)剩余的anchor，如果其与某个标定区域重叠比例大于0.7，记为正样本（每个ground true box可能会对应多个正样本anchor。但每个正样本anchor 只可能对应一个grand true box）；如果其与任意一个标定的重叠比例都小于0.3，记为负样本。
	-   对上面两步剩余的anchor，弃去不用；
	-   跨越图像边界的anchor弃去不用
- 特征提取  
  在 depth 上，卷积特征图对图片的所有信息进行了编码，同时保持相对于原始图片所编码“things”的位置. 例如，如果在图片的左上角存在一个红色正方形，而且卷积层有激活响应，那么该红色正方形的信息被卷积层编码后，仍在卷积特征图的左上角.


## Yolo v1
### 理论    
从R-CNN到Fast R-CNN一直采用的思路是proposal+分类 （proposal 提供位置信息， 分类提供类别信息）精度已经很高，但是速度还不行。
- 网络结构   
  网络结构借鉴了 GoogLeNet    
  ![enter image description here](https://lh3.googleusercontent.com/CE6J_4qjwva6anshO2CDWwuhU3gwcW3GaikPxv3PZXN8ksY9Ng2i2FDCBD2XJ4RS0KW54uHub29D)
- **流程简介**     
	- 针对一个输入图像，首先将图像划分成7 * 7的网格。

	- 对于每个网格，每个网格预测2个bouding box（每个box包含5个预测量）以及20个类别概率，总共输出7×7×（2*5+20）=1470个tensor

	- 根据上一步可以预测出7 * 7 * 2 = 98个目标窗口，然后根据阈值去除可能性比较低的目标窗口，再由NMS去除冗余窗口即可

- **输出**：  
  输出为:**S×S×(B×5+C)**个预测值，即**7×7×(2×5+20)**，最终的输出为**7×7×30**的张量
  - **分成单元格**  
    首先会把原始图片**resize到448×448**，放缩到这个尺寸是为了后面整除来的方便。再把整个图片**分成S×S**(例:**7×7**)个单元格，对应了输出的7x7。此后以每个单元格为单位进行预测分析。
  - **单元格内**  
    - 如果一个object的中心落在某个单元格上，那么这个单元格负责预测这个物体。（即生成的label中，该网格置信度为1）
    - 每个单元格需要预测B个bbox值(**bbox值包括坐标和宽高**，原文中B=2)，同时为每个bbox值预测一个**置信度**(confidence scores)。也就是每个单元格需要预测B×(4+1)个值。
    - 每个单元格需要**预测C(物体种类个数**，原文C=20，这个与使用的数据库有关)个条件概率值.
  - **输出含义**  
    - **框的长宽及中心位置**  
      每个bounding box**输出4个数值**来表示其位置(Center_x,Center_y,width,height)，即(bounding box的中心点的x坐标，y坐标，bounding box的宽度，高度)，2个bounding box共需要8个数值来表示其位置  
      - 输出框的中心位置代表的是归一化的结果  
        输出其中的(x,y)代表的是框的相对于单元格的中心坐标，即bbox的中心相对于单元格的offset。  
        $x=\frac{x_c}{w_i}{S}−x_{col} , y=\frac{y_c}{h_i}{S}−y_{row}$
	        其中wi和hi代表整张图片的宽和高。x,y为网络输出，xc和yc为绝对坐标。
	  - 输出框的宽和高代表的是归一化结果  
	      $w=\frac{w_b}{w_i} , h=\frac{h_b}{h_i}$
	      > 代码中网络输出的宽和高可以是归一化结果的开方值，为了之后的loss计算。  
	      宽高开方的原因，是为了解决不同大小框，针对同样大小的边框差别，计算loss时影响相同的问题。[具体参考](https://www.jianshu.com/p/fefeb37a125a)
  
    - **置信度**   
      输出**1个数值**
    bounding box的置信度 = 该bounding box内存在对象的概率 * 该bounding box与该对象
    $P_r(Object)∗IOU^{truth}_{pred}$    
	    - 如果格子内有物体，则Pr(Object)=1，此时置信度等于IoU
	    - 如果格子内没有物体，则Pr(Object)=0，此时置信度为0
    - **分类**  
      输出是**20个值**，分别代表**网格**内物体是这20个类别的概率。  

	综合之上的输出为4+1+20=25，因为每个网格预测了两个框，故为7x7x(4+1+4+1+20)=7x7x30
- loss  
  ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/12486617-b4c64351c82653cc.png)
- 测试输出  
  因为测试文件无label框，所以用到NMS方法得到最终物体检测框输出。
  - 计算方法：  
    1. 每个框的输出置信度分别乘以20个类别的概率  
       $Score_{ij} = P(C_i|Object) * Confidence_j$  
       49个单元格，每个单元格两个检测框，所以共98个检测框。故可以得到98x20的得分值。
     2. 根据这98x20个得分，分别对每一类的98个得分值运用NMS方法。
     3. 最后输出的框即为最终检测框
- 优缺点   
   - 优点   
     - 速度快，能够达到实时的要求  
     - 使用全图作为 Context 信息，背景错误（把背景错认为物体）比较少。    
     - 泛化能力强。
   - 缺点   
     - YOLO对相互靠的很近的物体（挨在一起且中点都落在同一个格子上的情况），还有很小的群体 检测效果不好，这是因为一个网格中只预测了两个框，并且只属于一类。  
     - 测试图像中，当同一类物体出现的不常见的长宽比和其他情况时泛化能力偏弱。
     - 由于损失函数的问题，定位误差是影响检测效果的主要原因，尤其是大小物体的处理上，还有待加强   
     - 使用了end-to-end的回归方法，没有region proposal步骤，直接回归便完成了位置和类别的判定。可能造成定位误差从而影响精度。
### 代码总结

- 测试  
  测试中运用了NMS方法。  soft-NMS
- 问题  
    1. 论文中不是应该针对每一个类分别运用NMS求iou去掉重叠大的框，为什么这个代码里是直接对所有框进行NMS。
    2. 为什么将输入图像归一化到[-1,1]，而不是[0，1]
    3. object_delta = object_mask * (predict_scales - iou_predict_truth) 减去iou_predict_truth这个检测框与真实框的iou可以么？论文中是怎么讲述的？   

### Yolo v2   
针对Yolo v1精度不够的问题进行改进，提升网络精度。    

- 提高精度   
  - batch normlization   
    通过在卷积层后添加bn提高网络精度。  
  - High Resolution Classiﬁer
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTExMDk4NTIyNTIsMjExMDQwMDc5OSwtMT
IzODc5NDMzMCw4OTk4ODYxNiwtMTU4NDI4NjQxMSw4MDQ0NTMy
ODAsLTcwNzY3OTUwMSwxNDMwNzgzMDg0LDc0MjAzMzYzNywxMj
gxMTM4NjI3LC02MTIzMjQyMCwtNjIxMDM0NDMyLDczNDA2MjYz
OCwxMDM2MDA5NzMxLC0yMTY1ODcyOTUsMTY3NDMwNDA3MiwtNj
Q2NTk4NjE5LDIxMTAzNTI3MjgsLTExMTAwMzU1NjIsLTEzODM4
NzU1NDNdfQ==
-->