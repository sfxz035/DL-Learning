### RCNN
- 不懂之处  
  1. 过计算 IoU 指标，采取**非极大性抑制**，以最高分的区域为基础，剔除掉那些重叠位置的区域？
  2. 训练了一个线性回归模型，在给定一个选择区域的pool5特征时去预测一个新的检测窗口。pool5特征是什么？怎么预测一个新的检测窗口

### sppnet
- spp层去掉了原始图像上的crop/warp等操作。  
   主要原因是CNN的全连接层要求输入图片是大小一致的，而实际中的输入图片往往大小不一，如果直接缩放到同一尺寸，很可能有的物体会充满整个图片，而有的物体可能只能占到图片的一角。传统的解决方案是进行不同位置的裁剪，但是这些裁剪技术都可能会导致一些问题出现，比如图7中的crop会导致物体不全，warp导致物体被拉伸后形变严重，SPP就是为了解决这种问题的。  
   空间金字塔池化

### Fast RCNN
- **流程简介**  
  输入图像和多个感兴趣区域（RoI）被输入到完全卷积网络中。网络首先使用几个卷积层（conv）和最大池化层来处理整个图像，以产生卷积特征图, 每个RoI汇集到一个固定大小的特征映射中(RoI池化层从特征图中提取固定长度的特征向量)，然后通过完全连接的层（FC）映射到特征向量, 其最终分支成两个同级输出层。
- RCNN缺点
- 损失函数  
  多损失融合（分类损失和回归损失融合）  
  - 分类损失  
    分类经过softmax层，分类损失采用负log函数计算损失 
  - **回归损失层**
    回归经过smoothL1层（**不太懂**）
- **SVD改进网络层**  
  - 采用**SVD**对Fast R-CNN网络末尾并行的全连接层进行分解，减少计算复杂度，加快检测速度  
  - **由于卷积层计算针对的是一整张图片，而全连接层需要对每一个region proposal都作用一次**，所以全连接层的计算占网络计算的将近一半。作者采用SVD来简化全连接层计算。  
  - 层的权重矩阵通过SVD被近似分解  
  - 详见[https://blog.csdn.net/P_LarT/article/details/84647113](https://blog.csdn.net/P_LarT/article/details/84647113)
- 输入数据  
  输入是224x224图片，训练过程中每个mini-batch包含2张图像和128个region proposal（即ROI，64个ROI/张）。**region proposal的提取仍然采用image selective search**。整个检测流程时间大多消耗在这上面  
  **region proposal是怎么输入网络的**？？？？region proposal经过ROIpooling后怎么映射到特征图书上。
  - 各个RoI在特征图上的投影.参考  
    [https://blog.csdn.net/P_LarT/article/details/84647113](https://blog.csdn.net/P_LarT/article/details/84647113)
 - **单尺度VS多尺度**  
   多尺度表示输入图像采用多种尺度输入，在测试的时候发现多尺度虽然能在mAP上得到些许提升但也增加了时间开销（作者给出原因：深度卷积网络可以学习尺度不变性）？？？？文章中描述没看懂。


### faster RCNN
- RPN  
  RPN(Region Propose Network) 对提取的卷积特征图进行处理. RPN 用于寻找可能包含 objects 的**预定义数量的区域(regions，边界框)**.
- 特征提取  
  在 depth 上，卷积特征图对图片的所有信息进行了编码，同时保持相对于原始图片所编码“things”的位置. 例如，如果在图片的左上角存在一个红色正方形，而且卷积层有激活响应，那么该红色正方形的信息被卷积层编码后，仍在卷积特征图的左上角.


## Yolo v1
### 理论  
- 网络结构
- 重要要点：  
  输出为:**S×S×(B×5+C)**个预测值，即**7×7×(2×5+20)**，最终的输出为**7×7×30**的张量
  - **分成单元格**  
    首先会把原始图片**resize到448×448**，放缩到这个尺寸是为了后面整除来的方便。再把整个图片**分成S×S**(例:**7×7**)个单元格，对应了输出的7x7。此后以每个单元格为单位进行预测分析。
  - **单元格内**  
    - 如果一个object的中心落在某个单元格上，那么这个单元格负责预测这个物体。（即生成label中，该网格置信度为1。  ）
    - 每个单元格需要预测B个bbox值(**bbox值包括坐标和宽高**，原文中B=2)，同时为每个bbox值预测一个**置信度**(confidence scores)。也就是每个单元格需要预测B×(4+1)个值。
    - 每个单元格需要**预测C(物体种类个数**，原文C=20，这个与使用的数据库有关)个条件概率值.
  - 输出含义  
    - 框的长宽及中心位置  
      每个bounding box需要4个数值来表示其位置，(Center_x,Center_y,width,height)，即(bounding box的中心点的x坐标，y坐标，bounding box的宽度，高度)，2个bounding box共需要8个数值来表示其位置  
      (x,y)代表的是框的相对于单元格的中心坐标，即bbox的中心相对于单元格的offset。
    - 
  - 
- 

### 代码总结

- 测试  
  测试中运用了NMS方法。  soft-NMS
- 问题  
    1. 论文中不是应该针对每一个类分别运用NMS求iou去掉重叠大的框，为什么这个代码里是直接对所有框进行NMS。
    2. 为什么将输入图像归一化到[-1,1]，而不是[0，1]
    3. object_delta = object_mask * (predict_scales - iou_predict_truth) 减去iou_predict_truth这个预测框与真实框的iou可以么？论文中是怎么讲述的？
<!--stackedit_data:
eyJoaXN0b3J5IjpbMTg5MTM0NDQ0MCwyMDU1Nzk0NDgxLC04Mz
A5NzMzMjMsMTcwODgzMjUxOSwtNzk5NDI2ODIxLC0xOTYzMTA2
MDE1LDE2NzI3MjExMzYsLTEwMjkxMDMwNiwzNzY3OTEzMzIsMT
IyMDY5NzY0MCwtMTA3MTkxNDc4Myw5NzkzMzA3MzAsLTgyNTQ1
MjA1MywtMzQ1MTIwMzI2LDE5NTM1MDY4MDQsLTIyNzc4MjA5Ni
wxMDIyNzEzMzI4LDgwODQ3OTU4NSwtNTYxNTgyNjIzLDc5NTE4
MDQ4OF19
-->