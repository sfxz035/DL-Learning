
### SRCNN
- **数据输入输出**  
   - 输入：SRCNN首先使用双三次(bicubic)插值将低分辨率图像放大成目标尺寸。(例如33x33)  
  - 输出：经卷积后的大小，卷积不进行padding。(例如33x33输入，输出21x21)  
  - 标签：作为标签数据的则为图像中心的21×21图像块（与卷积层细节设置相关）。  
> 测试时需将卷积模式改为padding
- **网络结构**  
  将3层网络划分为图像块提取(Patch extraction and representation)、非线性映射(Non-linear mapping)以及最终的重建(Reconstruction)。
  三层卷积神经网络，网络形式为(conv1+relu1)—(conv2+relu2)—(conv3)）。
  - 第一层卷积：卷积核尺寸9×9(f1×f1)，卷积核数目64(n1)，输出64张特征图；第二层卷积：卷积核尺寸1×1(f2×f2)，卷积核数目32(n2)，输出32张特征图；第三层卷积：卷积核尺寸5×5(f3×f3)，卷积核数目1(n3)。
- **问题**：  
  在本论文中的其中一实验相关设置，对YCrCb颜色空间中的Y通道进行重建。那么彩色图像应该怎么重建？所有彩色通道都重建？不同彩色模式对应的重建对象也不同？ 

### FSRCNN
- 针对SRCNN改进：
	- SRCNN因为输入的是cheap interpolated LR image, 所以有很多冗余计算。  
	  FSRCNN直接输入低分辩图，不用插值。
	- SRCNN的Non-linear Mapping的参数量比较大  
	  FSRCNN改变特征维数，使用更小的卷积核和使用更多的映射层。
	 - 是可以共享其中的映射层，如果需要训练不同上采样倍率的模型，只需要fine-tuning最后的反卷积层。前面的特征提取和非线性映射都不用再训练。  
- 网络结构  
	 - 特征提取层  
	     kernel size为5x5，输入通道1，输出特征通道数为d.
	 - 收缩  
	     用1x1卷积对LR feature降维, 减少后面的计算量。输入通道数d，输出s，s<<d。
	 - 非线性映射  
	     由m个3x3卷积层构成。特征通道数不变，为s。感受野大，表现的更好
	 - 扩张  
	     用1x1的卷积对特征维度进行上升，即增加特征通道数。输入通道数为s,输出为d。相当于收缩的反操作。  
	     >因为作者发现低纬度的HR feature的重建效果不好, 所以在mapping之后, 又用1x1 conv将HR feature升维, 类似Shrinking的反操作
	   
	 - 反卷积重建  
	     卷积核大小为9x9，输入通道数为d，输出通道数为1，实现上采样，进行尺寸放大。因为stride=k的conv卷积会将feature map缩小k倍, 所以stride为k的tranpose conv会将feature map放大k倍。
	   对比各种参数的结果，选取 m=4, d=56, s=12  
	  - 激活函数采用PReLU  
	  
<!--stackedit_data:
eyJoaXN0b3J5IjpbMTAxODczNDQyLC00Njg5Mjc3MzIsLTE5Nz
g1MTIyODAsNDI2MTI5NTU0LC00OTM3OTA2NzcsLTkzODA4NDM0
NSwzODQ0NzQ2NDIsLTE2NjkzODk0ODcsOTM0NzU0MTAyLDI1Mz
AwNjc2OCwyMDQwMjk3NjIyXX0=
-->