## 数据预处理
- **标准化**  
  数据标准化：**均值去除 和 按方差比例缩放**
  当单个特征的样本取值相差甚大或明显不遵从高斯正态分布时，标准化表现的效果较差。实际操作中，经常忽略特征数据的分布形状，移除每个特征均值，划分离散特征的标准差，从而等级化，进而实现数据中心化。  
  - 特点  
  对不同特征维度的伸缩变换的目的是使得不同度量之间的特征具有可比性。同时不改变原始数据的分布。  
  - 好处  
    1 使得不同度量之间的特征具有可比性，对目标函数的影响体现在几何分布上，而不是数值上    
    2 不改变原始数据的分布
   - **代码实现**  
     - sklearn preprocessing  
       1. `preprocessing.scale(X)`相当于z-score 标准化(zero-mean normalization)  
	          ```
	          def scale(X, axis=0, with_mean=True, with_std=True,copy=True)```  
          参数解释：  
          X：{array-like, sparse matrix} 数组或者矩阵，一维的数据都可以（但是在0.19版本后一维的数据会报错了！）
    axis：int类型，初始值为0，axis用来计算均值 means 和标准方差 standard deviations.如果是0，则单独的标准化每个特征（列），如果是1，则标准化每个观测样本（行）。  
    with_mean: boolean类型，默认为True，表示将数据均值规范到0  
    with_std: boolean类型，默认为True，表示将数据方差规范到
       2.  `sklearn.preprocessing.StandardScaler()`  
          可保存训练集的标准化参数(均值、方差)，然后应用在转换测试集数据。 一般我们的标准化先在训练集上进行，在测试集上也应该做同样 mean 和 variance 的标准化
	          ```
	          scaler = preprocessing.StandardScaler().fit(X)
	          ```
      - 自定义  
          ```x_norm = (x-np.mean(x))/np.std(x)  ```
 - **归一化**  
   **将数据特征缩放至某一范围**  
   - 特点  
     对不同特征维度的伸缩变换的目的是使各个特征维度对目标函数的影响权重是一致的，即使得那些扁平分布的数据伸缩变换成类圆形。这也就改变了原始数据的一个分布。
    - 好处  
      **两个特征区间相差非常大时，形成的等高线偏椭圆，迭代时很有可能走“之字型”路线（垂直长轴），从而导致需要迭代很多次才能收敛。而对两个特征进行了归一化，对应的等高线就会变圆，在梯度下降进行求解时能较快的收敛。**
       1 提高迭代求解的收敛速度
       2 提高迭代求解的精度
    - **代码实现**  
      - sklearn preprocessing  
        ```
        min_max_scaler = preprocessing.MinMaxScaler()
        X_train_minmax = min_max_scaler.fit_transform(X_train)
        ``` 
      - 自定义  
        ```x_norm = (x-np.min(x))/(np.max(x)-np.min(x))  ```
 - **两者区别**  
   参考[https://www.zhihu.com/question/20467170/answer/222792995](https://www.zhihu.com/question/20467170/answer/222792995)
 - **归一化问题**
   - 对每一个样本进行进行归一化（按行归一化）还是对每一个维度进行归一化（按列归一化）？  
     **对每一个维度进行归一化**。对于每个样本，由于它的每一个维度的量纲不同，若对每一个样本进行归一化且在量纲数量级差别悬殊时会使样本中较低数量级的属性变为0，会使原始信息过多丧失。
   - 对训练集归一化后再映射到测试集归一化，还是训练集测试集一起归一化？  
     **应该训练集归一化后再映射到测试集归一化**
   - **不能对样本一个一个逐一归一化，要整个训练集一起归一化**

## batchnorm (批标准化) 
#### batchnorm要解决的问题
- **内部协变量偏移（Internal Covariate Shift）**    
  训练深度网络的时候经常发生训练困难的问题，因为，每一次参数迭代更新后，上一层网络的输出数据经过这一层网络计算后，数据的分布会发生变化，为下一层网络的学习带来困难（神经网络本来就是要学习数据的分布，要是分布一直在变，学习就很难了），此现象称之为Internal Covariate Shift。
- 协变量偏移（covariate shift）  
  Internal Covariate Shift 和Covariate Shift具有相似性，但并不是一个东西，前者发生在神经网络的内部，所以是Internal，后者发生在输入数据上。Covariate Shift主要描述的是由于训练数据和测试数据存在分布的差异性，给网络的泛化性和训练速度带来了影响，我们经常使用的方法是做归一化或者白化。
#### batchnorm原理  
 直接对每层进行归一化会降低层的表达力，每一层的数据分布都是标准正太分布，导致其完全学习不到输入数据的特征。

	1.先求出此次批量数据x的均值，μ
	2.求出此次batch的方差，σ
	3.接下来就是对x做归一化，得到x−i  
	4.最重要的一步，引入缩放和平移变量γ和β ,计算归一化后的值，yi=γxi + β  
#### **计算方式**  
  **逐channel地计算**同一batch中所有数据的mean和variance，再对input使用mean和variance进行归一化，最后的输出再进行线性平移，得到batch_norm的最终结果。  
  ```
  for i in range(channel):
	  x = input[:,:,:,i]
	  mean = mean(x)
	  variance = variance(x)
	  x = (x - mean) / sqrt(variance)
	  x = scale * x + offset
	  input[:,:,:,i] = x
  ```  
  #### 均值和方差的更新方式   
  - train阶段每个batch计算的mean/variance采用**指数加权平均**来得到test阶段mean/variance的估计  
	  ```  
	  1.moving_mean= momentum * moving_mean+ (1 - momentum) * x_mean
	  2.moving_var= momentum * moving_var+ (1 - momentum) * x_var
	  ```  
	  - 默认初始值：momentum默认初始值为0.99  
	  - $\gamma,\beta$是训练中学习到的？
#### batchnorm优点
1. 避免了梯度消失和梯度爆炸
2. 加速网络的收敛
3. 提高了网络的泛化能力。由于在训练的过程中批量标准化所用到的均值和方差是在一小批样本(mini-batch)上计算的，而不是在整个数据集上，所以均值和方差会有一些小噪声产生。
#### batchnorm缺点
1. batchsize的问题  
   对batchsize的大小比较敏感，由于每次计算均值和方差是在一个batch上，所以如果batchsize太小，则计算的均值、方差不足以代表整个数据分布；
2. batchnorm和RNN  
   BN实际使用时需要计算并且保存某一层神经网络batch的均值和方差等统计信息，对于对一个固定深度的前向神经网络（DNN，CNN）使用BN，很方便；但对于RNN来说，sequence的长度是不一致的这样training时，计算很麻烦。
#### **batchnorm使用**
参考：
以下参考皆是tf.layers.batch_normalization
1. 详细train和test中的使用 [https://zhuanlan.zhihu.com/p/34879333] 
2. [https://github.com/udacity/cn-deep-learning/blob/master/tutorials/batch-norm/Batch_Normalization_Solutions.ipynb]


## 总结  
- **输入标准化**
调整样本数据每个维度的量纲，让每个维度数据量纲相同或接近。通常我们在使用数据之前，会对输入数据做了标准化处理：处理后的任意一个特征在数据集中所有样本上的均值为 0、标准差为 1。标准化处理输入数据使各个特征的分布相近：这往往更容易训练出有效的模型。因为两个特征区间相差非常大时，形成的等高线偏椭圆，迭代时很有可能走“之字型”路线（垂直长轴），从而导致需要迭代很多次才能收敛。而对两个特征进行了归一化，对应的等高线就会变圆，在梯度下降进行求解时能较快的收敛。
  - 样本间归一化 和特征间归一化  
    如果进行样本间归一化，即每个样本逐一归一化，那么每个样本会被统一到一个量纲下，样本间差异就会损失，是不合理的。而需要进行的是特征间归一化，能将每个维度特征的量纲相同或相近。不同维度特征的量纲差距大，这样每个维度的影响不一样（如1.0m和1000cm）。不同维度差距大，在超空间中，会形成超椭球的形状。
- **层间标准化（batchnorm批标准化）**  
1. （虽然输入数据标准化，但层间数据分布发生剧烈变化--自己的理解）每一次参数迭代更新后，上一层网络的输出数据经过这一层网络计算后，数据的分布会发生变化，为下一层网络的学习带来困难。
2. 随着网络深度加深或者在训练过程中，其分布逐渐发生偏移或者变动，之所以训练收敛慢，一般是整体分布逐渐往非线性函数的取值区间的上下限两端靠近（对于Sigmoid函数来说，意味着激活输入值WU+B是大的负值或正值），所以这导致反向传播时低层神经网络的梯度消失。BN通过归一化把每层神经网络任意神经元这个输入值的分布强行拉回到均值为0方差为1的标准正态分布，这样使得激活输入值落在非线性函数对输入比较敏感的区域，这样输入的小变化就会导致损失函数较大的变化。**经过BN后，目前大部分Activation的值落入非线性函数的线性区内，其对应的导数远离导数饱和区，这样来加速训练收敛过程**
在层间加入标准后可以解决分布变化太剧烈的问题，但单纯的对每一层进行归一化就会使得每层分布区间完全相同。将大部分ctivation的值落入非线性函数的线性区内，使得非线性函数替换成线性函数效果相同了，深度网络就失去了意义，多层网络等同于了一层网络的线性表达。**所以BN为了保证非线性的获得，对变换后的满足均值为0方差为1的x又进行了scale加上shift操作**，这两个参数是通过训练学习到的，达到一个一个线性和非线性的较好平衡点。  

## Layer Normalizaiton  
针对BN的缺点，LN的提出针对解决这两个问题。  
和BN不同的是，BN对一个batch中的**不同样本的同一特征维度**标准化（对不同的特征维度逐一进行计算），而LN是对**同一样本的所有不同特征维度**进行标准化（对不同的样本逐一进行计算）
- 方法  
  - 针对每个样本的所有维度
IN对所有通道一起标准化
1. 什么时候分别对不同特征通道标准化，什么时候可以将不同特征通道一起标准化  
   答：当特征通道分别独立时对不同通道进行标准化？各个特征通道相关联的时候一起进行标准化？（个人猜测），具体例子？
2. 什么时候可以对所有样本一起标准化，什么时候分别对单个数据进行标准化？  
   答：BN注重对每个batch进行归一化，保证数据分布一致，因为判别模型中结果取决于数据整体分布。  
   但是**图像风格化**中，生成结果主要依赖于某个图像实例，所以对整个batch归一化不适合图像风格化中，因而对HW做归一化。可以加速模型收敛，并且保持每个图像实例之间的独立。
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTE3MDU4NDIyMzAsNzMxNzM3NTg0LDEzMz
IxMDIyNTksLTQxMTg1OTc5OSwxNTk2NDk3MTQ3LC0xODY4NDAx
NjE0LC0xNzAxODE2MDUzLDQxMDMyODY1LDEyNzQ2Mzk2MjksLT
E2MzY4NTg0MTAsLTEwMzc2NjQ5NjIsNDIyOTAwNDM4LC0yODc4
MDU5MiwxNjE5MzU1MzM5LC0xNjMzODY3NjA1LC0xMzQzNzYwNT
k3LDEyODkxNjQ1OTcsMzcyMTg3NTc2LDE5NDMzMzk2NjUsLTE2
NjI5NDI5ODRdfQ==
-->