## 数据预处理
- **标准化**  
  数据标准化：**均值去除 和 按方差比例缩放**
  当单个特征的样本取值相差甚大或明显不遵从高斯正态分布时，标准化表现的效果较差。实际操作中，经常忽略特征数据的分布形状，移除每个特征均值，划分离散特征的标准差，从而等级化，进而实现数据中心化。  
  - 特点  
  对不同特征维度的伸缩变换的目的是使得不同度量之间的特征具有可比性。同时不改变原始数据的分布。  
  - 好处  
    1 使得不同度量之间的特征具有可比性，对目标函数的影响体现在几何分布上，而不是数值上    
    2 不改变原始数据的分布
   - **代码实现**  
     - sklearn preprocessing  
       1. `preprocessing.scale(X)`相当于z-score 标准化(zero-mean normalization)  
	          ```
	          def scale(X, axis=0, with_mean=True, with_std=True,copy=True)```  
          参数解释：  
          X：{array-like, sparse matrix} 数组或者矩阵，一维的数据都可以（但是在0.19版本后一维的数据会报错了！）
    axis：int类型，初始值为0，axis用来计算均值 means 和标准方差 standard deviations.如果是0，则单独的标准化每个特征（列），如果是1，则标准化每个观测样本（行）。  
    with_mean: boolean类型，默认为True，表示将数据均值规范到0  
    with_std: boolean类型，默认为True，表示将数据方差规范到
       2.  `sklearn.preprocessing.StandardScaler()`  
          可保存训练集的标准化参数(均值、方差)，然后应用在转换测试集数据。 一般我们的标准化先在训练集上进行，在测试集上也应该做同样 mean 和 variance 的标准化
	          ```
	          scaler = preprocessing.StandardScaler().fit(X)
	          ```
      - 自定义  
          ```x_norm = (x-np.mean(x))/np.std(x)  ```
 - **归一化**  
   **将数据特征缩放至某一范围**  
   - 特点  
     对不同特征维度的伸缩变换的目的是使各个特征维度对目标函数的影响权重是一致的，即使得那些扁平分布的数据伸缩变换成类圆形。这也就改变了原始数据的一个分布。
    - 好处  
      **两个特征区间相差非常大时，形成的等高线偏椭圆，迭代时很有可能走“之字型”路线（垂直长轴），从而导致需要迭代很多次才能收敛。而对两个特征进行了归一化，对应的等高线就会变圆，在梯度下降进行求解时能较快的收敛。**
       1 提高迭代求解的收敛速度
       2 提高迭代求解的精度
    - **代码实现**  
      - sklearn preprocessing  
        ```
        min_max_scaler = preprocessing.MinMaxScaler()
        X_train_minmax = min_max_scaler.fit_transform(X_train)
        ``` 
      - 自定义  
        ```x_norm = (x-np.min(x))/(np.max(x)-np.min(x))  ```
 - **两者区别**  
   参考[https://www.zhihu.com/question/20467170/answer/222792995](https://www.zhihu.com/question/20467170/answer/222792995)
 - **归一化问题**
   - 对每一个样本进行进行归一化（按行归一化）还是对每一个维度进行归一化（按列归一化）？  
     **对每一个维度进行归一化**。对于每个样本，由于它的每一个维度的量纲不同，若对每一个样本进行归一化且在量纲数量级差别悬殊时会使样本中较低数量级的属性变为0，会使原始信息过多丧失。
   - 对训练集归一化后再映射到测试集归一化，还是训练集测试集一起归一化？  
     **应该训练集归一化后再映射到测试集归一化**
   - **不能对样本一个一个逐一归一化，要整个训练集一起归一化**
- **标准化/白化问题**  
   为什么白化可以加快训练呢 ？ 
   - 参数初始化都是位于零左右的值，而白化后可以将数据分布拉回零范围左右，无疑可以加快学习的速度。更进一步的话我们对数据再进行去相关操作，使得数据更加容易区分，这样又会加快训练。
   参考：[https://blog.csdn.net/shenziheng1/article/details/81254206](https://blog.csdn.net/shenziheng1/article/details/81254206)
## batchnorm (批标准化) 
#### batchnorm要解决的问题
- **内部协变量偏移（Internal Covariate Shift）**    
  训练深度网络的时候经常发生训练困难的问题，因为，每一次参数迭代更新后，上一层网络的输出数据会发生变化，为下一层网络的学习带来困难（神经网络本来就是要学习数据的分布，要是分布一直在变，学习就很难了），层层叠加，深层的输入分布变化会非常剧烈，这就使得深层需要不断去重新适应底层的参数更新。此现象称之为Internal Covariate Shift。  
  >每一次参数更新都是根据前一层
- 协变量偏移（covariate shift）  
  Internal Covariate Shift 和Covariate Shift具有相似性，但并不是一个东西，前者发生在神经网络的内部，所以是Internal，后者发生在输入数据上。Covariate Shift主要描述的是由于训练数据和测试数据存在分布的差异性，给网络的泛化性和训练速度带来了影响，我们经常使用的方法是做归一化或者白化。
#### batchnorm原理  
 直接对每层进行归一化会降低层的表达力，每一层的数据分布都是标准正太分布，导致其完全学习不到输入数据的特征。

	1.先求出此次批量数据x的均值，μ
	2.求出此次batch的方差，σ
	3.接下来就是对x做归一化，得到x−i  
	4.最重要的一步，引入缩放和平移变量γ和β ,计算归一化后的值，yi=γxi + β  
#### **计算方式**  
  **逐channel地计算**同一batch中所有数据的mean和variance，再对input使用mean和variance进行归一化，最后的输出再进行线性平移，得到batch_norm的最终结果。  
  ```
  for i in range(channel):
	  x = input[:,:,:,i]
	  mean = mean(x)
	  variance = variance(x)
	  x = (x - mean) / sqrt(variance)
	  x = scale * x + offset
	  input[:,:,:,i] = x
  ```  
  #### 均值和方差的更新方式   
  - train阶段每个batch计算的mean/variance采用**指数加权平均**来得到test阶段mean/variance的估计  
	  ```  
	  1.moving_mean= momentum * moving_mean+ (1 - momentum) * x_mean
	  2.moving_var= momentum * moving_var+ (1 - momentum) * x_var
	  ```  
	  - 默认初始值：momentum默认初始值为0.99  
	  - $\gamma,\beta$是训练中学习到的？
#### batchnorm优点
1. 避免了梯度消失和梯度爆炸
2. 加速网络的收敛
3. 提高了网络的泛化能力。由于在训练的过程中批量标准化所用到的均值和方差是在一小批样本(mini-batch)上计算的，而不是在整个数据集上，所以均值和方差会有一些小噪声产生。
#### batchnorm缺点
1. batchsize的问题  
   对batchsize的大小比较敏感，由于每次计算均值和方差是在一个batch上，所以batchsize较大时，效果较好。而如果batchsize太小，则计算的均值、方差不足以代表整个数据分布；
2. batchnorm和RNN  
   BN实际使用时需要计算并且保存某一层神经网络batch的均值和方差等统计信息，对于对一个固定深度的前向神经网络（DNN，CNN）使用BN，很方便；但对于RNN来说，sequence的长度是不一致的这样training时，计算很麻烦。
#### **batchnorm使用**
参考：
以下参考皆是tf.layers.batch_normalization
1. 详细train和test中的使用 [https://zhuanlan.zhihu.com/p/34879333] 
2. [https://github.com/udacity/cn-deep-learning/blob/master/tutorials/batch-norm/Batch_Normalization_Solutions.ipynb]


## 总结  
- **输入标准化**
调整样本数据每个维度的量纲，让每个维度数据量纲相同或接近。通常我们在使用数据之前，会对输入数据做了标准化处理：处理后的任意一个特征在数据集中所有样本上的均值为 0、标准差为 1。标准化处理输入数据使各个特征的分布相近：这往往更容易训练出有效的模型。因为两个特征区间相差非常大时，形成的等高线偏椭圆，迭代时很有可能走“之字型”路线（垂直长轴），从而导致需要迭代很多次才能收敛。而对两个特征进行了归一化，对应的等高线就会变圆，在梯度下降进行求解时能较快的收敛。
  - 样本间归一化 和特征间归一化  
    如果进行样本间归一化，即每个样本逐一归一化，那么每个样本会被统一到一个量纲下，样本间差异就会损失，是不合理的。而需要进行的是特征间归一化，能将每个维度特征的量纲相同或相近。不同维度特征的量纲差距大，这样每个维度的影响不一样（如1.0m和1000cm）。不同维度差距大，在超空间中，会形成超椭球的形状。
- **层间标准化（batchnorm批标准化）**  
1. （虽然输入数据标准化，但层间数据分布发生剧烈变化--自己的理解）每一次参数迭代更新后，上一层网络的输出数据经过计算后，数据的分布会发生变化，为下一层网络的学习带来困难。
2. 随着网络深度加深或者在训练过程中，其分布逐渐发生偏移或者变动，之所以训练收敛慢，一般是整体分布逐渐往非线性函数的取值区间的上下限两端靠近（对于Sigmoid函数来说，意味着激活输入值WU+B是大的负值或正值），所以这导致反向传播时低层神经网络的梯度消失。BN通过归一化把每层神经网络任意神经元这个输入值的分布强行拉回到均值为0方差为1的标准正态分布，这样使得激活输入值落在非线性函数对输入比较敏感的区域，这样输入的小变化就会导致损失函数较大的变化。**经过BN后，目前大部分Activation的值落入非线性函数的线性区内，其对应的导数远离导数饱和区，这样来加速训练收敛过程**  
   >一，浅层参数需要不断适应新的输入数据分布，降低学习速度。  
   >二，浅层输入的变化可能趋向于变大或者变小，导致深层落入饱和区，使得学习过早停止，参数无法得到有效更新。而且落入饱和区，无论输入值之间差距多大，激活后都是接近1的值，失去了对数据的敏感性。
	
	- 平移缩放的原因  
	  为了保证模型的表达能力不因为规范化而下降
	  - 在层间加入标准后可以解决分布变化太剧烈的问题，但单纯的对每一层进行归一化就会使得每层分布区间完全相同。将大部分ctivation的值落入非线性函数的线性区内，使得非线性函数替换成线性函数效果相同了，深度网络就失去了意义，多层网络等同于了一层网络的线性表达。**所以BN为了保证非线性的获得，对变换后的满足均值为0方差为1的x又进行了scale加上shift操作**，这两个参数是通过训练学习到的，达到一个一个线性和非线性的较好平衡点。  
		- 要点: 缩放平移操作是为了 1.充分利用前面层学习的能力，自适应地去调整层特征分布  2.增强非线性表达能力。

- 为什么进行白化和batchnorm  
  -    为什么白化可以加快训练呢 ？  
    参数初始化都是位于零左右的值，而白化后可以将数据分布拉回零范围左右，无疑可以加快学习的速度。更进一步的话我们对数据再进行去相关操作，使得数据更加容易区分，这样又会加快训练。
   -  假设条件  
     为了有效地学习神经网络，神经网络的每一层分布都应该：均值为0、始终保持相同的分布（和参数初始化为零左右的值有关？）；
     第二个条件意味着通过批梯度下降输入到网路层的数据分布不应该变化太多，并且随着训练的进行它应该保持不变，而不是每一层的分布都在发生变化。 

## Layer Normalizaiton  
#### 方法介绍
针对BN的缺点，LN的提出针对解决这两个问题。  
LN是对**同一样本的所有不同特征维度**进行标准化（对不同的样本逐一进行计算）
- 方法  
  - 针对**每个样本**的**所有维度**计算方差，均值。进行标准化
    >和批标准化不同的是层标准化在训练和测试时执行同样的计算，即不需要再训练时保存每层的方差和均值。
    注意该步骤统计量的计算是和样本数量没有关系的，它的数量只取决于隐层节点的数量，所以只要隐层节点的数量足够多，我们就能保证LN的归一化统计量足够具有代表性。
  
  - 进行自适应增益和偏差计算。  
    >在LN中这组参数叫做增益（gain） g和偏置（bias） b（等同于BN中的 $\gamma$和$\beta$）

- 代码示例  
  ```
  def Layernorm(x, gamma, beta):
  # x_shape:[B, C, H, W]
    results = 0.
    eps = 1e-5
    x_mean = np.mean(x, axis=(1, 2, 3), keepdims=True)
    x_var = np.var(x, axis=(1, 2, 3), keepdims=True0)
    x_normalized = (x - x_mean) / np.sqrt(x_var + eps)
    results = gamma * x_normalized + beta
    return results
  ```
#### 和BN不同之处  
- BN对一个batch中的**不同样本的同一特征维度**标准化，而LN是对**同一样本的所有不同特征维度**进行标准化。因此不依赖于batch大小的影响。
- 和批标准化不同的是层标准化在训练和测试时执行同样的计算，即不需要再训练时保存每层的方差和均值。由此可摆脱固定网络深度的影响。
#### LN优点缺点
- 优点   
  LN 针对单个训练样本进行，不依赖于其他数据，因此可以避免 BN 中受 mini-batch 数据分布影响的问题，可以用于 小mini-batch场景、动态网络场景和 RNN，特别是自然语言处理领域。此外，LN 不需要保存 mini-batch 的均值和方差，节省了额外的存储空间。
- 缺点  
  BN 的转换是针对单个特征通道，那么不同特征通道的输入经过再平移和再缩放后分布在不同的区间，而 LN 对于所有特征通道，所有的输入都在同一个区间范围内。如果不同输入特征不属于相似的类别（比如颜色和大小），那么 LN 的处理可能会降低模型的表达能力。
#### 适用范围
- LN仅针对单个样本进行标准化，可能不适用于依赖数据整体分布特点来进行学习的数据。比如CNN中的判别模型。（个人推测）
- LN用于RNN效果比较明显，但是在CNN上，不如BN。

## Instance Normalization  
IN是逐个样本，逐个特征维度进行标准化。
> BN注重对每个batch进行归一化，保证数据分布一致，因为判别模型中结果取决于数据整体分布。
但是**图像风格化**中，生成结果主要依赖于某个图像实例，所以对整个batch归一化不适合，因而对HW做归一化。可以加速模型收敛，并且保持每个图像实例之间的独立。
- 方法代码  
  ```
  def Instancenorm(x, gamma, beta):
    # x_shape:[B, C, H, W]
    results = 0.
    eps = 1e-5
    x_mean = np.mean(x, axis=(2, 3), keepdims=True)
    x_var = np.var(x, axis=(2, 3), keepdims=True)
    x_normalized = (x - x_mean) / np.sqrt(x_var + eps)
    results = gamma * x_normalized + beta
    return result
    ```
## BN,LN,IN总结  
- **方法总结**  
  BN如图所示，它是取不同样本的同一个通道的特征做标准化；  
  LN则是如图所示，它取的是同一个样本的不同通道做标准化。  
  IN则是取同一样本的同一通道进行标准化
![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/17196290-08c9aa4f6ab47e37.png)   
	- **图解**  
	  深度网络中的数据维度一般是[N, C, H, W]或者[N, H, W，C]格式。图中C代表通道;N代表样本数，即batchsize;H,W代表特征图的高和宽。压缩H/W至一个维度，其三维的表示如上图。
 - **适用范围**：
   - **BN与IN**：  
     在图片视频分类等特征提取网络中大多数情况BN效果优于IN，在生成式类任务中的网络IN优于BN。
     - **BN适用于判别模型**，因为BN注重对每个batch进行标准化，从而保证**数据分布的一致性**，而判别模型的结果正是取决于数据整体分布
     - **IN适用于生成模型**，比如图片风格迁移，GAN。因为IN是对单个样本进行标准化，而生成模型中的结果主要依赖于某个图像实例，单个图像具有较强独立性，不与batch中其他样本有太大联系。
    - **BN与LN**：
       - BN对单个通道进行标准化，可以用于各个特征通道不属于相似类别的样本。  
         BN对每个batch进行标准化，对batchsize的大小比较敏感，batchsize较大时，效果较好。而如果batchsize太小，则计算的均值、方差不足以代表整个数据分布。
       - LN不适用于各个特征通道属于不相似类别的样本。LN对所有特征通道标准化，所有的输入都在同一个区间范围内。如果不同输入特征不属于相似的类别（比如颜色和大小），那么 LN 的处理可能会降低模型的表达能力。  
         LN对单个样本进行标准化，因此可以避免 BN 中受 mini-batch 数据分布影响的问题，可以用于 小mini-batch场景、动态网络场景和 RNN，特别是自然语言处理领域。
----
#### 问题
1. 什么时候分别对不同特征通道标准化，什么时候可以将不同特征通道一起标准化  
   答：当特征通道分别独立时对不同通道进行标准化？各个特征通道相关联的时候一起进行标准化？（个人猜测），具体例子？
2. 什么时候可以对所有样本一起标准化，什么时候分别对单个数据进行标准化？  
   答：BN注重对每个batch进行归一化，保证数据分布一致，因为判别模型中结果取决于数据整体分布。  
   但是**图像风格化**中，生成结果主要依赖于某个图像实例，所以对整个batch归一化不适合图像风格化中，因而对HW做归一化。可以加速模型收敛，并且保持每个图像实例之间的独立。
3. 为什么要标准化

## 深度学习   

### 各种卷积
- **卷积前后尺寸计算**  
  N = (W − F + 2P )/S+1  
  -  N为卷积后尺寸大小，W为卷积前尺寸大小，S为步长，P为padding的像素数，F为卷积核大小  
  - 计算卷积后map尺寸时若不为整数则向下取整，而计算pooling后尺寸时则向上取整。  
  - 例子  
    128x128的特征图，padding='SAME'，卷积核为3x3,步长为2。卷积后大小为：$N=\frac {(128-3+2)}{2}+1=64.5$，向上取整为64，故卷积后为64x64  
- **卷积padding**  
    越是边缘的像素点，对于输出的影响越小，因为卷积运算在移动的时候到边缘就结束了。中间的像素点有可能会参与多次计算，但是边缘像素点可能只参与一次。所以我们的结果可能会丢失边缘信息。  
    那么为了解决这个问题，我们引入padding， 什么是padding呢，就是我们认为的**扩充图片， 在图片外围补充一些像素点，把这些像素点初始化为0.**   
- **感受野计算**  
  - 计算公式  
    $$r_n = r_{n-1}*k_n-(k_n-1)*(r_{n-1}-\prod_{i=1}^{n-1}s_i)，n>=2$$   
    
    等价于$r_n=r_{n-1}+(k-1)\prod_{i=1}^{n-1}s_i$  
    
    上式中$k_n,s_n,r_n$分别代表第n层的kernel_size，stride，receptive_field。  
  - 图   
		    ![enter image description here](https://lh3.googleusercontent.com/Zw58SX4xvxf6r0x6FxGBoNJ3GLpwY1wyY6axWkBGJhO4Cydijq5KYEzjMtLl4ihSTsaSVwYsN5c3)    
- **池化**   
  - 池化作用  
    1. 不变性，这种不变性包括translation(平移)，rotation(旋转)，scale(尺度)，池化捕捉的恰好是目标的特征，并不是目标所在的位置，因此增加了平移不变性。   
    2. 保留主要的特征同时减少参数(降维，效果类似PCA)和计算量，可以去除卷积重叠操作中的冗余信息，防止过拟合，提高模型泛化能力。    
    3. 提高感受野大小   
  - 重叠池化和不重叠池化   
    AlexNet中用到了Overlapping pooling，  
  
### 过拟合解决方法   
-  **正则化**     
   - L1   
     在原始的代价函数后面加上一个L1正则化项，即所有权重w的绝对值的和，乘以λ/n。    
     $$C=C_0+\frac{\lambda}{n}\sum_{\omega}|\omega|$$   
     先计算导数：  
     $$\frac {\partial C}{\partial \omega}=\frac {\partial C_0}{\partial \omega}+\frac {\lambda}{n}sgn(\omega)$$    
     上式中sgn(w)表示 w 的符号函数，那么权重w的更新规则为：   
     $$\omega\rightarrow\omega^{'}=\omega-\frac {\eta\lambda}{n}sgn(\omega)-\eta\frac{\partial C_0}{\partial \omega}$$    
     其中$\eta$为学习率，比原始的更新规则多出了这一项。   
     
     当w为正时，sgn(w)>0, 则更新后的w变小。   
     当w为负时，sgn(w)>0, 则更新后的w变大——因此它的效果就是让w往0靠，使网络中的权重尽可能为0，也就相当于减小了网络复杂度，防止过拟合。   

	- L2     
	   全部參数 w 的平方和，除以训练集的样本大小n,  λ 就是正则项系数。   
	   $$C=C_0+\frac {\lambda}{2n}\sum_\omega \omega^2$$   
	   求导：   
	   $$\frac{\partial C}{\partial \omega}=\frac{\partial C_0}{\partial \omega}+\frac {\lambda}{n}\omega$$    
	   
	   $$\frac{\partial C}{\partial b} = \frac{\partial C_0}{\partial b}$$   
	    能够发现L2正则化项对 b 的更新没有影响，可是对于w的更新有影响:    
	    $$\omega\rightarrow\omega-\frac {\eta\lambda}{n}\omega-\eta\frac{\partial C_0}{\partial \omega}=(1-\frac {\eta\lambda}{n})\omega-\eta\frac{\partial C_0}{\partial \omega}$$      
	    在不使用L2正则化时。求导结果中 w 前系数为 1，经变化后w前面系数为 1−ηλ/n ，由于η、λ、n都是正的。所以 1−ηλ/n小于1，它的效果是减小w，这也就是权重衰减（weight decay）的由来。   
	    
	    **可以防止过拟合原因**：更小的权值w，从某种意义上说，表示网络的复杂度更低，对数据的拟合刚刚好（这个法则也叫做奥卡姆剃刀）。不管什么任务，过拟合都可能是由于特征过多造成的，通过正则项减少特征或者惩罚不重要特征的权重来解决过拟合，即特征的权重也会成为模型损失函数的一部分。   
	    
	    L1就是按绝对值函数的“坡”下降的，而L2是按二次函数的“坡”下降。所以实际上在0附近，L1的下降速度比L2的下降速度要快，所以会非常快得降到0。   
	    
	    L1称Lasso，L2称Ridge。   
	    总结就是：L1会趋向于产生少量的特征，而其他的特征都是0，而L2会选择更多的特征，这些特征都会接近于0。Lasso在特征选择时候非常有用，而Ridge就只是一种规则化而已。   
	 - 两者区别   
	   - L2 norm相当于在权重前乘以了个衰减系数，而L1 norm相当于减去固定常数方法，所以越大的数L2的惩罚性越大。使用L2 Norm求出来的解是比较均匀的会产生更多地特征但是都会接近于0，L1 会产生稀疏的特征。      
	   - L1 Norm对outlier没有L2 Norm那么敏感    
	   - L1计算效率低，L2计算效率高
	   - L1 nrom几乎没有比L2 norm表现好的时候，优先使用L2 norm是比较好的选择
- **数据增强**  
   对图片进行翻，随机剪切，或加入随机噪声
- **Dropout**    
   将神经网络某层的输出节点随机置零。   
   1. 首先随机（临时）删掉网络中一半的隐藏神经元，输入输出神经元保持不变   
   2. 然后把输入x通过修改后的网络前向传播，然后把得到的损失结果通过修改的网络反向传播。一小批训练样本执行完这个过程后，在没有被删除的神经元上按照随机梯度下降法更新对应的参数（w，b）。   
   3. 然后继续重复这一过程：  
	   恢复被删掉的神经元（此时被删除的神经元保持原样，而没有被删除的神经元已经有所更新）  
	   从隐藏层神经元中随机选择一个一半大小的子集临时删除掉（备份被删除神经元的参数）。   
	   对一小批训练样本，先前向传播然后反向传播损失并根据随机梯度下降法更新参数（w，b） （没有被删除的那一部分参数得到更新，删除的神经元参数保持被删除前的结果）。   
	   不断重复这一过程。  

### 梯度弥散和梯度爆炸，以及如何解决：   
- 梯度弥散   
   这本质上是由于激活函数的选择导致的， 最简单的sigmoid函数为例，在函数的两端梯度求导结果非常小（饱和区），当网络层数较多时，导致反向传播过程中由于多次用到激活函数的导数值使得整体的乘积梯度结果变得越来越小，也就出现了梯度消失的现象。这种情况下根据训练数据反馈更新网络参数就非常慢。      
   
   解决方法：使用ReLU激活函数，加入BN层，残差网络   
 
- **梯度爆炸**    

    解决方法：   
    1. 梯度剪切这个方案主要是针对梯度爆炸提出的，其思想是设置一个梯度剪切阈值，然后更新梯度的时候，如果梯度超过这个阈值，那么就将其强制限制在这个范围之内。这可以防止梯度爆炸。   
    2. 加入正则项。发生梯度爆炸往往权值的范数就会变得非常大，通过正则项约束权值，可以限制部分梯度爆炸。   
    3. ReLU 激活函数的导数为1，那么就不存在梯度消失爆炸的问题了  
    4. Batchnorm,就是通过对每一层的输出规范为均值和方差一致的方法，消除了w带来的放大缩小的影响    
    5. 残差网络  

### **反向传播算法原理：**     
- **前向传播计算：**      
   ![enter image description here](https://lh3.googleusercontent.com/c_e2XMHLasFaXAmkv4WGSyeM8YTS57TYJaYrAX39C4wr_eKMZaL4hnf8kSbM4H5c1Kx3gunAfj3e)    
   ![enter image description here](https://lh3.googleusercontent.com/gw35prtBllNi8j-myQLaH_rCS-CdaHHcoekmtv7Ik7ZnTw6tVrayn9HJsOW3-9rI7FdO3Yg3HZhK)
- **反向传播误差**：   
  ![enter image description here](https://lh3.googleusercontent.com/keLzwAAGhwGvAdv_Jlo9QelTZEVpyUlkIU0uJUAKSvZGPBhbpursEKeq62_QnP47DkiUg6p3_m6W)   
  ![enter image description here](https://lh3.googleusercontent.com/5yz5CaGI5Aswbn_9CmmRH0psJqHrl320Ej0IXqvieCicbSGh5mqX8c4Xz6TUKPRjazNzj8kPHXFc)     
- **输出层权重参数更新：**   
    ![enter image description here](https://lh3.googleusercontent.com/SIWy2yYtYLfOfK-zeJGDCjN8LhGqpv2YFSHOfX-TYuXmTPdvukxTk4t7QXbIMH7WOQzGbTIgOgOS)     
- **隐藏层权重更新：**   
   ![enter image description here](https://lh3.googleusercontent.com/UimQLK-RNufNRWshBBN3b703cStHT71NwmoLIk4IjjcHY4O_MqXtIwNtoHar29FmidhjmwHBn5If)    
   所以：   
   ![enter image description here](https://lh3.googleusercontent.com/EAwzgVNameq0MVR8xZN5KhPe2NqCCsZ4rBFpsha3oBXCTn6N8g50AGB8dkvq3xCCC_FbFL51woXP)    
   其中：  
   ![enter image description here](https://lh3.googleusercontent.com/2_0C0RLcTa-v4eFKMLIYrTLQDV6s7IzPwI0jCupHlylI1EQq3hTBHH8tuwpuymLXY_Z5X3BUoZlP)
参考： [https://blog.csdn.net/qq_32865355/article/details/80260212]   
- 卷积和池化的反向传播：   

### **权重初始化**   
   参数初始化的目的是为了让神经网络在训练过程中学习到有用的信息，这意味着参数梯度不应该为0。而我们知道在全连接的神经网络中，参数梯度和反向传播得到的状态梯度以及入激活值有关——激活值饱和会导致该层状态梯度信息为0，然后导致下面所有层的参数梯度为0；入激活值为0会导致对应参数梯度为0。所以如果要保证参数梯度不等于0，那么参数初始化应该使得各层激活值不会出现饱和现象且激活值不为0。我们把这两个条件总结为参数初始化条件    
   1.初始化必要条件一：各层激活值不会出现饱和现象。   
   2.初始化必要条件二：各层激活值不为0。    
 - **全零初始化**   
      tf.constant_initializer(0) 或 tf.zeros_initializer()   
      是不是把参数都初始化为0会是比较好的初始化？这样做其实会带来一个问题，经过正向传播和反向传播后，参数的不同维度之间经过相同的更新，迭代的结果是不同维度的参数是一样的，严重地影响了模型的性能。   
      权重初始化为0是件很糟糕的事。
 
 -  **高斯分布/截断正态分布**    
   **简介**：高斯分布为均值为0，方差为1 的标准高斯分布。从截断的正态分布中输出随机值。通常初始化时，均值为零，标准差为0.1。（如果生成的值大于平均值2个标准偏差的值则丢弃重新选择—截断正态分布--tf.constant_initializer；普通正太分布--tf.random_normal）   
   **好处**：使用ReLU激活函数时，使用正太分布可以使参数加一点噪声，打破完全对称并且避免0梯度。有时还需给偏置赋上一些小的非零值来避免dead neuron(死亡神经) 例tf.constant_initializer(0.1, shape=shape)    
   **缺点：**（待深究）如果初始的方差小，如0.1，就会导致在前向传播过程中，不同的层的输入不断减小。这会导致很多问题，因为权重的梯度跟输入息息相关。在前面的文章可以看到很多时候w在这一层的梯度就等于输入x。那这样子会导致权重的更新速度很慢很慢。   
    - 那初始化的方差如果太大呢，就会使得每一层的输出越来越大。形如tanh激活函数，就会容易导致梯度饱和的现象
### **CNN****参数计算以及时间复杂度**    
<!--stackedit_data:
eyJoaXN0b3J5IjpbOTEyMDUxNTAyLC0xNTcyOTUyNjM0LC0xOT
MwMjA4NzIyLC0xMDQzNzgxODQyLDE3MzczOTkwNjcsNzgyNTEw
MjA2LC0xMzMyNDc0MDQwLC0xMjUxNTU3NTkwLDEzNzU1MzE1Mj
EsNDc5NDkyNzE2LC00MjY4MTMxMDEsMTAzMzk1MTU5OCw1MTcx
MTQ5NzksMTA2ODM0MzkyMCw1MzE5OTIzNCwtOTA4NjIyNTUzLD
czNDg4NDEyNiw3OTkyNzUzOTEsLTE0MTM4MDM0NDQsNzY0MTIw
Njg0XX0=
-->