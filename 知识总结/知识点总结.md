## 数据预处理
- **标准化**  
  数据标准化：**均值去除 和 按方差比例缩放**
  当单个特征的样本取值相差甚大或明显不遵从高斯正态分布时，标准化表现的效果较差。实际操作中，经常忽略特征数据的分布形状，移除每个特征均值，划分离散特征的标准差，从而等级化，进而实现数据中心化。  
  - 特点  
  对不同特征维度的伸缩变换的目的是使得不同度量之间的特征具有可比性。同时不改变原始数据的分布。  
  - 好处  
    1 使得不同度量之间的特征具有可比性，对目标函数的影响体现在几何分布上，而不是数值上    
    2 不改变原始数据的分布
   - **代码实现**  
     - sklearn preprocessing  
       1. `preprocessing.scale(X)`相当于z-score 标准化(zero-mean normalization)  
	          ```
	          def scale(X, axis=0, with_mean=True, with_std=True,copy=True)```  
          参数解释：  
          X：{array-like, sparse matrix} 数组或者矩阵，一维的数据都可以（但是在0.19版本后一维的数据会报错了！）
    axis：int类型，初始值为0，axis用来计算均值 means 和标准方差 standard deviations.如果是0，则单独的标准化每个特征（列），如果是1，则标准化每个观测样本（行）。  
    with_mean: boolean类型，默认为True，表示将数据均值规范到0  
    with_std: boolean类型，默认为True，表示将数据方差规范到
       2.  `sklearn.preprocessing.StandardScaler()`  
          可保存训练集的标准化参数(均值、方差)，然后应用在转换测试集数据。 一般我们的标准化先在训练集上进行，在测试集上也应该做同样 mean 和 variance 的标准化
	          ```
	          scaler = preprocessing.StandardScaler().fit(X)
	          ```
      - 自定义  
          ```x_norm = (x-np.mean(x))/np.std(x)  ```
 - **归一化**  
   **将数据特征缩放至某一范围**  
   - 特点  
     对不同特征维度的伸缩变换的目的是使各个特征维度对目标函数的影响权重是一致的，即使得那些扁平分布的数据伸缩变换成类圆形。这也就改变了原始数据的一个分布。
    - 好处  
      **两个特征区间相差非常大时，形成的等高线偏椭圆，迭代时很有可能走“之字型”路线（垂直长轴），从而导致需要迭代很多次才能收敛。而对两个特征进行了归一化，对应的等高线就会变圆，在梯度下降进行求解时能较快的收敛。**
       1 提高迭代求解的收敛速度
       2 提高迭代求解的精度
    - **代码实现**  
      - sklearn preprocessing  
        ```
        min_max_scaler = preprocessing.MinMaxScaler()
        X_train_minmax = min_max_scaler.fit_transform(X_train)
        ``` 
      - 自定义  
        ```x_norm = (x-np.min(x))/(np.max(x)-np.min(x))  ```
 - **两者区别**  
   参考[https://www.zhihu.com/question/20467170/answer/222792995](https://www.zhihu.com/question/20467170/answer/222792995)
 - **归一化问题**
   - 对每一个样本进行进行归一化（按行归一化）还是对每一个维度进行归一化（按列归一化）？  
     **对每一个维度进行归一化**。对于每个样本，由于它的每一个维度的量纲不同，若对每一个样本进行归一化且在量纲数量级差别悬殊时会使样本中较低数量级的属性变为0，会使原始信息过多丧失。
   - 对训练集归一化后再映射到测试集归一化，还是训练集测试集一起归一化？  
     **应该训练集归一化后再映射到测试集归一化**
   - 不能对样本一个一个逐一归一化，要整个训练集一起归一化
--------------
### batchnorm  
#### batchnorm要解决的问题
- **内部协变量偏移（Internal Covariate Shift）**    
  训练深度网络的时候经常发生训练困难的问题，因为，每一次参数迭代更新后，上一层网络的输出数据经过这一层网络计算后，数据的分布会发生变化，为下一层网络的学习带来困难（神经网络本来就是要学习数据的分布，要是分布一直在变，学习就很难了），此现象称之为Internal Covariate Shift。
- 协变量偏移（covariate shift）  
  Internal Covariate Shift 和Covariate Shift具有相似性，但并不是一个东西，前者发生在神经网络的内部，所以是Internal，后者发生在输入数据上。Covariate Shift主要描述的是由于训练数据和测试数据存在分布的差异性，给网络的泛化性和训练速度带来了影响，我们经常使用的方法是做归一化或者白化。
#### batchnorm原理  
 直接对每层进行归一化会降低层的表达力，每一层的数据分布都是标准正太分布，导致其完全学习不到输入数据的特征。

	1.先求出此次批量数据x的均值，μ
	2.求出此次batch的方差，σ
	3.接下来就是对x做归一化，得到x−i  
	4.最重要的一步，引入缩放和平移变量γ和β ,计算归一化后的值，yi=γxi + β  

#### batchnorm优点
	1. 避免了梯度消失和梯度爆炸
	2. 加速网络的收敛
	3. 提高了网络的泛化能力。由于在训练的过程中批量标准化所用到的均值和方差是在一小批样本(mini-batch)上计算的，而不是在整个数据集上，所以均值和方差会有一些小噪声产生。
### 总结  
- **输入标准化**
调整样本数据每个维度的量纲，让每个维度数据量纲相同或接近。通常我们在使用数据之前，会对输入数据做了标准化处理：处理后的任意一个特征在数据集中所有样本上的均值为 0、标准差为 1。标准化处理输入数据使各个特征的分布相近：这往往更容易训练出有效的模型。因为两个特征区间相差非常大时，形成的等高线偏椭圆，迭代时很有可能走“之字型”路线（垂直长轴），从而导致需要迭代很多次才能收敛。而对两个特征进行了归一化，对应的等高线就会变圆，在梯度下降进行求解时能较快的收敛。
- **层间标准化（batchnorm）**  
1. （虽然输入数据标准化，但层间数据分布发生剧烈变化--自己的理解）每一次参数迭代更新后，上一层网络的输出数据经过这一层网络计算后，数据的分布会发生变化，为下一层网络的学习带来困难。
2. 随着网络深度加深或者在训练过程中，其分布逐渐发生偏移或者变动，之所以训练收敛慢，一般是整体分布逐渐往非线性函数的取值区间的上下限两端靠近（对于Sigmoid函数来说，意味着激活输入值WU+B是大的负值或正值），所以这导致反向传播时低层神经网络的梯度消失。BN通过归一化把每层神经网络任意神经元这个输入值的分布强行拉回到均值为0方差为1的标准正态分布，这样使得激活输入值落在非线性函数对输入比较敏感的区域，这样输入的小变化就会导致损失函数较大的变化。**经过BN后，目前大部分Activation的值落入非线性函数的线性区内，其对应的导数远离导数饱和区，这样来加速训练收敛过程**
在层间加入标准后可以解决分布变化太剧烈的问题，但单纯的对每一层进行归一化就会使得每层分布区间完全相同。将大部分ctivation的值落入非线性函数的线性区内，使得非线性函数替换成线性函数效果相同了，深度网络就失去了意义，多层网络等同于了一层网络的线性表达。**所以BN为了保证非线性的获得，对变换后的满足均值为0方差为1的x又进行了scale加上shift操作**，这两个参数是通过训练学习到的，达到一个一个线性和非线性的较好平衡点。
<!--stackedit_data:
eyJoaXN0b3J5IjpbMTMxNTYxNzc5LDM3MjE4NzU3NiwxOTQzMz
M5NjY1LC0xNjYyOTQyOTg0LC0yNDkyNTkyNTcsLTg0OTUzODAz
NiwtMTMxMDQzMjMxNSwtOTQ1MzI1ODkxLC0xNDQ2Nzk0NTQsLT
g3OTU3NTUzOCwtNzEyMTgxNDAxLDEyNTQwMjk0MTksLTIwODg3
NDY2MTIsMTg0NDI5NTkzNCwtOTkzNTMwNDA3LDcwNDMwMDY2Ni
wtNDEyOTgxMzksMTI1MDY2NzgyNiwxMTQwOTcwMjc1LDcyNzky
MDI4MF19
-->