## 数据预处理
- **标准化**  
  数据标准化：**均值去除 和 按方差比例缩放**
  当单个特征的样本取值相差甚大或明显不遵从高斯正态分布时，标准化表现的效果较差。实际操作中，经常忽略特征数据的分布形状，移除每个特征均值，划分离散特征的标准差，从而等级化，进而实现数据中心化。  
  - 特点  
  对不同特征维度的伸缩变换的目的是使得不同度量之间的特征具有可比性。同时不改变原始数据的分布。  
  - 好处  
    1 使得不同度量之间的特征具有可比性，对目标函数的影响体现在几何分布上，而不是数值上    
    2 不改变原始数据的分布
   - **代码实现**  
     - sklearn preprocessing  
       1. `preprocessing.scale(X)`相当于z-score 标准化(zero-mean normalization)  
	          ```
	          def scale(X, axis=0, with_mean=True, with_std=True,copy=True)```  
          参数解释：  
          X：{array-like, sparse matrix} 数组或者矩阵，一维的数据都可以（但是在0.19版本后一维的数据会报错了！）
    axis：int类型，初始值为0，axis用来计算均值 means 和标准方差 standard deviations.如果是0，则单独的标准化每个特征（列），如果是1，则标准化每个观测样本（行）。  
    with_mean: boolean类型，默认为True，表示将数据均值规范到0  
    with_std: boolean类型，默认为True，表示将数据方差规范到
       2.  `sklearn.preprocessing.StandardScaler()`  
          可保存训练集的标准化参数(均值、方差)，然后应用在转换测试集数据。 一般我们的标准化先在训练集上进行，在测试集上也应该做同样 mean 和 variance 的标准化
	          ```
	          scaler = preprocessing.StandardScaler().fit(X)
	          ```
      - 自定义  
          ```x_norm = (x-np.mean(x))/np.std(x)  ```
 - **归一化**  
   **将数据特征缩放至某一范围**  
   - 特点  
     对不同特征维度的伸缩变换的目的是使各个特征维度对目标函数的影响权重是一致的，即使得那些扁平分布的数据伸缩变换成类圆形。这也就改变了原始数据的一个分布。
    - 好处  
      **两个特征区间相差非常大时，形成的等高线偏椭圆，迭代时很有可能走“之字型”路线（垂直长轴），从而导致需要迭代很多次才能收敛。而对两个特征进行了归一化，对应的等高线就会变圆，在梯度下降进行求解时能较快的收敛。**
       1 提高迭代求解的收敛速度
       2 提高迭代求解的精度
    - **代码实现**  
      - sklearn preprocessing  
        ```
        min_max_scaler = preprocessing.MinMaxScaler()
        X_train_minmax = min_max_scaler.fit_transform(X_train)
        ``` 
      - 自定义  
        ```x_norm = (x-np.min(x))/(np.max(x)-np.min(x))  ```
 - **两者区别**  
   参考[https://www.zhihu.com/question/20467170/answer/222792995](https://www.zhihu.com/question/20467170/answer/222792995)
 - **归一化问题**
   - 对每一个样本进行进行归一化（按行归一化）还是对每一个维度进行归一化（按列归一化）？  
     **对每一个维度进行归一化**。对于每个样本，由于它的每一个维度的量纲不同，若对每一个样本进行归一化且在量纲数量级差别悬殊时会使样本中较低数量级的属性变为0，会使原始信息过多丧失。
   - 对训练集归一化后再映射到测试集归一化，还是训练集测试集一起归一化？  
     **应该训练集归一化后再映射到测试集归一化**
   - **不能对样本一个一个逐一归一化，要整个训练集一起归一化**
- **标准化/白化问题**  
   为什么白化可以加快训练呢 ？ 
   - 参数初始化都是位于零左右的值，而白化后可以将数据分布拉回零范围左右，无疑可以加快学习的速度。更进一步的话我们对数据再进行去相关操作，使得数据更加容易区分，这样又会加快训练。
   参考：[https://blog.csdn.net/shenziheng1/article/details/81254206](https://blog.csdn.net/shenziheng1/article/details/81254206)
## batchnorm (批标准化) 
#### batchnorm要解决的问题
- **内部协变量偏移（Internal Covariate Shift）**    
  训练深度网络的时候经常发生训练困难的问题，因为，每一次参数迭代更新后，上一层网络的输出数据会发生变化，为下一层网络的学习带来困难（神经网络本来就是要学习数据的分布，要是分布一直在变，学习就很难了），层层叠加，深层的输入分布变化会非常剧烈，这就使得深层需要不断去重新适应底层的参数更新。此现象称之为Internal Covariate Shift。  
  >每一次参数更新都是根据前一层
- 协变量偏移（covariate shift）  
  Internal Covariate Shift 和Covariate Shift具有相似性，但并不是一个东西，前者发生在神经网络的内部，所以是Internal，后者发生在输入数据上。Covariate Shift主要描述的是由于训练数据和测试数据存在分布的差异性，给网络的泛化性和训练速度带来了影响，我们经常使用的方法是做归一化或者白化。
#### batchnorm原理  
 直接对每层进行归一化会降低层的表达力，每一层的数据分布都是标准正太分布，导致其完全学习不到输入数据的特征。

	1.先求出此次批量数据x的均值，μ
	2.求出此次batch的方差，σ
	3.接下来就是对x做归一化，得到x−i  
	4.最重要的一步，引入缩放和平移变量γ和β ,计算归一化后的值，yi=γxi + β  
#### **计算方式**  
  **逐channel地计算**同一batch中所有数据的mean和variance，再对input使用mean和variance进行归一化，最后的输出再进行线性平移，得到batch_norm的最终结果。  
  ```
  for i in range(channel):
	  x = input[:,:,:,i]
	  mean = mean(x)
	  variance = variance(x)
	  x = (x - mean) / sqrt(variance)
	  x = scale * x + offset
	  input[:,:,:,i] = x
  ```  
  #### 均值和方差的更新方式   
  - train阶段每个batch计算的mean/variance采用**指数加权平均**来得到test阶段mean/variance的估计  
	  ```  
	  1.moving_mean= momentum * moving_mean+ (1 - momentum) * x_mean
	  2.moving_var= momentum * moving_var+ (1 - momentum) * x_var
	  ```  
	  - 默认初始值：momentum默认初始值为0.99  
	  - $\gamma,\beta$是训练中学习到的？
#### batchnorm优点
1. 避免了梯度消失和梯度爆炸
2. 加速网络的收敛
3. 提高了网络的泛化能力。由于在训练的过程中批量标准化所用到的均值和方差是在一小批样本(mini-batch)上计算的，而不是在整个数据集上，所以均值和方差会有一些小噪声产生。
#### batchnorm缺点
1. batchsize的问题  
   对batchsize的大小比较敏感，由于每次计算均值和方差是在一个batch上，所以batchsize较大时，效果较好。而如果batchsize太小，则计算的均值、方差不足以代表整个数据分布；
2. batchnorm和RNN  
   BN实际使用时需要计算并且保存某一层神经网络batch的均值和方差等统计信息，对于对一个固定深度的前向神经网络（DNN，CNN）使用BN，很方便；但对于RNN来说，sequence的长度是不一致的这样training时，计算很麻烦。
#### **batchnorm使用**
参考：
以下参考皆是tf.layers.batch_normalization
1. 详细train和test中的使用 [https://zhuanlan.zhihu.com/p/34879333] 
2. [https://github.com/udacity/cn-deep-learning/blob/master/tutorials/batch-norm/Batch_Normalization_Solutions.ipynb]


## 总结  
- **输入标准化**
调整样本数据每个维度的量纲，让每个维度数据量纲相同或接近。通常我们在使用数据之前，会对输入数据做了标准化处理：处理后的任意一个特征在数据集中所有样本上的均值为 0、标准差为 1。标准化处理输入数据使各个特征的分布相近：这往往更容易训练出有效的模型。因为两个特征区间相差非常大时，形成的等高线偏椭圆，迭代时很有可能走“之字型”路线（垂直长轴），从而导致需要迭代很多次才能收敛。而对两个特征进行了归一化，对应的等高线就会变圆，在梯度下降进行求解时能较快的收敛。
  - 样本间归一化 和特征间归一化  
    如果进行样本间归一化，即每个样本逐一归一化，那么每个样本会被统一到一个量纲下，样本间差异就会损失，是不合理的。而需要进行的是特征间归一化，能将每个维度特征的量纲相同或相近。不同维度特征的量纲差距大，这样每个维度的影响不一样（如1.0m和1000cm）。不同维度差距大，在超空间中，会形成超椭球的形状。
- **层间标准化（batchnorm批标准化）**  
1. （虽然输入数据标准化，但层间数据分布发生剧烈变化--自己的理解）每一次参数迭代更新后，上一层网络的输出数据经过计算后，数据的分布会发生变化，为下一层网络的学习带来困难。
2. 随着网络深度加深或者在训练过程中，其分布逐渐发生偏移或者变动，之所以训练收敛慢，一般是整体分布逐渐往非线性函数的取值区间的上下限两端靠近（对于Sigmoid函数来说，意味着激活输入值WU+B是大的负值或正值），所以这导致反向传播时低层神经网络的梯度消失。BN通过归一化把每层神经网络任意神经元这个输入值的分布强行拉回到均值为0方差为1的标准正态分布，这样使得激活输入值落在非线性函数对输入比较敏感的区域，这样输入的小变化就会导致损失函数较大的变化。**经过BN后，目前大部分Activation的值落入非线性函数的线性区内，其对应的导数远离导数饱和区，这样来加速训练收敛过程**  
   >一，浅层参数需要不断适应新的输入数据分布，降低学习速度。  
   >二，浅层输入的变化可能趋向于变大或者变小，导致深层落入饱和区，使得学习过早停止，参数无法得到有效更新。而且落入饱和区，无论输入值之间差距多大，激活后都是接近1的值，失去了对数据的敏感性。
	
	- 平移缩放的原因  
	  为了保证模型的表达能力不因为规范化而下降
	  - 在层间加入标准后可以解决分布变化太剧烈的问题，但单纯的对每一层进行归一化就会使得每层分布区间完全相同。将大部分ctivation的值落入非线性函数的线性区内，使得非线性函数替换成线性函数效果相同了，深度网络就失去了意义，多层网络等同于了一层网络的线性表达。**所以BN为了保证非线性的获得，对变换后的满足均值为0方差为1的x又进行了scale加上shift操作**，这两个参数是通过训练学习到的，达到一个一个线性和非线性的较好平衡点。  
		- 要点: 缩放平移操作是为了 1.充分利用前面层学习的能力，自适应地去调整层特征分布  2.增强非线性表达能力。

- 为什么进行白化和batchnorm  
  -    为什么白化可以加快训练呢 ？  
    参数初始化都是位于零左右的值，而白化后可以将数据分布拉回零范围左右，无疑可以加快学习的速度。更进一步的话我们对数据再进行去相关操作，使得数据更加容易区分，这样又会加快训练。
   -  假设条件  
     为了有效地学习神经网络，神经网络的每一层分布都应该：均值为0、始终保持相同的分布（和参数初始化为零左右的值有关？）；
     第二个条件意味着通过批梯度下降输入到网路层的数据分布不应该变化太多，并且随着训练的进行它应该保持不变，而不是每一层的分布都在发生变化。 

## Layer Normalizaiton  
#### 方法介绍
针对BN的缺点，LN的提出针对解决这两个问题。  
LN是对**同一样本的所有不同特征维度**进行标准化（对不同的样本逐一进行计算）
- 方法  
  - 针对**每个样本**的**所有维度**计算方差，均值。进行标准化
    >和批标准化不同的是层标准化在训练和测试时执行同样的计算，即不需要再训练时保存每层的方差和均值。
    注意该步骤统计量的计算是和样本数量没有关系的，它的数量只取决于隐层节点的数量，所以只要隐层节点的数量足够多，我们就能保证LN的归一化统计量足够具有代表性。
  
  - 进行自适应增益和偏差计算。  
    >在LN中这组参数叫做增益（gain） g和偏置（bias） b（等同于BN中的 $\gamma$和$\beta$）

- 代码示例  
  ```
  def Layernorm(x, gamma, beta):
  # x_shape:[B, C, H, W]
    results = 0.
    eps = 1e-5
    x_mean = np.mean(x, axis=(1, 2, 3), keepdims=True)
    x_var = np.var(x, axis=(1, 2, 3), keepdims=True0)
    x_normalized = (x - x_mean) / np.sqrt(x_var + eps)
    results = gamma * x_normalized + beta
    return results
  ```
#### 和BN不同之处  
- BN对一个batch中的**不同样本的同一特征维度**标准化，而LN是对**同一样本的所有不同特征维度**进行标准化。因此不依赖于batch大小的影响。
- 和批标准化不同的是层标准化在训练和测试时执行同样的计算，即不需要再训练时保存每层的方差和均值。由此可摆脱固定网络深度的影响。
#### LN优点缺点
- 优点   
  LN 针对单个训练样本进行，不依赖于其他数据，因此可以避免 BN 中受 mini-batch 数据分布影响的问题，可以用于 小mini-batch场景、动态网络场景和 RNN，特别是自然语言处理领域。此外，LN 不需要保存 mini-batch 的均值和方差，节省了额外的存储空间。
- 缺点  
  BN 的转换是针对单个特征通道，那么不同特征通道的输入经过再平移和再缩放后分布在不同的区间，而 LN 对于所有特征通道，所有的输入都在同一个区间范围内。如果不同输入特征不属于相似的类别（比如颜色和大小），那么 LN 的处理可能会降低模型的表达能力。
#### 适用范围
- LN仅针对单个样本进行标准化，可能不适用于依赖数据整体分布特点来进行学习的数据。比如CNN中的判别模型。（个人推测）
- LN用于RNN效果比较明显，但是在CNN上，不如BN。

## Instance Normalization  
IN是逐个样本，逐个特征维度进行标准化。
> BN注重对每个batch进行归一化，保证数据分布一致，因为判别模型中结果取决于数据整体分布。
但是**图像风格化**中，生成结果主要依赖于某个图像实例，所以对整个batch归一化不适合，因而对HW做归一化。可以加速模型收敛，并且保持每个图像实例之间的独立。
- 方法代码  
  ```
  def Instancenorm(x, gamma, beta):
    # x_shape:[B, C, H, W]
    results = 0.
    eps = 1e-5
    x_mean = np.mean(x, axis=(2, 3), keepdims=True)
    x_var = np.var(x, axis=(2, 3), keepdims=True)
    x_normalized = (x - x_mean) / np.sqrt(x_var + eps)
    results = gamma * x_normalized + beta
    return result
    ```
## BN,LN,IN总结  
- **方法总结**  
  BN如图所示，它是取不同样本的同一个通道的特征做标准化；  
  LN则是如图所示，它取的是同一个样本的不同通道做标准化。  
  IN则是取同一样本的同一通道进行标准化
![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/17196290-08c9aa4f6ab47e37.png)   
	- **图解**  
	  深度网络中的数据维度一般是[N, C, H, W]或者[N, H, W，C]格式。图中C代表通道;N代表样本数，即batchsize;H,W代表特征图的高和宽。压缩H/W至一个维度，其三维的表示如上图。
 - **适用范围**：
   - **BN与IN**：  
     在图片视频分类等特征提取网络中大多数情况BN效果优于IN，在生成式类任务中的网络IN优于BN。
     - **BN适用于判别模型**，因为BN注重对每个batch进行标准化，从而保证**数据分布的一致性**，而判别模型的结果正是取决于数据整体分布
     - **IN适用于生成模型**，比如图片风格迁移，GAN。因为IN是对单个样本进行标准化，而生成模型中的结果主要依赖于某个图像实例，单个图像具有较强独立性，不与batch中其他样本有太大联系。
    - **BN与LN**：
       - BN对单个通道进行标准化，可以用于各个特征通道不属于相似类别的样本。  
         BN对每个batch进行标准化，对batchsize的大小比较敏感，batchsize较大时，效果较好。而如果batchsize太小，则计算的均值、方差不足以代表整个数据分布。
       - LN不适用于各个特征通道属于不相似类别的样本。LN对所有特征通道标准化，所有的输入都在同一个区间范围内。如果不同输入特征不属于相似的类别（比如颜色和大小），那么 LN 的处理可能会降低模型的表达能力。  
         LN对单个样本进行标准化，因此可以避免 BN 中受 mini-batch 数据分布影响的问题，可以用于 小mini-batch场景、动态网络场景和 RNN，特别是自然语言处理领域。
----
#### 问题
1. 什么时候分别对不同特征通道标准化，什么时候可以将不同特征通道一起标准化  
   答：当特征通道分别独立时对不同通道进行标准化？各个特征通道相关联的时候一起进行标准化？（个人猜测），具体例子？
2. 什么时候可以对所有样本一起标准化，什么时候分别对单个数据进行标准化？  
   答：BN注重对每个batch进行归一化，保证数据分布一致，因为判别模型中结果取决于数据整体分布。  
   但是**图像风格化**中，生成结果主要依赖于某个图像实例，所以对整个batch归一化不适合图像风格化中，因而对HW做归一化。可以加速模型收敛，并且保持每个图像实例之间的独立。
3. 为什么要标准化

## 深度学习   

### 各种卷积
- **卷积前后尺寸计算**  
  N = (W − F + 2P )/S+1  
  -  N为卷积后尺寸大小，W为卷积前尺寸大小，S为步长，P为padding的像素数，F为卷积核大小  
  - 计算卷积后map尺寸时若不为整数则向下取整，而计算pooling后尺寸时则向上取整。  
  - 例子  
    128x128的特征图，padding='SAME'，卷积核为3x3,步长为2。卷积后大小为：$N=\frac {(128-3+2)}{2}+1=64.5$，向上取整为64，故卷积后为64x64  
- **卷积padding**  
    越是边缘的像素点，对于输出的影响越小，因为卷积运算在移动的时候到边缘就结束了。中间的像素点有可能会参与多次计算，但是边缘像素点可能只参与一次。所以我们的结果可能会丢失边缘信息。  
    那么为了解决这个问题，我们引入padding， 什么是padding呢，就是我们认为的**扩充图片， 在图片外围补充一些像素点，把这些像素点初始化为0.**   
- **感受野计算**  
  - 计算公式  
    $$r_n = r_{n-1}*k_n-(k_n-1)*(r_{n-1}-\prod_{i=1}^{n-1}s_i)，n>=2$$   
    
    等价于$r_n=r_{n-1}+(k-1)\prod_{i=1}^{n-1}s_i$  
    
    上式中$k_n,s_n,r_n$分别代表第n层的kernel_size，stride，receptive_field。  
  - 图   
		    ![enter image description here](https://lh3.googleusercontent.com/Zw58SX4xvxf6r0x6FxGBoNJ3GLpwY1wyY6axWkBGJhO4Cydijq5KYEzjMtLl4ihSTsaSVwYsN5c3)    
- **池化**   
  - 池化作用  
    1. 不变性，这种不变性包括translation(平移)，rotation(旋转)，scale(尺度)，池化捕捉的恰好是目标的特征，并不是目标所在的位置，因此增加了平移不变性。   
    2. 保留主要的特征同时减少参数(降维，效果类似PCA)和计算量，可以去除卷积重叠操作中的冗余信息，防止过拟合，提高模型泛化能力。    
    3. 提高感受野大小   
  - 重叠池化和不重叠池化   
    AlexNet中用到了Overlapping pooling，  
  
### 过拟合解决方法   
-  **正则化**     
   - L1   
     在原始的代价函数后面加上一个L1正则化项，即所有权重w的绝对值的和，乘以λ/n。    
     $$C=C_0+\frac{\lambda}{n}\sum_{\omega}|\omega|$$   
     先计算导数：  
     $$\frac {\partial C}{\partial \omega}=\frac {\partial C_0}{\partial \omega}+\frac {\lambda}{n}sgn(\omega)$$
- 
  
<!--stackedit_data:
eyJoaXN0b3J5IjpbMTE0NzM1MTUxNywxMDY4MzQzOTIwLDUzMT
k5MjM0LC05MDg2MjI1NTMsNzM0ODg0MTI2LDc5OTI3NTM5MSwt
MTQxMzgwMzQ0NCw3NjQxMjA2ODQsMTkwNjUwMjQ5NCwtMTU0MT
M4ODAyOCw3NzY3MzExMDQsLTExNTE5NDMyNDQsLTE4MzMwMDU5
NTMsMTA3Njk4NzM1NiwyMTE4Mjk2Nzc2LC0xOTAwNTI0NjYzLC
0zOTgzMDQwODcsLTE5NDE2MzgyOTAsLTExMjQxNzExOTQsMTM4
MTE1NjY3N119
-->