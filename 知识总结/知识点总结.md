## 数据预处理
- **标准化**  
  数据标准化：**均值去除 和 按方差比例缩放**
  当单个特征的样本取值相差甚大或明显不遵从高斯正态分布时，标准化表现的效果较差。实际操作中，经常忽略特征数据的分布形状，移除每个特征均值，划分离散特征的标准差，从而等级化，进而实现数据中心化。  
  - 特点  
  对不同特征维度的伸缩变换的目的是使得不同度量之间的特征具有可比性。同时不改变原始数据的分布。  
  - 好处  
    1 使得不同度量之间的特征具有可比性，对目标函数的影响体现在几何分布上，而不是数值上    
    2 不改变原始数据的分布
   - **代码实现**  
     - sklearn preprocessing  
       1. `preprocessing.scale(X)`相当于z-score 标准化(zero-mean normalization)  
	          ```
	          def scale(X, axis=0, with_mean=True, with_std=True,copy=True)```  
          参数解释：  
          X：{array-like, sparse matrix} 数组或者矩阵，一维的数据都可以（但是在0.19版本后一维的数据会报错了！）
    axis：int类型，初始值为0，axis用来计算均值 means 和标准方差 standard deviations.如果是0，则单独的标准化每个特征（列），如果是1，则标准化每个观测样本（行）。  
    with_mean: boolean类型，默认为True，表示将数据均值规范到0  
    with_std: boolean类型，默认为True，表示将数据方差规范到
       2.  `sklearn.preprocessing.StandardScaler()`  
          可保存训练集的标准化参数(均值、方差)，然后应用在转换测试集数据。 一般我们的标准化先在训练集上进行，在测试集上也应该做同样 mean 和 variance 的标准化
	          ```
	          scaler = preprocessing.StandardScaler().fit(X)
	          ```
      - 自定义  
          ```x_norm = (x-np.mean(x))/np.std(x)  ```
 - **归一化**  
   **将数据特征缩放至某一范围**  
   - 特点  
     对不同特征维度的伸缩变换的目的是使各个特征维度对目标函数的影响权重是一致的，即使得那些扁平分布的数据伸缩变换成类圆形。这也就改变了原始数据的一个分布。
    - 好处  
      两个特征区间相差非常大时，形成的等高线偏椭圆，迭代时很有可能走“之字型”路线（垂直长轴），从而导致需要迭代很多次才能收敛。而对两个特征进行了归一化，对应的等高线就会变圆，在梯度下降进行求解时能较快的收敛。
       1 提高迭代求解的收敛速度
       2 提高迭代求解的精度
    - **代码实现**  
      - sklearn preprocessing  
        ```
        min_max_scaler = preprocessing.MinMaxScaler()
        X_train_minmax = min_max_scaler.fit_transform(X_train)
        ``` 
      - 自定义  
        ```x_norm = (x-np.min(x))/(np.max(x)-np.min(x))  ```
 - **两者区别**  
   参考[https://www.zhihu.com/question/20467170/answer/222792995](https://www.zhihu.com/question/20467170/answer/222792995)
--------------
### batchnorm  
#### batchnorm要解决的问题
- **内部协变量偏移（Internal Covariate Shift）**    
  训练深度网络的时候经常发生训练困难的问题，因为，每一次参数迭代更新后，上一层网络的输出数据经过这一层网络计算后，数据的分布会发生变化，为下一层网络的学习带来困难（神经网络本来就是要学习数据的分布，要是分布一直在变，学习就很难了），此现象称之为Internal Covariate Shift。
- 协变量偏移（covariate shift）  
  Internal Covariate Shift 和Covariate Shift具有相似性，但并不是一个东西，前者发生在神经网络的内部，所以是Internal，后者发生在输入数据上。Covariate Shift主要描述的是由于训练数据和测试数据存在分布的差异性，给网络的泛化性和训练速度带来了影响，我们经常使用的方法是做归一化或者白化。
#### batchnorm原理
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTg0OTUzODAzNiwtMTMxMDQzMjMxNSwtOT
Q1MzI1ODkxLC0xNDQ2Nzk0NTQsLTg3OTU3NTUzOCwtNzEyMTgx
NDAxLDEyNTQwMjk0MTksLTIwODg3NDY2MTIsMTg0NDI5NTkzNC
wtOTkzNTMwNDA3LDcwNDMwMDY2NiwtNDEyOTgxMzksMTI1MDY2
NzgyNiwxMTQwOTcwMjc1LDcyNzkyMDI4MCw5NTI0NTQzMTIsMT
EwODQ4OTE1NiwtMTk2OTk5NTcwMiwxNTc2NTIwMjAxLC0xMjQ5
MTIzODQ1XX0=
-->