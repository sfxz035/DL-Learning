 # **机器学习**

**偏差与方差**

**生成模型与判别模型**




## **SVM（支持向量机）**
#### SVM简介  
- 支持向量机（support vector machines）是一种二分类模型，它的目的是寻找一个超平面来对样本进行分割，分割的原则是间隔最大化，最终转化为一个凸二次规划问题来求解。它的基本模型是定义在特征空间上的**间隔最大的线性分类器**，间隔最大使它有别于感知机；支持向量机还包括**核技巧**，这使其成为实质上的**非线性分类器**。
- 由简至繁的支持向量机分类
  - 当训练数据**线性可分**时，通过**硬间隔最大化**，学习一个**线性分类器**，即线性可分支持向量机，又称**硬间隔支持向量机**。
  - 当训练数据**接近线性可分**时，通过**软间隔最大化**，学习一个线性分类器，即线性支持向量机，又称**软间隔支持向量机**。
  - 当训练数据**线性不可分**时，通过使用**核技巧**及软间隔最大化，学习非线性支持向量机。  
#### 相关定义
- 支持向量  
     训练数据集中与分离超平面距离最近的样本点的实例称为支持向量。
     - 在寻找分离超平面的时候，一般只需看两类数据，它们各自最靠近划分直线的那些点，而其他的点起不了决定作用。这些决定超平面的点就叫支撑向量。
- 函数间隔和几何间隔  
  - 函数间隔(间隔)  
    所有点中该函数值最小的为函数间隔 
   ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556094678%281%29.jpg)
  - 几何间隔  
    所有点中该数值最小的为几何间隔
    ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556094722%281%29.jpg)
    由于不同的训练集各点的间距尺度不太一样，因此用间隔（而不是几何间隔）来衡量有利于我们表达形式的简洁。
 #### 基本知识  
 - 间隔最大化  
   一个简单的二维空间的例子，一边代表正类，一边代表负类，样本是线性可分的，但是很显然不只有这一条直线可以将样本分开，而是有无数条，我们所说的**线性可分支持向量机**就对应着能将数据正确划分并且**间隔最大**的直线。
		   ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556095720%281%29.jpg)
     - **两类点分布在支持向量两端**：
       1. 方法一    
          **固定函数间隔为1**
          两类点分布在两端满足以下
![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556011439%281%29.jpg)
	      该公式等价于![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556012611%281%29.jpg)
	         该公式被称为最大间隔假设。即**固定函数间隔为1**（即支持向量的函数间隔为1），使所有点（同样计算方式）要大于函数间隔，从而**保证两类点分别在支持向量的两端**，而不是支持向量的中间。  
	   2. 方法二
	     两类点分布在两端，即所有点（与几何间隔同意计算方式）要大于几何间隔：
	       	     ![](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556098617%281%29.jpg)  
	     **固定函数间隔为1** 后化简为：  
	     ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556012611%281%29.jpg)
   - **最大化几何间隔**  
     1. 推导方法一
     i 几何间隔就等于两个异类支持向量的差在 w 上的投影，即：     ![](https://raw.githubusercontent.com/sfxz035/DL-Learning/master/picture/20180328160123465.png?token=AH7MXQ7BKZKFGDJMFXSVQJC4YAQ7U)
	  ii **固定函数间隔为1** 时，正负支持向量又满足以下公式：  
	 	![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556098617%281%29.jpg)
	   推导可得  
	   ![](https://raw.githubusercontent.com/sfxz035/DL-Learning/master/picture/1556095526%281%29.jpg?token=AH7MXQ65BV6JRC6CHU2PP2C4YARJC)
	   2. 推导方法二  
	      i 最大化几何间隔：  
	     $\begin{aligned} &{\color{Red} \underset{w,b}{\max}} \quad\gamma \ & \quad\end{aligned}$  
	     ii **固定函数间隔为1**，最大化 `1/||w||` 等价于最小化`1/2*||w||^2`,所以  
	     ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556100245%281%29.jpg)
	- **综上**
	     - 最大化支持向量的几何间隔的同时，保证所有点的间隔大于等于1（即保证），满足这两个条件。于是就有了获得线性支持向量机的以下约束条件：
	     - ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556097977%281%29.jpg)
		   以上皆是在固定间隔（例如固定为1），寻找最小的||w||。为什么令`γ^=1`？——比例改变`(ω,b)`，超平面不会改变，但函数间隔`γ^`会成比例改变，因此可以通过等比例改变`(ω,b)`使函数间隔`γ^=1`。
		 - 该约束最优化问题即为**线性支持向量机**的标准问题——这是一个**凸二次优化**问题，可以使用商业 QP 代码完成。理论上，线性 SVM 的问题已经解决了；但在高等数学中，**带约束的最优化问题**还可以用另一种方法求解——**拉格朗日乘子法**。该方法的优点一是更容易求解，而是自然引入**核函数**，进而推广到非线性的情况。
#### 对偶推导  
- 1 构建拉格朗日函数  
  $\begin{aligned} L(w,b,{\color{Red} \alpha})=&\frac{1}{2}w^Tw-\sum_{i=1}^N{\color{Red} \alpha_i}[y_i(w^Tx_i+b)-1]\ &{\color{Red} \alpha_i \geq 0},\quad i=1,2,\cdots,N \end{aligned}$  
- 2 求导进而求 `L` 对 `(w,b)` 的极小  
  $\begin{aligned} \mathrm{set}\quad \frac{\partial L}{\partial w}=0 ;;&\Rightarrow; w-\sum_{i=1}^N {\color{Red} \alpha_i y_i x_i}=0\ &\Rightarrow; w=\sum_{i=1}^N {\color{Red} \alpha_i y_i x_i} \end{aligned}$  
  $\begin{aligned} \mathrm{set}\quad \frac{\partial L}{\partial b}=0 ;;&\Rightarrow; \sum_{i=1}^N {\color{Red} \alpha_i y_i}=0 \end{aligned}$  
  代入L，得：  
  ![](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556186443%281%29.jpg)
  即  
  ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556186536%281%29.jpg)
- 3 求 `L` 对 `α` 的极大，即  
		    ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556186654%281%29.jpg)
    - 该问题的对偶问题  
      ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556186722%281%29.jpg)  
      于是，标准问题最后等价于求解该**对偶问题**  
 - 4 求解：设 `α` 的解为 `α*`，则存在下标`j`使`α_j > 0`，可得标准问题的解为：  
   ![
](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556186771%281%29.jpg) 
   可得分离超平面及分类决策函数为：  
   ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556186319%281%29.jpg)
#### 非线性支持向量机（核函数）  
	通过上述的对偶问题求解方法，可以解决线性可分支持向量机的问题。以下方式是解决非线性支持向量机问题。  
- 非线性问题转化为线性问题  
  当遇到非线性问题时，线性模型已经无法将样本分开。非线性模型不好求解，我们依然希望通过线性模型来完成分类。所以可以通过将训练样本从原始空间映射到一个更高维的特征空间，使得样本在这个空间中线性可分，如果原始空间维数是有限的，即属性是有限的，那么一定存在一个高维特征空间使样本可分。**令${\phi(x)}$为x的高维特征空间的表示**，于是可得：  
![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556355513%281%29.jpg)  
	- 这其中涉及到计算${\phi(x)^T}{\phi(x)}$计算，而映射到高维空间后，维数可能很高，直接计算${\phi(x)^T}{\phi(x)}$非常困难。于是通过**核函数**解决这一问题：  
	![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556356453%281%29.jpg)  
	即通过低维空间的输入值代入核函数K中，就可得到高位空间的内积值。  
于是  
![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556356887%281%29.jpg)   
最后分离超平面为：  
![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556357777%281%29.jpg)
	- 常见核函数
	  ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556357969%281%29.jpg)  
	  一般来讲，**径向基核函数**是不会出太大偏差的一种
####  ## 软间隔支持向量机  
由于某些极少的偏离程度大的点，是线性可分问题变成了线性不可分问题，这种问题叫做“近似线性可分”的问题。对于这些**不能满足间隔大于等于1**的条件的点，引入一个**松弛变量**$\xi_i\geq0$，使得那些不满足条件的点加上松弛变量后间隔大于等于1。于是约束条件标为：  
![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556358803%281%29.jpg)  
该公式使间隔尽量大，同时使误分类点的个数尽量小，C是调和两者的系数。同时约束间隔大于等于（1-$\xi_i$）。
 - 对偶问题  
  与上述的未引入松弛变量前的对偶问题解决方法一致：  
  ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556359359%281%29.jpg)  
  ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556359421%281%29.jpg)![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556359434%281%29.jpg)  
  于是，可得分离超平面模型为：
  ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556359464%281%29.jpg)  
  #### **小结**
  对于SVM而言，它并没有对原始数据的分布做任何的假设，而有些算法在推导之前都是有假设条件的，也就是算法对数据分布的要求。比如：Logistics Regression，在Logistics Regression中，假设后验概率为Logistics 分布；再比如：LDA假设fk(x)fk(x)是均值不同，方差相同的高斯分布。而这就是SVM和LDA、Logistics Regression区别最大的地方。如果我们事先对数据的分布没有任何的先验信息，即，不知道是什么分布，那么SVM无疑是比较好的选择。

## 决策树
**决策树通常有三个步骤**：特征选择、决策树的生成、决策树的修剪。

#### 决策树相关知识
- **信息熵**  
					![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556611078%281%29.jpg)  
	熵越大，随机变量的不确定性就越大。  
	当熵中的概率由数据估计(特别是最大似然估计)得到时，所对应的熵称为经验熵(empirical entropy)。
- **条件熵**  
  条件熵H(Y∣X) H(Y|X)H(Y∣X)表示在已知随机变量X的条件下随机变量Y的不确定性。定义为X给定条件下Y的条件概率分布的熵对X的数学期望：  
  ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556611614%281%29.jpg)
- **信息增益**  
  信息增益是相对于特征而言的。所以，特征A对训练数据集D的信息增益g(D,A)，定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A)之差，即：  
  ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556611811%281%29.jpg)  
  - 理解  
    选择划分后信息增益大的作为划分特征，说明使用该特征后划分得到的子集纯度越高，即不确定性越小。信息增益值的大小相对于训练数据集而言的，并没有绝对意义，在分类问题困难时，也就是说在训练数据集经验熵大的时候，信息增益值会偏大，反之信息增益值会偏小，使用信息增益比可以对这个问题进行校正，这是特征选择的另一个标准。
  - 缺点：信息增益偏向取值较多的特征（原因：当特征的取值较多时，根据此特征划分更容易得到纯度更高的子集，因此划分后的熵更低，即不确定性更低，因此信息增益更大）
- **信息增益比**  
  特征A对训练数据集D的信息增益比$g_R(D,A)$定义为其信息增益g(D,A) 与训练数据集D的经验熵之比：  
  ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556612231%281%29.jpg)  
  - 与信息增益相比较：之所以引入了信息增益比，是由于信息增益的一个缺点。那就是：信息增益总是偏向于选择取值较多的属性。信息增益比在此基础上增加了一个罚项，解决了这个问题
- **基尼指数**  
  假设有K个类，样本点属于第k类的概率为$p_k$，则概率分布的基尼指数定义为：  
  ![](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556615409%281%29.jpg)
	- 特征条件下的基尼指数  
	  如果样本集合D根据特征A是否取某一可能值a被分割成D1和D2两部分：  
	  ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556615732%281%29.jpg) 
### 决策树生成  
- **ID3算法**  
  ID3算法的核心是在决策树各个结点上对应信息增益准则选择特征，递归地构建决策树。  
  - 具体方法是：  
	  1）从根结点(root node)开始，对结点计算所有可能的特征的信息增益，选择信息增益最大的特征作为结点的特征。
	  2）由该特征的不同取值建立子节点，再对子结点递归地调用以上方法，构建决策树；直到所有特征的信息增益均很小或没有特征可以选择为止；
	  3）最后得到一个决策树

- **C4.5的生成算法**   
  与ID3算法相似，但是做了改进，将信息增益比作为选择特征的标准。
  - C4.5对于在**决定连续特征的分界点**时采用**信息增益**这个指标，而**选择属性**的时候才使用**信息增益率**这个指标能选择出最佳分类特征。
> 以上两者的递归终止条件：程序完全遍历所有划分数据集的属性，或者每个分支下的所有实例都具有相同的分类
- **CART算法**
	- 基本概念  
	  1） CART既能是分类树，又能是分类树；  
	  - 特点  
	    i. 数据对象的属性特征为离散型或连续型，并不是区别分类树与回归树的标准
	    ii. **待预测结果(即输出结果)为离散型数据**则为分类树，**待预测结果为连续型数据**则为回归树。
	    iii. 作为**分类决策树**时，待预测样本落至某一叶子节点，则输出该叶子节点中所有样本**所属类别最多的那一类**;作为**回归决策树**时，待预测样本落至某一叶子节点，则**输出该叶子节点中所有样本的均值**.
	    
	  2）当CART是分类树时，采用GINI值作为节点分裂的依据；当CART是回归树时，采用样本的最小方差作为节点分裂的依据；  
	  3）CART必须是一棵**二叉树**。
	-  **CART树的构建**：  
		- **假设输入空间已经划分**： 
			- 分类树——输出结果为离散型数据
			  遍历所有特征条件下的基尼指数，选择最小的基尼指数特征作为二叉树左右节点的分裂特征。特征条件下的基尼指数越小，说明二分之后的样本越纯净。之后递归建立二叉树。
			- 回归树——输出结果为连续型数据  
			  - 基本思想：选取使方差最小的特征作为分裂二叉树的分裂特征。方差越小，说明二分之后的子样本的“差异性”越小。  
			  - 方差计算方式：  
			  i. 方差公式为		 ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556618509%281%29.jpg)  
			    μ表示样本集S中预测结果的均值，yk表示第k个样本预测结果。 
			    ii. 对于含有N个样本的样本集S，根据属性A的第i个属性值，将数据集S划分成两部分，则划分成两部分之后方差计算：  
			    ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/20170515194034975.png)
		- **划分输入空间**：  
		  - 输入空间的属性值为连续值  
		    采用**二分**的思想。由于我们的数据集是有限的，即使是连续值，属性a在数据集中也只出现了有限个确定的值。记为（$a_1$,$a_2$，...，$a_n$），在分割之前，我们还需要对连续属性进行排序（升序），这样可以大大减少运算量。排序后使得$a_1<a_2...<a_n$。取每两个值得中间值作为分割点，  
		    ![](https://github.com/sfxz035/DL-Learning/raw/master/picture/1557395438%281%29.png)  
		    根据是否大于某个中间值可以将属性划分为两类。选择使基尼指数或者方差最小的中间值作为划分两类的依据。
		  - 输入空间的属性值为多个离散值  
		    因为CART是一棵二叉树，每一次分裂只会产生两个节点。需要将多个离散值而分化。将其中一个离散值独立作为一个节点，其他的离散值生成另外一个节点即可。如果某离散属性一个有三个离散值X，Y，Z，则该属性的分裂方法有{X}、{Y，Z}，{Y}、{X，Z}，{Z}、{X，Y}，分别计算每种划分方法的基尼值或者样本方差确定最优的方法。
		- 
-总结  
  - **ID3和C4.5生成的是多叉树** ，而CART必须是一棵**二叉树**。
  - C4.5和CART的对连续输入属性值的划分方法是一致的。
  - 无论是ID3还是C4.5最好在小数据集上使用，决策树分类一般只适用于小数据。当**属性取值很多时**最好选择C4.5算法，ID3得出的效果会非常差。
  - ID3只能对**分类变量**进行处理，C4.5和CART可以处理**连续和分类两种**自变量
  - ID3对**缺失值**敏感，而C4.5和CART对缺失值可以进行多种方式的处理
  - 只从**样本量考虑**，_小样本建议考虑c4.5、大样本建议考虑cart_。c4.5处理过程中需对数据集进行多次排序，处理成本耗时较高，而cart本身是一种大样本的统计方法，小样本处理下泛化误差较大
  -  ID3用信息增益来选择属性，信息增益容易偏向取值较多的特征的问题，C4.5采用信息增益率的方法，它是信息增益和特征熵的比值，特征数越多的特征对应的特征熵越大，它作为分母，可以校正信息增益偏向取值较多的特征的问题。问题是容易。
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTMyNjAwNTUxNSwtODU4Njg0MzUsLTEzMD
g4MzY2NjAsLTM4NjU2NTQ0LC03NTg2MjI5MzMsLTcyMDk2MDIz
Nyw4NDM1OTg3NzcsMTE2MTUyNTg3Nyw5OTQ3ODg5MTMsLTMyNj
EzMDk4MSwtMTk0MDg0MTcwMSwxNzYzNjQxMjAzLC02NTcxOTY4
NDgsLTE2MzQ1MzEyNDQsMTM5MDE4MTYxMCwtNzQ5MzI1MTk2LC
04OTQ5NjQxODUsMTkyNDg1MjYwOSw4ODkzNTY3NjEsMTA2MDMx
ODEyOV19
-->