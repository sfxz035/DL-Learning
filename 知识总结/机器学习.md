 ## **机器学习**

**偏差与方差**

**生成模型与判别模型**



----
### **SVM（支持向量机）**
#### SVM简介  
- 支持向量机（support vector machines）是一种二分类模型，它的目的是寻找一个超平面来对样本进行分割，分割的原则是间隔最大化，最终转化为一个凸二次规划问题来求解。它的基本模型是定义在特征空间上的**间隔最大的线性分类器**，间隔最大使它有别于感知机；支持向量机还包括**核技巧**，这使其成为实质上的**非线性分类器**。
- 由简至繁的支持向量机分类
  - 当训练数据**线性可分**时，通过**硬间隔最大化**，学习一个**线性分类器**，即线性可分支持向量机，又称**硬间隔支持向量机**。
  - 当训练数据**接近线性可分**时，通过**软间隔最大化**，学习一个线性分类器，即线性支持向量机，又称**软间隔支持向量机**。
  - 当训练数据**线性不可分**时，通过使用**核技巧**及软间隔最大化，学习非线性支持向量机。  
#### 相关定义
- 支持向量  
     训练数据集中与分离超平面距离最近的样本点的实例称为支持向量。
     - 在寻找分离超平面的时候，一般只需看两类数据，它们各自最靠近划分直线的那些点，而其他的点起不了决定作用。这些决定超平面的点就叫支撑向量。
- 函数间隔和几何间隔  
  - 函数间隔(间隔)  
    所有点中最小为函数间隔 
   ![enter image description here](https://raw.githubusercontent.com/sfxz035/DL-Learning/master/picture/1556094678%281%29.jpg?token=AH7MXQ6FUNCG2AYGJMND6TS4YAPZI)
  - 几何间隔  
    所有点中最小为几何间隔
    ![](https://raw.githubusercontent.com/sfxz035/DL-Learning/master/picture/1556094722%281%29.jpg?token=AH7MXQ6MPEBF2K6NEPCFJ724YASF4)
    由于不同的训练集各点的间距尺度不太一样，因此用间隔（而不是几何间隔）来衡量有利于我们表达形式的简洁。
 #### 基本知识  
 - 间隔最大化  
   一个简单的二维空间的例子，一边代表正类，一边代表负类，样本是线性可分的，但是很显然不只有这一条直线可以将样本分开，而是有无数条，我们所说的**线性可分支持向量机**就对应着能将数据正确划分并且**间隔最大**的直线。
		   ![enter image description here](https://raw.githubusercontent.com/sfxz035/DL-Learning/master/picture/1556095720%281%29.jpg?token=AH7MXQ6XZEKDJQIT6L35ZPS4YARUW)
     - **两类点分布在支持向量两端**：
       1. 方法一    
          **固定函数间隔为1**
          两类点分布在两端满足以下
![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556011439%281%29.jpg)
	      该公式等价于![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556012611%281%29.jpg)
	         该公式被称为最大间隔假设。即**固定函数间隔为1**（即支持向量的函数间隔为1），使所有点（同样计算方式）要大于函数间隔，从而**保证两类点分别在支持向量的两端**，而不是支持向量的中间。  
	   2. 方法二
	     两类点分布在两端，即所有点（与几何间隔同意计算方式）要大于几何间隔：
	       	     ![](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556098617%281%29.jpg)  
	     **固定函数间隔为1** 后化简为：  
	     ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556012611%281%29.jpg)
   - **最大化几何间隔**  
     1. 推导方法一
     i 几何间隔就等于两个异类支持向量的差在 w 上的投影，即：     ![](https://raw.githubusercontent.com/sfxz035/DL-Learning/master/picture/20180328160123465.png?token=AH7MXQ7BKZKFGDJMFXSVQJC4YAQ7U)
	  ii **固定函数间隔为1** 时，正负支持向量又满足以下公式：  
	 	![enter image description here](https://raw.githubusercontent.com/sfxz035/DL-Learning/master/picture/20180328160552158.png?token=AH7MXQ4PVUBPX5OG2DG2X5K4YAQ6I)
	   推导可得  
	   ![](https://raw.githubusercontent.com/sfxz035/DL-Learning/master/picture/1556095526%281%29.jpg?token=AH7MXQ65BV6JRC6CHU2PP2C4YARJC)
	   2. 推导方法二  
	      i 最大化几何间隔：  
	     $\begin{aligned} &{\color{Red} \underset{w,b}{\max}} \quad\gamma \ & \quad\end{aligned}$  
	     ii **固定函数间隔为1**，最大化 `1/||w||` 等价于最小化`1/2*||w||^2`,所以  
	     ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556100245%281%29.jpg)
	- **综上**
	     - 最大化支持向量的几何间隔的同时，保证所有点的间隔大于等于1（即保证），满足这两个条件。于是就有了获得线性支持向量机的以下约束条件：
		     ![enter image description here](https://raw.githubusercontent.com/sfxz035/DL-Learning/master/picture/1556097977%281%29.jpg?token=AH7MXQY7MZ5BPZDHJKXJE2S4YAWES)
		   以上皆是在固定间隔（例如固定为1），寻找最小的||w||。为什么令`γ^=1`？——比例改变`(ω,b)`，超平面不会改变，但函数间隔`γ^`会成比例改变，因此可以通过等比例改变`(ω,b)`使函数间隔`γ^=1`。
		 - 该约束最优化问题即为**线性支持向量机**的标准问题——这是一个**凸二次优化**问题，可以使用商业 QP 代码完成。理论上，线性 SVM 的问题已经解决了；但在高等数学中，**带约束的最优化问题**还可以用另一种方法求解——**拉格朗日乘子法**。该方法的优点一是更容易求解，而是自然引入**核函数**，进而推广到非线性的情况。
#### 对偶推导  
- 构建拉格朗日函数  
  $\begin{aligned} L(w,b,{\color{Red} \alpha})=&\frac{1}{2}w^Tw-\sum_{i=1}^N{\color{Red} \alpha_i}[y_i(w^Tx_i+b)-1]\ &{\color{Red} \alpha_i \geq 0},\quad i=1,2,\cdots,N \end{aligned}$  
- 求导进而求 `L` 对 `(w,b)` 的极小  
  $\begin{aligned} \mathrm{set}\quad \frac{\partial L}{\partial w}=0 ;;&\Rightarrow; w-\sum_{i=1}^N {\color{Red} \alpha_i y_i x_i}=0\ &\Rightarrow; w=\sum_{i=1}^N {\color{Red} \alpha_i y_i x_i} \end{aligned}$  
  $\begin{aligned} \mathrm{set}\quad \frac{\partial L}{\partial b}=0 ;;&\Rightarrow; \sum_{i=1}^N {\color{Red} \alpha_i y_i}=0 \end{aligned}$  
  代入L，得：  
  $\begin{aligned} L(w,b,{\color{Red} \alpha}) &=\frac{1}{2}w^Tw-\sum_{i=1}^N{\color{Red} \alpha_i}[y_i(w^Tx_i+b)-1]\ &=\frac{1}{2}w^Tw-w^T\sum_{i=1}^N \alpha_iy_ix_i-b\sum_{i=1}^N \alpha_iy_i+\sum_{i=1}^N \alpha_i\ &=\frac{1}{2}w^Tw-w^Tw+\sum_{i=1}^N \alpha_i\ &=-\frac{1}{2}w^Tw+\sum_{i=1}^N \alpha_i\ &=-\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N \alpha_i\alpha_j\cdot y_iy_j\cdot {\color{Red} x_i^Tx_j}+\sum_{i=1}^N \alpha_i \end{aligned}$  
  即  
  $\begin{aligned} L(w,b,{\color{Red} \alpha}) &=\frac{1}{2}w^Tw-\sum_{i=1}^N{\color{Red} \alpha_i}[y_i(w^Tx_i+b)-1]\ &=\frac{1}{2}w^Tw-w^T\sum_{i=1}^N \alpha_iy_ix_i-b\sum_{i=1}^N \alpha_iy_i+\sum_{i=1}^N \alpha_i\ &=\frac{1}{2}w^Tw-w^Tw+\sum_{i=1}^N \alpha_i\ &=-\frac{1}{2}w^Tw+\sum_{i=1}^N \alpha_i\ &=-\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N \alpha_i\alpha_j\cdot y_iy_j\cdot {\color{Red} x_i^Tx_j}+\sum_{i=1}^N \alpha_i \end{aligned}$  
  - 求 `L` 对 `α` 的极大，即
<!--stackedit_data:
eyJoaXN0b3J5IjpbMjAzMDkyMDk2LC05MTMzODgwNiwxNTAxND
I0Mzg4LC0yMTEwMzkyNzIxLDE3NTM4ODU3ODcsOTc1MTM3Mjg2
LDE3MzQ0MDE3NDUsMTAxNTM1NTQ1LC04NDQ3NjM1OSwtMTQxNz
k1Mzk4MywxMTcyMjMxMzUzLDEwMTA4ODUxMTAsMTI2Njc5MzM5
NywtMTY3ODY3NzM3NiwtMTYxMDg0MzcxNiwxMTQzNTg1MTE2LC
0yMDM3ODQyMzQ3LDExMDY4ODIzMTYsMTgzMTgwMzg5OCwtMzU4
MjQ2ODM2XX0=
-->