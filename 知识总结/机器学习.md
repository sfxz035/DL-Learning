 # **机器学习**
要回答这个问题需要考虑很多因素：（1）数据的大小，质量和类型；（2）完成计算所需要的时间；（3）任务的紧迫程度；（4）你需要对数据做什么处理。
**偏差与方差**

## **生成模型与判别模型**

参考：[https://blog.csdn.net/zl3090/article/details/82683889]  
### **涉及概率知识**：  
- 先验概率和条件概率    

  - **条件概率**（似然概率）:  
		  一个事件发生后另一个事件发生的概率。一般的形式为P(x|y)表示y发生的条件下x发生的概率。  
   - **先验概率:**  
      事件发生前的预判概率。可以是基于历史数据的统计，可以由背景常识得出，也可以是人的主观观点给出。一般都是单独事件概率，如P(x),P(y)。  
    - **后验概率：**  
      基于先验概率求得的反向条件概率P(Y|X)，形式上与条件概率相同（若 P(X|Y) 为正向，则 P(Y|X) 为反向）。  
    - **贝叶斯公式**  
      $$P(y|x) = \frac{( P(x|y) * P(y) ) }{P(x)}$$
      
     参考：[https://blog.csdn.net/suranxu007/article/details/50326873]   
     
### **基础知识**：  
- 监督学习的任务是学习一个模型，对给定的输入预测相应的输出
- 这个模型的一般形式为一个1.决策函数：Y=f(X) 或是2.条件概率分布P(Y|X)
	- 决策函数：输入 X 返回 Y；其中 Y 与一个阈值比较，然后根据比较结果判定 X 的类别 
	- 条件概率分布：输入 X 返回 X 属于每个类别的概率；将其中概率最大的作为 X 所属的类别

- **生成模型和判别模型**
	- 判别模型直接学习决策函数或者条件概率分布  
	  直观来说，判别模型学习的是类别之间的最优分隔面，反映的是不同类数据之间的差异
	- 生成模型学习的是联合概率分布P(X,Y)，然后根据条件概率公式计算 P(Y|X)   
		  $$P(X|Y)=\frac{P(X,Y)}{P(Y)}$$

### **两者之间的联系**
- 由生成模型可以得到判别模型，但由判别模型得不到生成模型。

 - 当存在“隐变量”时，只能使用生成模型  
  
   隐变量：当我们找不到引起某一现象的原因时，就把这个在起作用，但无法确定的因素，叫“隐变量”

### **优缺点：**  
- **判别模型：**  
	- 优点  
	  1. 直接面对预测，往往学习的准确率更高  
	  2. 由于直接学习 P(Y|X) 或 f(X)，可以对数据进行各种程度的抽象，定义特征并使用特征，以简化学习过程
	- 缺点  
	  不能反映训练数据本身的特性

- **生成模型：**  
	- 优点： 
	  1. 可以还原出联合概率分布 P(X,Y)，判别方法不能  
	  2. 学习收敛速度更快——即当样本容量增加时，学到的模型可以更快地收敛到真实模型  
	  3. 当存在“隐变量”时，只能使用生成模型
	- 缺点  
	  学习和计算过程比较复杂

### **常见模型**  
- 判别模型  
  K 近邻、感知机（神经网络）、决策树、逻辑斯蒂回归、最大熵模型、SVM、提升方法、条件随机场
- 生成模型  
  朴素贝叶斯法、隐马尔可夫模型、混合高斯模型、贝叶斯网络、马尔可夫随机场  

## **信息论**  

### **自信息**    
- 公式  
  
  $$I(X)=-logP(x)$$     
  
  当该对数的底数为自然对数 e 时，单位为奈特（nats）；当以 2 为底数时，单位为比特（bit）或香农（shannons）

### **信息熵（Information-entropy）**   
用于对整个概率分布中的不确定性总量进行量化：
- 公式  
  $$H(x)=-\sum_{x \in X}{P(x)logP(x)}$$   
  
### **Softmax**  
- 公式  
 
  $$softmax(x)=\frac{exp(x_i)}{\sum_j{exp(x_j)}}$$    
  
  通过求各类特征的exp值，然后对他们归一化，使得和为1（总概率为1），特征的值越大，输出的softmax值越大，代表概率越大。    
 
 - **note**  
   softmax后一般接交叉熵损失。如果不经过softmax归一化，那么假如输出值全为1的话，那么得到的交叉熵损失为0，显然这是不合理的。所以需要经过归一化再进行交叉熵损失。  
   
###  **交叉熵（cross-entropy）:**  
- 公式  
 
  $$H_P(Q)=-\sum_{x \in X}{P(x)log(Q(x))}$$    
  
  其中，P(X)是真实概率，即label，Q(X)为预测概率。  
- 本质  	
 交叉熵本质上相当于衡量两个编码方式之间的差值，因为只有当猜测的分布约接近于真实分布，则其值越小。通过它可以判断预测的概率分布的准确程度。  
  
  交叉熵用来衡量在给定的真实分布下，使用非真实分布所指定的策略消除系统的不确定性所需要付出成本的大小。交叉的字面意思在于：真实分布与非真实分布的交叉。  
- 使用  
   在LR中，使用交叉熵作为损失函数比均方差损失的好处：  
   1. 在LR中，如果用平方损失函数，则损失函数是一个非凸的，而用cross-entropy的话就是一个凸函数  
   2. 均方差更容易出现梯度消失，而交叉熵相比较不容易。

### **KL散度**  
- **定义**：  	
  KL散度/距离是衡量两个分布的距离,KL距离一般用D(P||Q)或者$D_P(Q)$ 称之为**P对Q的相对熵:**    
  $$D_P(Q)=\sum_{x \in X}{P(x)[logP(x)-logQ(x)]}$$  
  
- **性质**  
  - 非负，KL 散度为 0 当且仅当P 和 Q 在离散型变量的情况下是相同的分布  
  
  - 不对称，D(P||Q)与D(Q||P)不相等

### **交叉熵与KL散度关系：**          
1. 针对 Q 最小化交叉熵等价于最小化 P 对 Q 的 KL 散度  
 
2. **最大似然估计**中，最小化 KL 散度其实就是在最小化分布之间的交叉熵

## **逻辑斯蒂回归（LR）**   

-  **逻辑回归简介及来由**    
 
	LR模型可以被认为就是一个被Sigmoid函数（logistic方程）所归一化后的线性回归模型。  
	
	线性回归进行预测时，容易收到离群值的影响，而造成阈值选取偏离，最终影响模型效果。而通过逻辑回归函数，输出结果不再是一个预测结果，而是输出一个条件概率，不再容易收到离群值的影响  
-  **逻辑斯蒂回归模型定义**  

	  $$z=W^TH=\omega_0x_0+\omega_1x_1+...+\omega_nx_n$$  
	  
	  上式为线性回归的假设函数（此处省略偏置b）  
	  
	  $$g(z)=\frac{1}{1+e^{-z}}$$
	  
	  上式为sigmoid函数，代入线性回归假设函数得到逻辑回归的假设函数。
	  - 二项逻辑斯蒂回归模型即如下的条件概率分布：  
	    ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1561035802%281%29.jpg)   
	    其中$x_i$为样本特征，$Y\in \{0,1\}$  
	    可以看出，上边线性回归的假设函数套入sigmoid函数即可以得  
	    $$P(Y=1|x)$$
- **代价函数**  
	- **MSE**  
	  逻辑回归中，无法应用代价函数MSE（误差平方和）。因为将逻辑回归的假设函数（决策函数）$g(x)=\frac{1}{1+e^{-W^Tx}}$代入代价函数后，得到的是非凸函数，如下图：  
	  ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/feitu.png)    
	  这样的函数拥有多个局部极小值，较大可能得到局部最小值，而不是全局最小值。  
	  
	 - **对数损失函数**： 
	   $$Cost(g(x),y)=\begin{cases} -log(g(x)) ,&\text {if y=1}\\-log(1-g(x)), &\text{if y=0}\end{cases}$$
	   分别对应下图：  
	   ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1561039283%281%29.jpg)   
	   可以看出，当预测结果与标签结果越接近（同为1或同为0时），代价越小（最小为0）。而当预测结果与标签恰好相反时，代价无穷大。该损失函数就可以解决非凸问题。  
	   
	   把Cost写再一行为：  
	   $$Cost(g(x),y) = -ylog(g(x))-(1-y)log(1-g(x))$$  

-  **逻辑斯蒂公式推导**  
	- 逻辑斯蒂回归定义  
	  $$P(Y=1|x)=\frac{1}{1+exp(-wx)}=\sigma(x)$$    
	  $$P(Y=0|x)=1-\sigma(x)$$
	- 损失函数  
	  - 似然函数：  
		  $$\prod_{i=1}^{N}{[\sigma(x_i)]^{y_i}[1-\sigma(x_i)]^{1-y_i}}$$  
      - 负对数似然函数：  
         $$L(\omega)    =  -log(\prod_{i=1}^N{[\sigma(x_i)]^{y_i}[1-\sigma(x_i)]^{1-y_i}}) \\ =-\sum_{i=1}^{N}[y_i log(\sigma(x_i))+(1-y_i)log(1-\sigma(x_i))] \\=-\sum_{i=1}^{N}{[y_ilog\frac{\sigma(x_i)}{1-\sigma(x_i)}+log(1-\sigma(x_i))]}$$  
          $$-log(\prod_{i=1}^N{[\sigma(x_i)]^{y_i}[1-\sigma(x_i)]^{1-y_i}}) \\ =-\sum_{i=1}^N[y_i log(\sigma(x_i))+(1-y_i)log(1-\sigma(x_i))] \\=-\sum_{i=1}^{N}{[y_ilog\frac{\sigma(x_i)}{1-\sigma(x_i)}+log(1-\sigma(x_i))]}$$  
         代入$\sigma(x_i)$，得到  
         
         $L(w)=-\sum_{i=1}^N[y_i(wx_i)-log(1+exp(wx_i))]$    
         
         （详见统计学习方法—李航）  
         上式为负对数，对L(w)求极小值，通常采用梯度下降法或牛顿法。而当为正对数（即似然函数代入正对数），变成求极大值，用梯度上升法。
   - 求梯度：  
     ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1561277135%281%29.jpg)   
     
 - **多分类逻辑斯蒂回归模型**   
   ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1561277382.jpg)   
 ## **朴素贝叶斯法**    
### **基本公式**  
- 联合概率（全概率）：
   $$P(AB) = P(A|B)P(B)=P(B|A)P(A)$$
- 条件概率  
  $$P(A|B) = \frac {P(AB)}{P(B)}$$  
- 由以上两个公式可得：  
  
  $$P(A|B)=\frac {P(B|A)P(A)}{P(B)}$$   
  而其中
    ![enter image description here](https://lh3.googleusercontent.com/EecaDHMv-gFkq-6Yo0aRUfj1ft3beSYoeBNwbNrcqjVHaW73Jo3HWWx1rWKRJg6c3OIXrl-7bvU)   
 ### 朴素贝叶斯公式  

朴素贝叶斯方法其实就是根据条件概率（P（B|A）），先验概率（P（B））求出联合分布概率（P（AB）），再除以P（B）求得我们想要的后验概率（P（A|B））。

根据《统计学习方法》一书

输出空间为类标记集合 {c_{1} ,c_{2}, ... ,c_{k}}。输入为特征向量 x 属于输入空间，输出为类标记 y 属于输出空间。X 是定义在输入空间上的随机向量，Y 是定义在输出空间上的随机向量。P(X,Y) 是 X 和 Y 的联合概率分布。训练集数据为：$T={(x_1,y_1),(x_2,y_2),L(x_n,y_n)}$  
- 先验概率分布:  
  $$P(Y=c_k),k=1,2,3...,K$$  
- 条件概率  
  
  ![enter image description here](https://lh3.googleusercontent.com/DVJDnPp2xlsgHY4C5dZ1ur8AqAy-z4m-k5E_y-dgcEdFdWPe8vO_ywZDjaw5b1CVFx6YxJtTumg)  
     
  而上式中有指数级数量的参数，其估计实际是不可行的。假设X的各个分量之间相互独立，即各个特征属性是条件独立的。上式可以写成:  
  
  ![enter image description here](https://lh3.googleusercontent.com/KgUVBayCjNqbWPdaJR6KZ8GpgIzryetjTBmfbofn4qr1xOLWVTfNFnUWCUpnCzZxax5yBhd40_E)   
- 后验概率可以写成：（**这是朴素贝叶斯基本公式)**  
  
  ![enter image description here](https://lh3.googleusercontent.com/PHIDKtfh5SLIzQcCZxgDz2BPL9X4UpAp22mun1I0YgNnu7qCkd2Op9pxh5AlZD9L-17Zf5jefC8)   
  
  由于分母是一个固定值，我们要求的是最大概率的结果，所以去掉不影响结果，故朴素贝叶斯分类器可以表示为   
  
  ![enter image description here](https://lh3.googleusercontent.com/V5AxG7uVYx2ykCBycQk7V9Y3DBto8YQlt5OjVhjaSbqIGtLBnvaBO5R__81QLYAs_Wme79CwMRI)   
  根据概率，即可确定x属于的类。  
### **朴素贝叶斯参数估计**   
  由以上公式可知，根据现有数据求得P（Y）和P（X|Y），就可以利用公式求得我们想要的P（Y|X）。那么P（Y）和P（X|Y）的估计则是用极大似然估计法估计相应的概率。  
 
  先验P（Y）的极大似然估计是：  
  
  ![enter image description here](https://lh3.googleusercontent.com/WJXOpzJUaEANKXhq7jTIK30zP0TP6DokiFodoan6PpziKgFpnpinGhFT_GGMRn9KfU5tCxsYwKI)    
  
  条件概率的极大似然估计是：  
  
  ![enter image description here](https://lh3.googleusercontent.com/09_IezbMDtx1ynyQx9r6-vqGNkYY3x_Rs8xiQdqabgYR5tzpiUnQKGR3Dj_XmDT24Bx2tLlb_fo)   
  
  I为指示函数，若括号内式子成立，则记为1，否则记为0。  
   
  极大似然估计的公式推导： 暂无   
- 拉普拉斯平滑   
  公式$P(y_k|x)=P(y_k)*\prod {P(x_i|y_k)}$是一个连乘项，其中有一项数值为零，则整个公式为零，会使分类产生偏差。通过拉普拉斯平滑，即每个分量加1，样本大的情况下加1的影响可以忽略不计。  
  
  $$P(y)=\frac {|D_y|+1}{|D|+N}$$    
  
  |Dy|表示分类y的样本数，|D|样本总数，N是类别数    
  $$P(x_i|D_y) = \frac {|D_y,x_i|+1}{D_y+N_i}$$  
  |Dy|表示分类y的样本数， Ni表示xi特征的可能的取值数    
  
  它的思想非常简单，就是对先验概率的分子（划分的计数）加1，分母加上类别数；对条件概率分子加1，分母加上对应特征的可能取值数量。这样在解决零概率问题的同时，也保证了概率和依然为1。  
 - **朴素贝叶斯优缺点**   
	优点：对小规模的数据表现很好，适合多分类任务，适合增量式训练。    
	缺点：对输入数据的表达形式很敏感（离散、连续，值极大极小之类的）。
## **SVM（支持向量机）**   
### SVM简介  
- 支持向量机（support vector machines）是一种二分类模型，它的目的是寻找一个超平面来对样本进行分割，分割的原则是间隔最大化，最终转化为一个凸二次规划问题来求解。它的基本模型是定义在特征空间上的**间隔最大的线性分类器**，间隔最大使它有别于感知机；支持向量机还包括**核技巧**，这使其成为实质上的**非线性分类器**。
- 由简至繁的支持向量机分类
  - 当训练数据**线性可分**时，通过**硬间隔最大化**，学习一个**线性分类器**，即线性可分支持向量机，又称**硬间隔支持向量机**。
  - 当训练数据**接近线性可分**时，通过**软间隔最大化**，学习一个线性分类器，即线性支持向量机，又称**软间隔支持向量机**。
  - 当训练数据**线性不可分**时，通过使用**核技巧**及软间隔最大化，学习非线性支持向量机。  
### 相关定义
- **支持向量**  
     训练数据集中与分离超平面距离最近的样本点的实例称为支持向量。
     - 在寻找分离超平面的时候，一般只需看两类数据，它们各自最靠近划分直线的那些点，而其他的点起不了决定作用。这些决定超平面的点就叫支撑向量。
- **函数间隔和几何间隔**  
  - 函数间隔(间隔)  
    所有点中该函数值最小的为函数间隔  
   ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556094678%281%29.jpg)
  - 几何间隔      
    所有点中该数值最小的为几何间隔  
   ![enter image description here](https://lh3.googleusercontent.com/OuP4FJA3gFL02UA-QQUWZSsIbyNb-_YzRMfRMAkM0kVMeBts3SIiyjdm7ySwMLDoqaRtCYS1onWu)
    由于不同的训练集各点的间距尺度不太一样，因此用间隔（而不是几何间隔）来衡量有利于我们表达形式的简洁。
 ### 基本知识  
 - 间隔最大化  
   一个简单的二维空间的例子，一边代表正类，一边代表负类，样本是线性可分的，但是很显然不只有这一条直线可以将样本分开，而是有无数条，我们所说的**线性可分支持向量机**就对应着能将数据正确划分并且**间隔最大**的直线。
		   ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556095720%281%29.jpg)
     - **两类点分布在支持向量两端**：
       1. 方法一    
          **固定函数间隔为1**
          两类点分布在两端满足以下：   
![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556011439%281%29.jpg)  

	      该公式等价于![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556012611%281%29.jpg)  
			
	         该公式被称为最大间隔假设。即**固定函数间隔为1**（即支持向量的函数间隔为1），使所有点（同样计算方式）要大于函数间隔，从而**保证两类点分别在支持向量的两端**，而不是支持向量的中间。   
	   2. 方法二  
	     两类点分布在两端，即所有点（与几何间隔同意计算方式）要大于几何间隔：
	       	     ![](https://github.com/sfxz035/DL-Learning//master/picture/1556098617%281%29.jpg)  
	     **固定函数间隔为1** 后化简为：  
	     ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556012611%281%29.jpg)
   - **最大化几何间隔**  
     1. 推导方法一  
     i. 几何间隔就等于两个异类支持向量的差在 w 上的投影，即： 
      ![enter image description here](https://lh3.googleusercontent.com/awZD21OG7rBt6J-0yRKk5JlDZyN71WuL5_wOl10nlreoD8FLdIeFeCnvp5UjzcZykm4kB9HexiWT)  
   
	     ii. **固定函数间隔为1** 时，正负支持向量又满足以下公式：  
	 	![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556098617%281%29.jpg)  
	 	  推导可得    
	 	  ![enter image description here](https://lh3.googleusercontent.com/2My4BQrFgphkSL5s0v-d8UP9V36_Hvlij8lCvXR8I4Sp8nMhdz0jVSj4S191LmZBZhjWkcJCZgG6)
	   2. 推导方法二  
	      i 最大化几何间隔：  
	      
	      $\begin{aligned} &{\color{Red} \underset{w,b}{\max}} \quad\gamma   
	     \ & \quad\end{aligned}$  
	      ii **固定函数间隔为1**，最大化 `1/||w||` 等价于最小化`1/2*||w||^2`,所以  
	     ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556100245%281%29.jpg)
	- **综上**
	     - 最大化支持向量的几何间隔的同时，保证所有点的间隔大于等于1（即保证），满足这两个条件。于是就有了获得线性支持向量机的以下约束条件：  
	        - ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556097977%281%29.jpg)  
	      
		   以上皆是在固定间隔（例如固定为1），寻找最小的||w||。为什么令`γ^=1`？——比例改变`(ω,b)`，超平面不会改变，但函数间隔`γ^`会成比例改变，因此可以通过等比例改变`(ω,b)`使函数间隔`γ^=1`。
		 - 该约束最优化问题即为**线性支持向量机**的标准问题——这是一个**凸二次优化**问题，可以使用商业 QP 代码完成。理论上，线性 SVM 的问题已经解决了；但在高等数学中，**带约束的最优化问题**还可以用另一种方法求解——**拉格朗日乘子法**。该方法的优点一是更容易求解，而是自然引入**核函数**，进而推广到非线性的情况。
#### 对偶推导  
- 1 构建拉格朗日函数  
  $\begin{aligned} L(w,b,{\color{Red} \alpha})=&\frac{1}{2}w^Tw-\sum_{i=1}^N{\color{Red} \alpha_i}[y_i(w^Tx_i+b)-1]\ &{\color{Red} \alpha_i \geq 0},\quad i=1,2,\cdots,N \end{aligned}$  
- 2 求导进而求 `L` 对 `(w,b)` 的极小  
  $\begin{aligned} \mathrm{set}\quad \frac{\partial L}{\partial w}=0 ;;&\Rightarrow; w-\sum_{i=1}^N {\color{Red} \alpha_i y_i x_i}=0\ &\Rightarrow; w=\sum_{i=1}^N {\color{Red} \alpha_i y_i x_i} \end{aligned}$  
  $\begin{aligned} \mathrm{set}\quad \frac{\partial L}{\partial b}=0 ;;&\Rightarrow; \sum_{i=1}^N {\color{Red} \alpha_i y_i}=0 \end{aligned}$  
  代入L，得：  
  ![](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556186443%281%29.jpg)
  即  
  ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556186536%281%29.jpg)
- 3 求 `L` 对 `α` 的极大，即  
		    ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556186654%281%29.jpg)
    - 该问题的对偶问题  
      ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556186722%281%29.jpg)  
      于是，标准问题最后等价于求解该**对偶问题**  
 - 4 求解：设 `α` 的解为 `α*`，则存在下标`j`使`α_j > 0`，可得标准问题的解为：  
   ![
](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556186771%281%29.jpg) 
   可得分离超平面及分类决策函数为：  
   ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556186319%281%29.jpg)
#### 非线性支持向量机（核函数）  
	通过上述的对偶问题求解方法，可以解决线性可分支持向量机的问题。以下方式是解决非线性支持向量机问题。  
- 非线性问题转化为线性问题  
  当遇到非线性问题时，线性模型已经无法将样本分开。非线性模型不好求解，我们依然希望通过线性模型来完成分类。所以可以通过将训练样本从原始空间映射到一个更高维的特征空间，使得样本在这个空间中线性可分，如果原始空间维数是有限的，即属性是有限的，那么一定存在一个高维特征空间使样本可分。**令${\phi(x)}$为x的高维特征空间的表示**，于是可得：  
![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556355513%281%29.jpg)  
	- 这其中涉及到计算${\phi(x)^T}{\phi(x)}$计算，而映射到高维空间后，维数可能很高，直接计算${\phi(x)^T}{\phi(x)}$非常困难。于是通过**核函数**解决这一问题：  
	![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556356453%281%29.jpg)  
	即通过低维空间的输入值代入核函数K中，就可得到高位空间的内积值。  
于是  
![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556356887%281%29.jpg)   
最后分离超平面为：  
![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556357777%281%29.jpg)
	- 常见核函数
	  ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556357969%281%29.jpg)  
	  一般来讲，**径向基核函数**是不会出太大偏差的一种
####  软间隔支持向量机  
由于某些极少的偏离程度大的点，是线性可分问题变成了线性不可分问题，这种问题叫做“近似线性可分”的问题。对于这些**不能满足间隔大于等于1**的条件的点，引入一个**松弛变量**$\xi_i\geq0$，使得那些不满足条件的点加上松弛变量后间隔大于等于1。于是约束条件标为：  
![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556358803%281%29.jpg)  
该公式使间隔尽量大，同时使误分类点的个数尽量小，C是调和两者的系数。同时约束间隔大于等于（1-$\xi_i$）。
 - 对偶问题  
  与上述的未引入松弛变量前的对偶问题解决方法一致：  
  ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556359359%281%29.jpg)  
  ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556359421%281%29.jpg)   
    ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556359434%281%29.jpg)  
  于是，可得分离超平面模型为：  
  
  ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556359464%281%29.jpg)  
  #### **损失函数**   
  - 合折页损失   
    $L_(y) = max(0 , 1 – t⋅y)$   
    其中，y是预测值(-1到1之间)，t为目标值(1或 -1)。其含义为，y的值在 -1到1之间即可。并不鼓励 |y|>1，即让某个样本能够正确分类就可以了，不鼓励分类器过度自信，当样本与分割线的距离超过1时并不会有任何奖励。目的在于使分类器更专注于整体的分类误差。     
    如下：   
    ![enter image description here](https://lh3.googleusercontent.com/WJRw9Dvo8YDS5n1jaa3Z0UrWc4l7TC6_GoPxiEViO0c8QGIDCez1YhPMaCEyYD38YMPOWVKhL6Q6)
    下标”+”表示以下取正值的函数，我们用z表示中括号中的部分：    
    ![enter image description here](https://lh3.googleusercontent.com/8s3N4O492yrc9wvH2JXFvcJ5RkjwYgp88nNPub01NAkpSvE3U61TChgDGE_MMJDJAYc2LQwi0jjE)
    也就是说，数据点如果被正确分类，损失为0（即间隔大于等于1时损失均为零）。如果没有被正确分类，损失为z。
  - SVM损失函数   
     SVM的损失函数就是合页损失函数加上正则化项：
    ![enter image description here](https://lh3.googleusercontent.com/8s3N4O492yrc9wvH2JXFvcJ5RkjwYgp88nNPub01NAkpSvE3U61TChgDGE_MMJDJAYc2LQwi0jjE)
  #### **小结**
  对于SVM而言，它并没有对原始数据的分布做任何的假设，而有些算法在推导之前都是有假设条件的，也就是算法对数据分布的要求。比如：Logistics Regression，在Logistics Regression中，假设后验概率为Logistics 分布；再比如：LDA假设fk(x)fk(x)是均值不同，方差相同的高斯分布。而这就是SVM和LDA、Logistics Regression区别最大的地方。如果我们事先对数据的分布没有任何的先验信息，即，不知道是什么分布，那么SVM无疑是比较好的选择。

通过使函数间隔大于某个固定值1（为什么是1？），保证两类点分别在线的两端。
   间隔就等于两个异类支持向量的差在 w 上的投影，即：
   固定间隔（例如固定为1），寻找最小的||w||。

#### LR和SVM异同：    
**区别**：   
1）LR是参数模型，SVM是非参数模型。   
2）从目标函数来看，两者损失不同。区别在于逻辑回归采用的是logistical loss，SVM采用的是hinge loss.这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。   
3）SVM的处理方法是只考虑support vectors，也就是和分类最相关的少数点，去学习分类器。而逻辑回归需要考虑全局的点。   
    线性SVM不直接依赖于数据分布，分类平面不受一类点影响；LR则受所有数据点的影响，如果数据不同类别strongly unbalance，一般需要先对数据做balancing。​    
4）LR可以给出每个点属于每一类的概率，而SVM是非概率的   
5）逻辑回归相对来说模型更简单，好理解，特别是大规模线性分类时比较方便。而SVM的理解和优化相对来说复杂一些，SVM转化为对偶问题后,分类只需要计算与少数几个支持向量的距离,这个在进行复杂核函数计算时优势很明显,能够大大简化模型和计算。   
6）logic 能做的 svm能做，但可能在准确率上有问题，svm能做的logic有的做不了。     
**相同之处：**      
- LR和SVM都是分类算法    
- 如果不考虑核函数，LR和SVM都是线性分类算法，也就是说他们的分类决策面都是线性的。   
- LR和SVM都是监督学习算法   
- LR和SVM都是判别模型


## 决策树
**决策树通常有三个步骤**：特征选择、决策树的生成、决策树的修剪。

#### 决策树相关知识
- **信息熵**  
					![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556611078%281%29.jpg)  
	熵越大，随机变量的不确定性就越大。  
	当熵中的概率由数据估计(特别是最大似然估计)得到时，所对应的熵称为经验熵(empirical entropy)。
- **条件熵**  
  条件熵H(Y∣X) H(Y|X)H(Y∣X)表示在已知随机变量X的条件下随机变量Y的不确定性。定义为X给定条件下Y的条件概率分布的熵对X的数学期望：  
  ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556611614%281%29.jpg)
- **信息增益**  
  信息增益是相对于特征而言的。所以，特征A对训练数据集D的信息增益g(D,A)，定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A)之差，即：  
  ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556611811%281%29.jpg)  
  - 理解  
    选择划分后信息增益大的作为划分特征，说明使用该特征后划分得到的子集纯度越高，即不确定性越小。信息增益值的大小相对于训练数据集而言的，并没有绝对意义，在分类问题困难时，也就是说在训练数据集经验熵大的时候，信息增益值会偏大，反之信息增益值会偏小，使用信息增益比可以对这个问题进行校正，这是特征选择的另一个标准。
  - 缺点：信息增益偏向取值较多的特征（原因：当特征的取值较多时，根据此特征划分更容易得到纯度更高的子集，因此划分后的熵更低，即不确定性更低，因此信息增益更大）
- **信息增益比**  
  特征A对训练数据集D的信息增益比$g_R(D,A)$定义为其信息增益g(D,A) 与训练数据集D的经验熵之比：  
  ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556612231%281%29.jpg)  
  - 与信息增益相比较：之所以引入了信息增益比，是由于信息增益的一个缺点。那就是：信息增益总是偏向于选择取值较多的属性。信息增益比在此基础上增加了一个罚项，解决了这个问题
- **基尼指数**  
  假设有K个类，样本点属于第k类的概率为$p_k$，则概率分布的基尼指数定义为：  
  ![](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556615409%281%29.jpg)
	- 特征条件下的基尼指数  
	  如果样本集合D根据特征A是否取某一可能值a被分割成D1和D2两部分：  
	  ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556615732%281%29.jpg) 
### 决策树生成  
- **ID3算法**  
  ID3算法的核心是在决策树各个结点上对应信息增益准则选择特征，递归地构建决策树。  
  - 具体方法是：  
	  1）从根结点(root node)开始，对结点计算所有可能的特征的信息增益，选择信息增益最大的特征作为结点的特征。
	  2）由该特征的不同取值建立子节点，再对子结点递归地调用以上方法，构建决策树；直到所有特征的信息增益均很小或没有特征可以选择为止；
	  3）最后得到一个决策树

- **C4.5的生成算法**   
  与ID3算法相似，但是做了改进，将信息增益比作为选择特征的标准。
  - 输入空间的属性是连续值时，与CART处理方法一致。
  - C4.5对于在**决定连续特征的分界点**时采用**信息增益**这个指标，而**选择属性**的时候才使用**信息增益率**这个指标能选择出最佳分类特征。

> 以上两者的递归终止条件：程序完全遍历所有划分数据集的属性，或者每个分支下的所有实例都具有相同的分类
- **CART算法**
	- 基本概念  
	  1） CART既能是分类树，又能是分类树；  
	  - 特点  
	    i. 数据对象的属性特征为离散型或连续型，并不是区别分类树与回归树的标准
	    ii. **待预测结果(即输出结果)为离散型数据**则为分类树，**待预测结果为连续型数据**则为回归树。
	    iii. 作为**分类决策树**时，待预测样本落至某一叶子节点，则输出该叶子节点中所有样本**所属类别最多的那一类**;作为**回归决策树**时，待预测样本落至某一叶子节点，则**输出该叶子节点中所有样本的均值**.
	    
	  2）当CART是分类树时，采用GINI值作为节点分裂的依据；当CART是回归树时，采用样本的最小方差作为节点分裂的依据；  
	  3）CART必须是一棵**二叉树**。
	-  **CART树的构建**：  
		- **假设输入空间已经划分**： 
			- 分类树——输出结果为离散型数据
			  遍历所有特征条件下的基尼指数，选择最小的基尼指数特征作为二叉树左右节点的分裂特征。特征条件下的基尼指数越小，说明二分之后的样本越纯净。之后递归建立二叉树。
			- 回归树——输出结果为连续型数据  
			  - 基本思想：选取使方差最小的特征作为分裂二叉树的分裂特征。方差越小，说明二分之后的子样本的“差异性”越小。  
			  - 方差计算方式：  
			  i. 方差公式为		 ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556618509%281%29.jpg)  
			    μ表示样本集S中预测结果的均值，yk表示第k个样本预测结果。 
			    ii. 对于含有N个样本的样本集S，根据属性A的第i个属性值，将数据集S划分成两部分，则划分成两部分之后方差计算：  
			    ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/20170515194034975.png)
		- **划分输入空间**：  
		  - 输入空间的属性值为连续值  
		    采用**二分**的思想。由于我们的数据集是有限的，即使是连续值，属性a在数据集中也只出现了有限个确定的值。记为（$a_1$,$a_2$，...，$a_n$），在分割之前，我们还需要对连续属性进行排序（升序），这样可以大大减少运算量。排序后使得$a_1<a_2...<a_n$。取每两个值得中间值作为分割点，  
		    ![](https://github.com/sfxz035/DL-Learning/raw/master/picture/1557395438%281%29.png)  
		    根据是否大于某个中间值可以将属性划分为两类。选择使基尼指数或者方差最小的中间值作为划分两类的依据。
		  - 输入空间的属性值为多个离散值  
		    因为CART是一棵二叉树，每一次分裂只会产生两个节点。需要将多个离散值而分化。将其中一个离散值独立作为一个节点，其他的离散值生成另外一个节点即可。如果某离散属性一个有三个离散值X，Y，Z，则该属性的分裂方法有{X}、{Y，Z}，{Y}、{X，Z}，{Z}、{X，Y}，分别计算每种划分方法的基尼值或者样本方差确定最优的方法。
		- 
- **总结**总结  
  - **ID3和C4.5生成的是多叉树** ，而CART必须是一棵**二叉树**。
  - C4.5和CART的对连续输入属性值的划分方法是一致的。
  - 无论是ID3还是C4.5最好在小数据集上使用，决策树分类一般只适用于小数据。当**属性取值很多时**最好选择C4.5算法，ID3得出的效果会非常差。
  - ID3只能对**分类变量**进行处理，C4.5和CART可以处理**连续和分类两种**自变量
  - ID3对**缺失值**敏感，而C4.5和CART对缺失值可以进行多种方式的处理
  - 只从**样本量考虑**，_小样本建议考虑c4.5、大样本建议考虑cart_。c4.5处理过程中需对数据集进行多次排序，处理成本耗时较高，而cart本身是一种大样本的统计方法，小样本处理下泛化误差较大
  -  ID3用信息增益来选择属性，信息增益容易偏向取值较多的特征的问题，C4.5采用信息增益率的方法，它是信息增益和特征熵的比值，特征数越多的特征对应的特征熵越大，它作为分母，可以校正信息增益偏向取值较多的特征的问题。问题是容易。


## 集成学习

- 基本思想：由多个学习器组合成一个性能更好的学习器
- 集成学习分为以下两类：  
	1. 串行集成方法，这种方法串行地生成基础模型
	2. 并行集成方法，这种方法并行地生成基础模型
- 学习器
  1. 强学习器亦叫做强可学习：是指一个模型针对学习任务结果具有很高的准确性；  
  2. 弱学习器亦叫做弱可学习：是指一个模型针对学习任务的结果比随机猜测的准确性好。
### 基本方法:
### 1. **Bagging**  
- Bagging 基于**并行策略**：基学习器之间不存在依赖关系，可同时生成。
- 代表算法/模型：  随机森林/神经网络的 Dropout 策略
- **方法过程**：
	- i)   从原始样本集中抽取训练集。每轮从原始样本集中使用Bootstraping（有放回）的方法抽取n个训练样本（在训练集中，有些样本可能被多次抽取到，而有些样本可能一次都没有被抽中）。共进行k轮抽取，得到k个训练集。（我们这里假设k个训练集之间是相互独立的，事实上不是完全独立）
	- ii)  每次使用一个训练集得到一个模型，k个训练集共得到k个模型。但是是同种模型。（注：，k个训练集虽然有重合不完全独立，训练出来的模型因为是同种模型也是不完全独立。这里并没有具体的分类算法或回归方法，我们可以根据具体问题采用不同的分类或回归方法，如决策树、感知器等）
	- iii)	对分类问题：将上步得到的k个模型采用投票的方式得到分类结果；对回归问题，计算上述模型的均值作为最后的结果。（所有模型的重要性相同）
- 相关总结  
	- Bagging通过**降低基分类器的方差**，改善了泛化误差
	- 其性能依赖于基分类器的稳定性；如果基分类器不稳定，bagging有助于降低训练数据的随机波动导致的误差；如果稳定，则集成分类器的误差主要由基分类器的偏倚引起
	- 由于每个样本被选中的概率相同，因此bagging并不侧重于训练数据集中的任何特定实例
	- 对于Bagging需要注意的是，每次训练集可以取全部的特征进行训练，也可以随机选取部分特征训练，例如随机森林就是每次随机选取部分特征
	- bagging对于弱学习器没有限制，这和Adaboost一样。但是最常用的一般也是决策树和神经网络。
	#### 随机森林（RF）
	在随机森林中，每个树模型都是装袋采样（**随机**）训练的。另外，特征也是**随机**选择的。它是Bagging算法的进化版，它是在bagging的基础上改进：
	>  两个随机性的引入对随机森林的分类性能至关重要。由于它们的引入，使得随机森林**不容易陷入过拟合**，并且具有很好得**抗噪能力**（比如：对缺省值不敏感）。
	
	- 具体方法：  
	  1. 在Bagging的基础上，RF使用CART决策树作为弱学习器。
		  >即如果训练集大小为N，对于每棵树而言，随机且有放回地从训练集中的抽取N个训练样本，作为该树的训练集。
	  
	  2. 如果每个样本的特征维度为M，指定一个常数m<<M，随机地从M个特征中选取m个特征子集，每次树进行分裂时，从这m个特征中选择最优的；
	  3. 每棵树都尽最大程度的生长，并且没有剪枝过程
	  4. 如果是分类算法预测，则T个弱学习器投出最多票数的类别或者类别之一为最终类别。如果是回归算法，T个弱学习器得到的回归结果进行算术平均得到的值为最终的模型输出。  
	  
	  当m=M时，RF的CART决策树和普通的CART决策树没有区别。m越小，则模型约健壮，当然此时对于训练集的拟合程度会变差。也就是说m越小，模型的方差会减小，但是偏倚会增大。在实际案例中，一般会通过交叉验证调参获取一个合适的m的值。
	
	- 小结  
		- 相关问题  
		  1. **为什么要随机抽样训练集？**  
		     如果不进行随机抽样，每棵树的训练集都一样，那么最终训练出的树分类结果也是完全一样的，这样的话完全没有bagging的必要；
		  2. **为什么要有放回地抽样？**  
		   我理解的是这样的：如果不是有放回的抽样，那么每棵树的训练样本都是不同的，都是没有交集的，这样每棵树都是"有偏的"，都是绝对"片面的"（当然这样说可能不对），也就是说每棵树训练出来都是有很大的差异的；而随机森林最后分类取决于多棵树（弱分类器）的投票表决，这种表决应该是"求同"，因此使用完全不同的训练集来训练每棵树这样对最终分类结果是没有帮助的，这样无异于是"盲人摸象"。

		 - **随机森林分类效果（错误率）与两个因素有关：**  
		   1. 森林中任意两棵树的相关性：相关性越大，错误率越大；
		   2. 森林中每棵树的分类能力：每棵树的分类能力越强，整个森林的错误率越低。  
		    
		    减小特征选择个数m，树的相关性和分类能力也会相应的降低；增大m，两者也会随之增大。所以关键问题是如何选择最优的m（或者是范围），这也是随机森林唯一的一个参数。
		 - 优缺点
			 - 优点  
			   1） 训练可以高度并行化，对于大数据时代的大样本训练速度有优势。
			   2） 由于可以随机选择决策树节点划分特征，这样在样本特征维度很高的时候，仍然能高效的训练模型。
			   3） 在训练后，可以给出各个特征对于输出的重要性
			   4） 由于采用了随机采样，训练出的模型的方差小，泛化能力强,不容易过拟合。
			   5） 相对于Boosting系列的Adaboost和GBDT， RF实现比较简单。
			   6） 对部分特征缺失不敏感。
			- 缺点  
			  1）在某些噪音比较大的样本集上，RF模型容易陷入过拟合。
			  2）取值划分比较多的特征容易对RF的决策产生更大的影响，从而影响拟合的模型的效果。
		 - 袋外错误率

### 2. **Boosting**  
- **Boosting 基于串行策略**：基学习器之间存在依赖关系，新的学习器需要依据旧的学习器生成。

- **代表算法/模型**：1. 提升方法 AdaBoost； 2. 提升树；3. 提升树中的梯度提升树 GBDT 

- **特点**：
  -   每次学习都会使用全部训练样本

- **方法过程**：  
  - i)	先从初始训练集训练出一个基学习器；
  - ii)  根据弱学习的学习误差率表现来更新训练样本的权重，使得之前弱学习器学习误差率高的训练样本点的权重变高
  - iii)  基于调整后的样本分布来训练下一个基学习器；
  - iiii) 重复进行上述步骤，直至基学习器数目达到事先指定的值T，最终将这T个基学习器进行加权结合。

- **Boosting 策略要解决的两个基本问题**：  
	1.  每一轮如何改变训练数据的权值或概率分布？
	2.  如何将弱分类器组合成一个强分类器？
    其他问题：3. 如何计算学习误差率e?		4. 如何得到弱学习器权重系数α?
	### **AdaBoost**  
	 >AdaBoost 是 Boosting 策略的一种具体算法 。基学习器可以使决策树，也可以是其他模型。 
	 
	 - **AdaBoost如何解决以上问题：**
		1. 每一轮如何改变数据的权值或概率分布？——开始时，每个样本的权值是一样的，AdaBoost 的做法是提高上一轮弱分类器错误分类样本的权值，同时降低那些被正确分类样本的权值。
		2. 如何将弱分类器组合成一个强分类器？—— AdaBoost 采取加权表决的方法（加法模型）。具体的，AdaBoost 会加大分类误差率小的基学习器的权值，使其在表决中起到更大的作用，同时减小分类误差率大的基学习器的权值。  
	
	- **AdaBoost算法思路**  
		- 输入：训练集$T={(x_1,y_1),..,(x_N,y_N)}, x_i ∈ R^n, y_i ∈ {-1,+1}$
		- 输出：最终学习器G(x)
		1） 初始化训练数据的权值分布
		  
			- $D_1=(w_{1,1},\cdots,w_{1,i},\cdots,w_{1,N}),\quad w_{1,i}=\frac{1}{N},\quad i=1,2,\cdots,N$
		
			2） 对于m=1,2,..,M，共M个弱分类器,通过以下步骤：  
			- i)  使用权值分布为$D_m$的**训练集**，得到基分类器：  
			  $G_m(x):\chi \rightarrow {-1,+1}$
			- ii) 计算$G_m$分类器的**误差率**  :
			  $\begin{aligned} e_m&=P(G_m(x_i)\neq y_i)\&=\sum_{i=1}^Nw_{m,i}\cdot {\color{Red} I(G_m(x_i)\neq y_i)} \end{aligned}$  
			  > `I(x)`  为指示函数：若`G(x)!=y`为真，则`I(G(x)!=y)=1`，反之为  `0`
	实际上分类误差率就等于所有**分类错误的数据的权值之和**
			
			- iii)  计算该分类器$G_m$的**权重**:  
			 
			  $\alpha_m=\frac{1}{2}\ln\frac{1-e_m}{e_m}$  
			  
			- iv) 更新训练集样本的权值分布（作为下一个分类器的训练集）：  
			  
			  ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/%E5%85%AC%E5%BC%8F_20180715140328.png)  
			  >其中 `Z_m` 为**规范化因子**，使 `D_m+1` 成为一个**概率分布**，类似 `Softmax` 函数
			
				另外：
				![](https://github.com/sfxz035/DL-Learning/raw/master/picture/%E5%85%AC%E5%BC%8F_20180715134916.png)
			  
			  因此 `w_{m+1,i}` 也可以写作:  
			  
			  ![](https://github.com/sfxz035/DL-Learning/raw/master/picture/%E5%85%AC%E5%BC%8F_20180715135945.png)  
			
			3） 构建基学习器的**线性组合**，得到最终的学习器：  
			
			$G(x)=\mathrm{sign}(\sum_{m=1}^M\alpha_mG_m(x))$  
		
	- **AdaBoost回归问题算法**  
		参考：[https://www.cnblogs.com/pinard/p/6133937.html]  
		回归问题算法和以上的分类算法大体一致，两点区别：
		1. 误差率计算的不同，回归问题对每个样本的计算相对误差(误差公式可选为平方差，指数误差等等)，再得到学习器的误差率。
		2. 强学习器的集成策略不同：回归采用的是对加权的弱学习器取权重中位数对应的弱学习器作为强学习器的方法
	- AdaBoost优缺点：
		- 优点  
		  1. Adaboost作为分类器时，分类精度很高
		  2. 不容易发生过拟合
		  3. 在Adaboost的框架下，可以使用各种回归分类模型来构建弱学习器，非常灵活
		- 缺点  
		  1. 对异常样本敏感，异常样本在迭代中可能会获得较高的权重，影响最终的强学习器的预测准确性
		  2. 在Adaboost训练过程中，Adaboost会使得难于分类样本的权值呈指数增长，训练将会过于偏向这类困难的样本，导致Adaboost算法易受噪声干扰。此外，Adaboost依赖于弱分类器，而弱分类器的训练时间往往很长。  
		  
	### **提升树**    
	>经典的AdaBoost算法只能处理采用指数损失函数的二分类学习任务，而梯度提升方法通过设置不同的可微损失函数可以处理各类学习任务（多分类、回归、Ranking等），应用范围大大扩展。
	- 以**决策树**为基学习器，对分类问题使用二叉分类树，回归问题使用二叉回归树。
	- 提升树是加法模型，则依然可以用前向分步式算法进行模型的求取，提升树模型则可表示为**决策树的加法模型**，
	- 对于**二分类问题**，提升树算法只需要将AdaBoost 算法中的基学习器限制为二叉分类树即可，**和上述的AdaBoost内容无差**。  
	 - 对于**回归问题**时，通过不断拟合残差得到新的树
	   
	   **算法思路：**    
	   - 输入：训练集  `T={(x1,y1),..,(xN,yN)}, xi ∈ R^n, yi ∈ R`  
	   - 输出：回归提升树  `f_M(x)`   
	     
	    1. 初始化$f_0(x)=0$  
	    2. 对 `m=1,2,..,M`:  
	       a. 计算**残差** :  ${\color{Red} r_{m,i}}=y_i-f_{m-1}(x_i),\quad i=1,2,..,N$  
			  
	       b.拟合残差，学习一个回归树，得到$T(x_i;θ_m)$  
		      - i. **拟合残差**学习下一个回归树的参数  
		        ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/%E5%85%AC%E5%BC%8F_20180717112341.png) 
		     - ii. 由参数生成下一个回归树$T(x_i;θ_m)$   
		  
		   c. 更新 $f_m(x)=f_{m−1}(x)+T(x;θ_m)$  
		 3. 得到回归提升树  
		    $f_M(x)=\sum_{m=1}^MT(x;\Theta_m)$ 
		      
	      详细步骤及例子参考：[https://www.jianshu.com/p/7902b2eb5f21]
	 	 
	### **GBDT**  
	
	> GBDT中的DT是回归树，不是分类树，也就是说，只有回归树才有所谓的梯度提升。  
	
	- 梯度提升是梯度下降的近似方法，其关键是利用损失函数的**负梯度作为残差的近似值**，来拟合下一个决策树。
	- 前面用到的损失函数都有比较好的性质，如平方误差、指数函数等，但对于一般的函数来说，问题可能会变得复杂些，针对这一问题Freidman提出了梯度提升（Gradient Boosting），这是利用最速下降的近似方法，其关键是利用损失函数的负梯度在当前模型的值  
	  
	  **算法思路**：  
	  - 输入：训练集  `T={(x1,y1),..,(xN,yN)}, xi ∈ R^n, yi ∈ R`；损失函数  `L(y,f(x))`；
	  - 输出：回归树  `f_M(x)`  
	  1. 初始化回归树  
	    $f_0(x)={\color{Red} \arg\underset{c}{\min}}\sum_{i=1}^NL(y_i,c)$ 
	  2. 对m=1,2,..,M:  
	    1）.对 `i=1,2,..,N`，计算残差/负梯度  
	    
		    $r_{m,i}=-\frac{\partial L(y_i,{\color{Red} f_{m-1}(x_i)}))}{\partial {\color{Red} f_{m-1}(x_i)}}$  
		    
		 2）. 对 `r_mi` 拟合一个回归树，得到第 `m` 棵树的叶节点区域   
		 
	      $R_{m,j},\quad j=1,2,..,J$ 
	    
	     3）. 对叶子区域j =1,2,..J,计算最佳拟合值  
	     
		    $c_{m,j}={\color{Red} \arg\underset{c}{\min}}\sum_{x_i\in R_{m,j}}L(y_i,{\color{Blue} f_{m-1}(x_i)+c})$   
	    
		 4）. 更新回归树  $f_m(x)=f_{m-1}+\sum_{j=1}^J c_{m,j}{I(x\in R_{m,j})}$  
	     
	  3. 得到强学习器
	   $f_M(x)=\sum_{i=1}^M\sum_{j=1}^Jc_{m,j}{\color{Blue} I(x\in R_{m,j})}$  
	 - 说明  
	   - 算法第 1 步初始化，估计使损失函数最小的常数值，得到一棵只有一个根节点的树  
	   - 第 2(i) 步计算损失函数的负梯度，将其作为残差的估计  
	     -   对平方损失而言，负梯度就是残差；对于一般的损失函数，它是残差的近似  
	  -   第 2(ii) 步估计回归树的节点区域，以拟合残差的近似值
	  -   第 2(iii) 步利用线性搜索估计叶节点区域的值，使损失函数最小化  
	  [具体算法实例流程参考](https://blog.csdn.net/zpalyq110/article/details/79527653)
	### **XGBoost**    
	(1). xgboost在目标函数中显示的加上了正则化项，基学习为CART时，正则化项与树的叶子节点的数量T和叶子节点的值有关。  
	(2)  GB中使用Loss Function对f(x)的一阶导数计算出伪残差用于学习生成fm(x)，xgboost不仅使用到了一阶导数，还使用二阶导数。 
	(3)上面提到CART回归树中寻找最佳分割点的衡量标准是最小化均方差，xgboost寻找分割点的标准是最大化，lamda，gama与正则化项相关
	xgboost算法的步骤和GB基本相同，都是首先初始化为一个常数，gb是根据一阶导数ri，xgboost是根据一阶导数gi和二阶导数hi，迭代生成基学习器，相加更新学习器   

	xgboost与gdbt除了上述三点的不同，xgboost在实现时还做了许多**优化**：

	-   在寻找最佳分割点时，考虑传统的枚举每个特征的所有可能分割点的贪心法效率太低，xgboost实现了一种近似的算法。大致的思想是根据百分位法列举几个可能成为分割点的候选者，然后从候选者中根据上面求分割点的公式计算找出最佳的分割点。
	-   xgboost考虑了训练数据为稀疏值的情况，可以为缺失值或者指定的值指定分支的默认方向，这能大大提升算法的效率，paper提到50倍。
	-   特征列排序后以块的形式存储在内存中，在迭代中可以重复使用；虽然boosting算法迭代必须串行，但是在处理每个特征列时可以做到并行。
	-   按照特征列方式存储能优化寻找最佳的分割点，但是当以行计算梯度数据时会导致内存的不连续访问，严重时会导致cache miss，降低算法效率。paper中提到，可先将数据收集到线程内部的buffer，然后再计算，提高算法的效率。
	-   xgboost 还考虑了当数据量比较大，内存不够时怎么有效的使用磁盘，主要是结合多线程、数据压缩、分片的方法，尽可能的提高算法的效率。
	### 前向分步算法
	前向分步算法提供了一种学习加法模型的普遍性方法。不同形式的基函数、不同形式的损失函数都可以用这种普遍性方法去求出加法模型的最优化参数，它是一种元算法。  
	参考：[https://blog.csdn.net/xueyingxue001/article/details/51304490]  
		
	

 
### **Bagging，Boosting二者小结**
- Bagging  
   - 基于**并行策略**：基学习器之间不存在依赖关系，可同时生成。  
   
   - 特点
	   - 训练每个基学习器时只使用一部分样本；  
	   - 偏好**不稳定**的学习器作为基学习器；
		   >所谓不稳定的学习器，指的是对**样本分布**较为敏感的学习器。

- Boosting  
  - 基于**串行策略**：基学习器之间存在依赖关系，新的学习器需要根据上一个学习器生成。  
  - 每次学习都会使用**全部训练样本**   
-  区别   
	- 样本选择上：  
	Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的。  
	Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整。
	- 例权重：  
		Bagging：使用均匀取样，每个样例的权重相等  
		Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大。  
	- 预测函数：  
	  Bagging：所有预测函数的权重相等。  
	  Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重。
	- 并行/串行计算：   
	  Bagging：各个预测函数可以并行  
	  Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果。
	- **性能：  
	  bagging是减少variance  
	  boosting是减少bias**通过使函数间隔大于某个固定值1（为什么是1？），保证两类点分别在线的两端。
   间隔就等于两个异类支持向量的差在 w 上的投影，即：
   固定间隔（例如固定为1），寻找最小的||w||。

<!--stackedit_data:
eyJoaXN0b3J5IjpbLTM5ODU5NTM1NCwxMzk1MjE3MzkwLDk0Nz
g1MzUzMiw0Mzg4NDQ5MjgsLTM3MTM0OTMyMiwzMTc0MjY2Nzgs
LTIwMzA5Nzg2NywxMzI3NDIxODI1LC0xMDYwOTA1NTUzLC0xNT
gyNDA1MDcyLDE5NTQzMTg1ODMsLTE5MDIzNzg5NzksLTkwMDA3
MzMxLDExNzI1NDM3ODksMjA3NjIyMzM2NSwxNzc0NTk5MDI1LC
02NzEyNzQ0NCwyMjk2ODE2NTQsMTE0ODIzODk0MCwxNDA1NDc5
ODMyXX0=
-->