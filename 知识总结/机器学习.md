 # **机器学习**

**偏差与方差**

**生成模型与判别模型**

要回答这个问题需要考虑很多因素：（1）数据的大小，质量和类型；（2）完成计算所需要的时间；（3）任务的紧迫程度；（4）你需要对数据做什么处理。


## **SVM（支持向量机）**
#### SVM简介  
- 支持向量机（support vector machines）是一种二分类模型，它的目的是寻找一个超平面来对样本进行分割，分割的原则是间隔最大化，最终转化为一个凸二次规划问题来求解。它的基本模型是定义在特征空间上的**间隔最大的线性分类器**，间隔最大使它有别于感知机；支持向量机还包括**核技巧**，这使其成为实质上的**非线性分类器**。
- 由简至繁的支持向量机分类
  - 当训练数据**线性可分**时，通过**硬间隔最大化**，学习一个**线性分类器**，即线性可分支持向量机，又称**硬间隔支持向量机**。
  - 当训练数据**接近线性可分**时，通过**软间隔最大化**，学习一个线性分类器，即线性支持向量机，又称**软间隔支持向量机**。
  - 当训练数据**线性不可分**时，通过使用**核技巧**及软间隔最大化，学习非线性支持向量机。  
#### 相关定义
- 支持向量  
     训练数据集中与分离超平面距离最近的样本点的实例称为支持向量。
     - 在寻找分离超平面的时候，一般只需看两类数据，它们各自最靠近划分直线的那些点，而其他的点起不了决定作用。这些决定超平面的点就叫支撑向量。
- 函数间隔和几何间隔  
  - 函数间隔(间隔)  
    所有点中该函数值最小的为函数间隔 
   ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556094678%281%29.jpg)
  - 几何间隔  
    所有点中该数值最小的为几何间隔
    ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556094722%281%29.jpg)
    由于不同的训练集各点的间距尺度不太一样，因此用间隔（而不是几何间隔）来衡量有利于我们表达形式的简洁。
 #### 基本知识  
 - 间隔最大化  
   一个简单的二维空间的例子，一边代表正类，一边代表负类，样本是线性可分的，但是很显然不只有这一条直线可以将样本分开，而是有无数条，我们所说的**线性可分支持向量机**就对应着能将数据正确划分并且**间隔最大**的直线。
		   ![enter image description here
](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556095720%281%29.jpg)
     - **两类点分布在支持向量两端**：
       1. 方法一    
          **固定函数间隔为1**
          两类点分布在两端满足以下
：  
![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556011439%281%29.jpg)
	     该公式等价于![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556012611%281%29.jpg)
	         该公式被称为最大间隔假设。即**固定函数间隔为1**（即支持向量的函数间隔为1），使所有点（同样计算方式）要大于函数间隔，从而**保证两类点分别在支持向量的两端**，而不是支持向量的中间。  
	   2. 方法二
	     两类点分布在两端，即所有点（与几何间隔同意计算方式）要大于几何间隔：
	       	     ![](https://github.com/sfxz035/DL-Learning//master/picture/1556098617%281%29.jpg)  
	     **固定函数间隔为1** 后化简为：  
	     ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556012611%281%29.jpg)
   - **最大化几何间隔**  
     1. 推导方法一
     i 几何间隔就等于两个异类支持向量的差在 w 上的投影，即：     ![](https://raw.githubusercontent.com/sfxz035/DL-Learning/master/picture/20180328160123465.png?token=AH7MXQ7BKZKFGDJMFXSVQJC4YAQ7U)
	  ii **固定函数间隔为1** 时，正负支持向量又满足以下公式：  
	 	![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556098617%281%29.jpg)
	   推导可得  
	   ![](https://raw.githubusercontent.com/sfxz035/DL-Learning/master/picture/1556095526%281%29.jpg?token=AH7MXQ65BV6JRC6CHU2PP2C4YARJC)
	   2. 推导方法二  
	      i 最大化几何间隔：  
	      $\begin{aligned} &{\color{Red} \underset{w,b}{\max}} \quad\gamma \ & \quad\end{aligned}$  
	     ii **固定函数间隔为1**，最大化 `1/||w||` 等价于最小化`1/2*||w||^2`,所以  
	     ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556100245%281%29.jpg)
	- **综上**
	     - 最大化支持向量的几何间隔的同时，保证所有点的间隔大于等于1（即保证），满足这两个条件。于是就有了获得线性支持向量机的以下约束条件：
	     - ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556097977%281%29.jpg)
		   以上皆是在固定间隔（例如固定为1），寻找最小的||w||。为什么令`γ^=1`？——比例改变`(ω,b)`，超平面不会改变，但函数间隔`γ^`会成比例改变，因此可以通过等比例改变`(ω,b)`使函数间隔`γ^=1`。
		 - 该约束最优化问题即为**线性支持向量机**的标准问题——这是一个**凸二次优化**问题，可以使用商业 QP 代码完成。理论上，线性 SVM 的问题已经解决了；但在高等数学中，**带约束的最优化问题**还可以用另一种方法求解——**拉格朗日乘子法**。该方法的优点一是更容易求解，而是自然引入**核函数**，进而推广到非线性的情况。
#### 对偶推导  
- 1 构建拉格朗日函数  
  $\begin{aligned} L(w,b,{\color{Red} \alpha})=&\frac{1}{2}w^Tw-\sum_{i=1}^N{\color{Red} \alpha_i}[y_i(w^Tx_i+b)-1]\ &{\color{Red} \alpha_i \geq 0},\quad i=1,2,\cdots,N \end{aligned}$  
- 2 求导进而求 `L` 对 `(w,b)` 的极小  
  $\begin{aligned} \mathrm{set}\quad \frac{\partial L}{\partial w}=0 ;;&\Rightarrow; w-\sum_{i=1}^N {\color{Red} \alpha_i y_i x_i}=0\ &\Rightarrow; w=\sum_{i=1}^N {\color{Red} \alpha_i y_i x_i} \end{aligned}$  
  $\begin{aligned} \mathrm{set}\quad \frac{\partial L}{\partial b}=0 ;;&\Rightarrow; \sum_{i=1}^N {\color{Red} \alpha_i y_i}=0 \end{aligned}$  
  代入L，得：  
  ![](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556186443%281%29.jpg)
  即  
  ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556186536%281%29.jpg)
- 3 求 `L` 对 `α` 的极大，即  
		    ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556186654%281%29.jpg)
    - 该问题的对偶问题  
      ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556186722%281%29.jpg)  
      于是，标准问题最后等价于求解该**对偶问题**  
 - 4 求解：设 `α` 的解为 `α*`，则存在下标`j`使`α_j > 0`，可得标准问题的解为：  
   ![
](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556186771%281%29.jpg) 
   可得分离超平面及分类决策函数为：  
   ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556186319%281%29.jpg)
#### 非线性支持向量机（核函数）  
	通过上述的对偶问题求解方法，可以解决线性可分支持向量机的问题。以下方式是解决非线性支持向量机问题。  
- 非线性问题转化为线性问题  
  当遇到非线性问题时，线性模型已经无法将样本分开。非线性模型不好求解，我们依然希望通过线性模型来完成分类。所以可以通过将训练样本从原始空间映射到一个更高维的特征空间，使得样本在这个空间中线性可分，如果原始空间维数是有限的，即属性是有限的，那么一定存在一个高维特征空间使样本可分。**令${\phi(x)}$为x的高维特征空间的表示**，于是可得：  
![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556355513%281%29.jpg)  
	- 这其中涉及到计算${\phi(x)^T}{\phi(x)}$计算，而映射到高维空间后，维数可能很高，直接计算${\phi(x)^T}{\phi(x)}$非常困难。于是通过**核函数**解决这一问题：  
	![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556356453%281%29.jpg)  
	即通过低维空间的输入值代入核函数K中，就可得到高位空间的内积值。  
于是  
![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556356887%281%29.jpg)   
最后分离超平面为：  
![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556357777%281%29.jpg)
	- 常见核函数
	  ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556357969%281%29.jpg)  
	  一般来讲，**径向基核函数**是不会出太大偏差的一种
####  ## 软间隔支持向量机  
由于某些极少的偏离程度大的点，是线性可分问题变成了线性不可分问题，这种问题叫做“近似线性可分”的问题。对于这些**不能满足间隔大于等于1**的条件的点，引入一个**松弛变量**$\xi_i\geq0$，使得那些不满足条件的点加上松弛变量后间隔大于等于1。于是约束条件标为：  
![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556358803%281%29.jpg)  
该公式使间隔尽量大，同时使误分类点的个数尽量小，C是调和两者的系数。同时约束间隔大于等于（1-$\xi_i$）。
 - 对偶问题  
  与上述的未引入松弛变量前的对偶问题解决方法一致：  
  ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556359359%281%29.jpg)  
  ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556359421%281%29.jpg)![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556359434%281%29.jpg)  
  于是，可得分离超平面模型为：
  ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556359464%281%29.jpg)  
  #### **小结**
  对于SVM而言，它并没有对原始数据的分布做任何的假设，而有些算法在推导之前都是有假设条件的，也就是算法对数据分布的要求。比如：Logistics Regression，在Logistics Regression中，假设后验概率为Logistics 分布；再比如：LDA假设fk(x)fk(x)是均值不同，方差相同的高斯分布。而这就是SVM和LDA、Logistics Regression区别最大的地方。如果我们事先对数据的分布没有任何的先验信息，即，不知道是什么分布，那么SVM无疑是比较好的选择。

通过使函数间隔大于某个固定值1（为什么是1？），保证两类点分别在线的两端。
   间隔就等于两个异类支持向量的差在 w 上的投影，即：
   固定间隔（例如固定为1），寻找最小的||w||。
## 决策树
**决策树通常有三个步骤**：特征选择、决策树的生成、决策树的修剪。

#### 决策树相关知识
- **信息熵**  
					![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556611078%281%29.jpg)  
	熵越大，随机变量的不确定性就越大。  
	当熵中的概率由数据估计(特别是最大似然估计)得到时，所对应的熵称为经验熵(empirical entropy)。
- **条件熵**  
  条件熵H(Y∣X) H(Y|X)H(Y∣X)表示在已知随机变量X的条件下随机变量Y的不确定性。定义为X给定条件下Y的条件概率分布的熵对X的数学期望：  
  ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556611614%281%29.jpg)
- **信息增益**  
  信息增益是相对于特征而言的。所以，特征A对训练数据集D的信息增益g(D,A)，定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A)之差，即：  
  ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556611811%281%29.jpg)  
  - 理解  
    选择划分后信息增益大的作为划分特征，说明使用该特征后划分得到的子集纯度越高，即不确定性越小。信息增益值的大小相对于训练数据集而言的，并没有绝对意义，在分类问题困难时，也就是说在训练数据集经验熵大的时候，信息增益值会偏大，反之信息增益值会偏小，使用信息增益比可以对这个问题进行校正，这是特征选择的另一个标准。
  - 缺点：信息增益偏向取值较多的特征（原因：当特征的取值较多时，根据此特征划分更容易得到纯度更高的子集，因此划分后的熵更低，即不确定性更低，因此信息增益更大）
- **信息增益比**  
  特征A对训练数据集D的信息增益比$g_R(D,A)$定义为其信息增益g(D,A) 与训练数据集D的经验熵之比：  
  ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556612231%281%29.jpg)  
  - 与信息增益相比较：之所以引入了信息增益比，是由于信息增益的一个缺点。那就是：信息增益总是偏向于选择取值较多的属性。信息增益比在此基础上增加了一个罚项，解决了这个问题
- **基尼指数**  
  假设有K个类，样本点属于第k类的概率为$p_k$，则概率分布的基尼指数定义为：  
  ![](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556615409%281%29.jpg)
	- 特征条件下的基尼指数  
	  如果样本集合D根据特征A是否取某一可能值a被分割成D1和D2两部分：  
	  ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556615732%281%29.jpg) 
### 决策树生成  
- **ID3算法**  
  ID3算法的核心是在决策树各个结点上对应信息增益准则选择特征，递归地构建决策树。  
  - 具体方法是：  
	  1）从根结点(root node)开始，对结点计算所有可能的特征的信息增益，选择信息增益最大的特征作为结点的特征。
	  2）由该特征的不同取值建立子节点，再对子结点递归地调用以上方法，构建决策树；直到所有特征的信息增益均很小或没有特征可以选择为止；
	  3）最后得到一个决策树

- **C4.5的生成算法**   
  与ID3算法相似，但是做了改进，将信息增益比作为选择特征的标准。
  - 输入空间的属性是连续值时，与CART处理方法一致。
  - C4.5对于在**决定连续特征的分界点**时采用**信息增益**这个指标，而**选择属性**的时候才使用**信息增益率**这个指标能选择出最佳分类特征。

> 以上两者的递归终止条件：程序完全遍历所有划分数据集的属性，或者每个分支下的所有实例都具有相同的分类
- **CART算法**
	- 基本概念  
	  1） CART既能是分类树，又能是分类树；  
	  - 特点  
	    i. 数据对象的属性特征为离散型或连续型，并不是区别分类树与回归树的标准
	    ii. **待预测结果(即输出结果)为离散型数据**则为分类树，**待预测结果为连续型数据**则为回归树。
	    iii. 作为**分类决策树**时，待预测样本落至某一叶子节点，则输出该叶子节点中所有样本**所属类别最多的那一类**;作为**回归决策树**时，待预测样本落至某一叶子节点，则**输出该叶子节点中所有样本的均值**.
	    
	  2）当CART是分类树时，采用GINI值作为节点分裂的依据；当CART是回归树时，采用样本的最小方差作为节点分裂的依据；  
	  3）CART必须是一棵**二叉树**。
	-  **CART树的构建**：  
		- **假设输入空间已经划分**： 
			- 分类树——输出结果为离散型数据
			  遍历所有特征条件下的基尼指数，选择最小的基尼指数特征作为二叉树左右节点的分裂特征。特征条件下的基尼指数越小，说明二分之后的样本越纯净。之后递归建立二叉树。
			- 回归树——输出结果为连续型数据  
			  - 基本思想：选取使方差最小的特征作为分裂二叉树的分裂特征。方差越小，说明二分之后的子样本的“差异性”越小。  
			  - 方差计算方式：  
			  i. 方差公式为		 ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556618509%281%29.jpg)  
			    μ表示样本集S中预测结果的均值，yk表示第k个样本预测结果。 
			    ii. 对于含有N个样本的样本集S，根据属性A的第i个属性值，将数据集S划分成两部分，则划分成两部分之后方差计算：  
			    ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/20170515194034975.png)
		- **划分输入空间**：  
		  - 输入空间的属性值为连续值  
		    采用**二分**的思想。由于我们的数据集是有限的，即使是连续值，属性a在数据集中也只出现了有限个确定的值。记为（$a_1$,$a_2$，...，$a_n$），在分割之前，我们还需要对连续属性进行排序（升序），这样可以大大减少运算量。排序后使得$a_1<a_2...<a_n$。取每两个值得中间值作为分割点，  
		    ![](https://github.com/sfxz035/DL-Learning/raw/master/picture/1557395438%281%29.png)  
		    根据是否大于某个中间值可以将属性划分为两类。选择使基尼指数或者方差最小的中间值作为划分两类的依据。
		  - 输入空间的属性值为多个离散值  
		    因为CART是一棵二叉树，每一次分裂只会产生两个节点。需要将多个离散值而分化。将其中一个离散值独立作为一个节点，其他的离散值生成另外一个节点即可。如果某离散属性一个有三个离散值X，Y，Z，则该属性的分裂方法有{X}、{Y，Z}，{Y}、{X，Z}，{Z}、{X，Y}，分别计算每种划分方法的基尼值或者样本方差确定最优的方法。
		- 
- 总结  
  - **ID3和C4.5生成的是多叉树** ，而CART必须是一棵**二叉树**。
  - C4.5和CART的对连续输入属性值的划分方法是一致的。
  - 无论是ID3还是C4.5最好在小数据集上使用，决策树分类一般只适用于小数据。当**属性取值很多时**最好选择C4.5算法，ID3得出的效果会非常差。
  - ID3只能对**分类变量**进行处理，C4.5和CART可以处理**连续和分类两种**自变量
  - ID3对**缺失值**敏感，而C4.5和CART对缺失值可以进行多种方式的处理
  - 只从**样本量考虑**，_小样本建议考虑c4.5、大样本建议考虑cart_。c4.5处理过程中需对数据集进行多次排序，处理成本耗时较高，而cart本身是一种大样本的统计方法，小样本处理下泛化误差较大
  -  ID3用信息增益来选择属性，信息增益容易偏向取值较多的特征的问题，C4.5采用信息增益率的方法，它是信息增益和特征熵的比值，特征数越多的特征对应的特征熵越大，它作为分母，可以校正信息增益偏向取值较多的特征的问题。问题是容易。


## 集成学习

- 基本思想：由多个学习器组合成一个性能更好的学习器
- 集成学习分为以下两类：  
	1. 串行集成方法，这种方法串行地生成基础模型
	2. 并行集成方法，这种方法并行地生成基础模型
- 学习器
  1. 强学习器亦叫做强可学习：是指一个模型针对学习任务结果具有很高的准确性；  
  2. 弱学习器亦叫做弱可学习：是指一个模型针对学习任务的结果比随机猜测的准确性好。
### 基本方法:
### 1. **Bagging**  
- Bagging 基于**并行策略**：基学习器之间不存在依赖关系，可同时生成。
- 代表算法/模型：  随机森林/神经网络的 Dropout 策略
- **方法过程**：
	- i)   从原始样本集中抽取训练集。每轮从原始样本集中使用Bootstraping（有放回）的方法抽取n个训练样本（在训练集中，有些样本可能被多次抽取到，而有些样本可能一次都没有被抽中）。共进行k轮抽取，得到k个训练集。（我们这里假设k个训练集之间是相互独立的，事实上不是完全独立）
	- ii)  每次使用一个训练集得到一个模型，k个训练集共得到k个模型。但是是同种模型。（注：，k个训练集虽然有重合不完全独立，训练出来的模型因为是同种模型也是不完全独立。这里并没有具体的分类算法或回归方法，我们可以根据具体问题采用不同的分类或回归方法，如决策树、感知器等）
	- iii)	对分类问题：将上步得到的k个模型采用投票的方式得到分类结果；对回归问题，计算上述模型的均值作为最后的结果。（所有模型的重要性相同）
- 相关总结  
	- Bagging通过**降低基分类器的方差**，改善了泛化误差
	- 其性能依赖于基分类器的稳定性；如果基分类器不稳定，bagging有助于降低训练数据的随机波动导致的误差；如果稳定，则集成分类器的误差主要由基分类器的偏倚引起
	- 由于每个样本被选中的概率相同，因此bagging并不侧重于训练数据集中的任何特定实例
	- 对于Bagging需要注意的是，每次训练集可以取全部的特征进行训练，也可以随机选取部分特征训练，例如随机森林就是每次随机选取部分特征
	- bagging对于弱学习器没有限制，这和Adaboost一样。但是最常用的一般也是决策树和神经网络。
	#### 随机森林（RF）
	在随机森林中，每个树模型都是装袋采样（**随机**）训练的。另外，特征也是**随机**选择的。它是Bagging算法的进化版，它是在bagging的基础上改进：
	>  两个随机性的引入对随机森林的分类性能至关重要。由于它们的引入，使得随机森林**不容易陷入过拟合**，并且具有很好得**抗噪能力**（比如：对缺省值不敏感）。
	
	- 具体方法：  
	  1. 在Bagging的基础上，RF使用CART决策树作为弱学习器。
		  >即如果训练集大小为N，对于每棵树而言，随机且有放回地从训练集中的抽取N个训练样本，作为该树的训练集。
	  
	  2. 如果每个样本的特征维度为M，指定一个常数m<<M，随机地从M个特征中选取m个特征子集，每次树进行分裂时，从这m个特征中选择最优的；
	  3. 每棵树都尽最大程度的生长，并且没有剪枝过程
	  4. 如果是分类算法预测，则T个弱学习器投出最多票数的类别或者类别之一为最终类别。如果是回归算法，T个弱学习器得到的回归结果进行算术平均得到的值为最终的模型输出。  
	  
	  当m=M时，RF的CART决策树和普通的CART决策树没有区别。m越小，则模型约健壮，当然此时对于训练集的拟合程度会变差。也就是说m越小，模型的方差会减小，但是偏倚会增大。在实际案例中，一般会通过交叉验证调参获取一个合适的m的值。
	
	- 小结  
		- 相关问题  
		  1. **为什么要随机抽样训练集？**  
		     如果不进行随机抽样，每棵树的训练集都一样，那么最终训练出的树分类结果也是完全一样的，这样的话完全没有bagging的必要；
		  2. **为什么要有放回地抽样？**  
		   我理解的是这样的：如果不是有放回的抽样，那么每棵树的训练样本都是不同的，都是没有交集的，这样每棵树都是"有偏的"，都是绝对"片面的"（当然这样说可能不对），也就是说每棵树训练出来都是有很大的差异的；而随机森林最后分类取决于多棵树（弱分类器）的投票表决，这种表决应该是"求同"，因此使用完全不同的训练集来训练每棵树这样对最终分类结果是没有帮助的，这样无异于是"盲人摸象"。

		 - **随机森林分类效果（错误率）与两个因素有关：**  
		   1. 森林中任意两棵树的相关性：相关性越大，错误率越大；
		   2. 森林中每棵树的分类能力：每棵树的分类能力越强，整个森林的错误率越低。  
		    
		    减小特征选择个数m，树的相关性和分类能力也会相应的降低；增大m，两者也会随之增大。所以关键问题是如何选择最优的m（或者是范围），这也是随机森林唯一的一个参数。
		 - 优缺点
			 - 优点  
			   1） 训练可以高度并行化，对于大数据时代的大样本训练速度有优势。
			   2） 由于可以随机选择决策树节点划分特征，这样在样本特征维度很高的时候，仍然能高效的训练模型。
			   3） 在训练后，可以给出各个特征对于输出的重要性
			   4） 由于采用了随机采样，训练出的模型的方差小，泛化能力强,不容易过拟合。
			   5） 相对于Boosting系列的Adaboost和GBDT， RF实现比较简单。
			   6） 对部分特征缺失不敏感。
			- 缺点  
			  1）在某些噪音比较大的样本集上，RF模型容易陷入过拟合。
			  2）取值划分比较多的特征容易对RF的决策产生更大的影响，从而影响拟合的模型的效果。
		 - 袋外错误率

### 2. **Boosting**  
- **Boosting 基于串行策略**：基学习器之间存在依赖关系，新的学习器需要依据旧的学习器生成。

- **代表算法/模型**：1. 提升方法 AdaBoost； 2. 提升树；3. 提升树中的梯度提升树 GBDT 

- **特点**：
  -   每次学习都会使用全部训练样本

- **方法过程**：  
  - i)	先从初始训练集训练出一个基学习器；
  - ii)  根据弱学习的学习误差率表现来更新训练样本的权重，使得之前弱学习器学习误差率高的训练样本点的权重变高
  - iii)  基于调整后的样本分布来训练下一个基学习器；
  - iiii) 重复进行上述步骤，直至基学习器数目达到事先指定的值T，最终将这T个基学习器进行加权结合。

- **Boosting 策略要解决的两个基本问题**：  
	1.  每一轮如何改变训练数据的权值或概率分布？
	2.  如何将弱分类器组合成一个强分类器？
    其他问题：3. 如何计算学习误差率e?		4. 如何得到弱学习器权重系数α?
	### **AdaBoost**  
	 >AdaBoost 是 Boosting 策略的一种具体算法 。基学习器可以使决策树，也可以是其他模型。 
	 
	 - **AdaBoost如何解决以上问题：**
		1. 每一轮如何改变数据的权值或概率分布？——开始时，每个样本的权值是一样的，AdaBoost 的做法是提高上一轮弱分类器错误分类样本的权值，同时降低那些被正确分类样本的权值。
		2. 如何将弱分类器组合成一个强分类器？—— AdaBoost 采取加权表决的方法（加法模型）。具体的，AdaBoost 会加大分类误差率小的基学习器的权值，使其在表决中起到更大的作用，同时减小分类误差率大的基学习器的权值。  
	
	- **AdaBoost算法思路**  
		- 输入：训练集$T={(x_1,y_1),..,(x_N,y_N)}, x_i ∈ R^n, y_i ∈ {-1,+1}$
		- 输出：最终学习器G(x)
		1） 初始化训练数据的权值分布
		  
			- $D_1=(w_{1,1},\cdots,w_{1,i},\cdots,w_{1,N}),\quad w_{1,i}=\frac{1}{N},\quad i=1,2,\cdots,N$
		
			2） 对于m=1,2,..,M，共M个弱分类器,通过以下步骤：  
			- i)  使用权值分布为$D_m$的**训练集**，得到基分类器：  
			  $G_m(x):\chi \rightarrow {-1,+1}$
			- ii) 计算$G_m$分类器的**误差率**  :
			  $\begin{aligned} e_m&=P(G_m(x_i)\neq y_i)\&=\sum_{i=1}^Nw_{m,i}\cdot {\color{Red} I(G_m(x_i)\neq y_i)} \end{aligned}$  
			  > `I(x)`  为指示函数：若`G(x)!=y`为真，则`I(G(x)!=y)=1`，反之为  `0`
	实际上分类误差率就等于所有**分类错误的数据的权值之和**
			
			- iii)  计算该分类器$G_m$的**权重**:  
			 
			  $\alpha_m=\frac{1}{2}\ln\frac{1-e_m}{e_m}$  
			  
			- iv) 更新训练集样本的权值分布（作为下一个分类器的训练集）：  
			  
			  ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/%E5%85%AC%E5%BC%8F_20180715140328.png)  
			  >其中 `Z_m` 为**规范化因子**，使 `D_m+1` 成为一个**概率分布**，类似 `Softmax` 函数
			
				另外：
				![](https://github.com/sfxz035/DL-Learning/raw/master/picture/%E5%85%AC%E5%BC%8F_20180715134916.png)
			  
			  因此 `w_{m+1,i}` 也可以写作:  
			  
			  ![](https://github.com/sfxz035/DL-Learning/raw/master/picture/%E5%85%AC%E5%BC%8F_20180715135945.png)  
			
			3） 构建基学习器的**线性组合**，得到最终的学习器：  
			
			$G(x)=\mathrm{sign}(\sum_{m=1}^M\alpha_mG_m(x))$  
		
	- **AdaBoost回归问题算法**  
		参考：[https://www.cnblogs.com/pinard/p/6133937.html]  
		回归问题算法和以上的分类算法大体一致，两点区别：
		1. 误差率计算的不同，回归问题对每个样本的计算相对误差(误差公式可选为平方差，指数误差等等)，再得到学习器的误差率。
		2. 强学习器的集成策略不同：回归采用的是对加权的弱学习器取权重中位数对应的弱学习器作为强学习器的方法
	- AdaBoost优缺点：
		- 优点  
		  1. Adaboost作为分类器时，分类精度很高
		  2. 不容易发生过拟合
		  3. 在Adaboost的框架下，可以使用各种回归分类模型来构建弱学习器，非常灵活
		- 缺点  
		  1. 对异常样本敏感，异常样本在迭代中可能会获得较高的权重，影响最终的强学习器的预测准确性
		  2. 在Adaboost训练过程中，Adaboost会使得难于分类样本的权值呈指数增长，训练将会过于偏向这类困难的样本，导致Adaboost算法易受噪声干扰。此外，Adaboost依赖于弱分类器，而弱分类器的训练时间往往很长。  
		  
	### **提升树**  
	- 以**决策树**为基学习器，对分类问题使用二叉分类树，回归问题使用二叉回归树。
	- 提升树是加法模型，则依然可以用前向分步式算法进行模型的求取，提升树模型则可表示为**决策树的加法模型**，
	- 对于**二分类问题**，提升树算法只需要将AdaBoost 算法中的基学习器限制为二叉分类树即可，**和上述的AdaBoost内容无差**。  
	 - 对于**回归问题**时，通过不断拟合残差得到新的树
	   **算法思路：**    
	   - 输入：训练集  `T={(x1,y1),..,(xN,yN)}, xi ∈ R^n, yi ∈ R`  
	   - 输出：回归提升树  `f_M(x)`   
	     
	    1. 初始化$f_0(x)=0$  
	    2. 对 `m=1,2,..,M`:  
	       a. 计算**残差** :  ${\color{Red} r_{m,i}}=y_i-f_{m-1}(x_i),\quad i=1,2,..,N$  
			  
	       b.拟合残差，学习一个回归树，得到$T(x_i;θ_m)$  
		      - i. **拟合残差**学习下一个回归树的参数  
		        $\hat{\Theta}_m=\arg\underset{\Theta_m}{\min}\sum_{i=1}^N L({\color{Red} r_{m,i}},{\color{Blue} T(x_i;\Theta_m)})$  
		     - ii. 由参数生成下一个回归树$T(x_i;θ_m)$   
		  
		   c. 更新 ${\color{Red}f_m}(x)=f_{m−1}(x)+T(x;θ_m)f_m(x)=f_{m−1}(x)+T(x;θ_m)$
		      
	      参考：[https://www.jianshu.com/p/7902b2eb5f21]
	 	 
	### **GBDT**  
	> GBDT中的DT是回归树，不是分类树，也就是说，只有回归树才有所谓的梯度提升。  
	
	### 前向分步算法
	前向分步算法提供了一种学习加法模型的普遍性方法。不同形式的基函数、不同形式的损失函数都可以用这种普遍性方法去求出加法模型的最优化参数，它是一种元算法  
	参考：[https://blog.csdn.net/xueyingxue001/article/details/51304490]

 
### **Bagging，Boosting二者之间的区别**

<!--stackedit_data:
eyJoaXN0b3J5IjpbLTExNjg0NTIxMjUsLTI3MTA1Mzk2OSwyNj
E5NjkyNjcsLTI3MjU4NjExMiwtMTIwNTcxMzgwNSwtMTAwMDc2
Nzc2MiwzNDAxMzU3MDMsLTE2NzMwNDIyOSwtMzM1NzMxODQ1LC
0xNTMzMTgxNzY0LDExODY0OTQ5NzMsLTQ0OTY4NjAwLDE5OTM4
MjE5NTQsMjExODk2NDI1MiwyMDQwNzI0MzE2LDE0NzI1MzAxMS
wyOTEzMjMxMjYsLTc2OTIzNzI0NywxMTAwOTQzMzM4LC0xNjI5
MDE2MTc4XX0=
-->