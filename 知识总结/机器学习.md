 ## **机器学习**

**偏差与方差**

**生成模型与判别模型**



----
### **SVM（支持向量机）**
#### SVM简介  
- 支持向量机（support vector machines）是一种二分类模型，它的目的是寻找一个超平面来对样本进行分割，分割的原则是间隔最大化，最终转化为一个凸二次规划问题来求解。它的基本模型是定义在特征空间上的**间隔最大的线性分类器**，间隔最大使它有别于感知机；支持向量机还包括**核技巧**，这使其成为实质上的**非线性分类器**。
- 由简至繁的支持向量机分类
  - 当训练数据**线性可分**时，通过**硬间隔最大化**，学习一个**线性分类器**，即线性可分支持向量机，又称**硬间隔支持向量机**。
  - 当训练数据**接近线性可分**时，通过**软间隔最大化**，学习一个线性分类器，即线性支持向量机，又称**软间隔支持向量机**。
  - 当训练数据**线性不可分**时，通过使用**核技巧**及软间隔最大化，学习非线性支持向量机。  
#### 相关定义
- 支持向量  
     训练数据集中与分离超平面距离最近的样本点的实例称为支持向量。
     - 在寻找分离超平面的时候，一般只需看两类数据，它们各自最靠近划分直线的那些点，而其他的点起不了决定作用。这些决定超平面的点就叫支撑向量。
- 函数间隔和几何间隔  
  - 函数间隔(间隔)  
    所有点中最小为函数间隔 
   ![enter image description here](https://raw.githubusercontent.com/sfxz035/DL-Learning/master/picture/1556094678%281%29.jpg?token=AH7MXQ6FUNCG2AYGJMND6TS4YAPZI)
  - 几何间隔  
    所有点中最小为几何间隔
    ![](https://raw.githubusercontent.com/sfxz035/DL-Learning/master/picture/1556094722%281%29.jpg?token=AH7MXQ6MPEBF2K6NEPCFJ724YASF4)
    由于不同的训练集各点的间距尺度不太一样，因此用间隔（而不是几何间隔）来衡量有利于我们表达形式的简洁。
 #### 基本知识  
 - 间隔最大化  
   一个简单的二维空间的例子，一边代表正类，一边代表负类，样本是线性可分的，但是很显然不只有这一条直线可以将样本分开，而是有无数条，我们所说的**线性可分支持向量机**就对应着能将数据正确划分并且**间隔最大**的直线。
		   ![enter image description here](https://raw.githubusercontent.com/sfxz035/DL-Learning/master/picture/1556095720%281%29.jpg?token=AH7MXQ6XZEKDJQIT6L35ZPS4YARUW)
     - **两类点分布在支持向量两端**：
       1. 方法一    
          **固定函数间隔为1**
          两类点分布在两端满足以下
![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556011439%281%29.jpg)
	      该公式等价于![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556012611%281%29.jpg)
	         该公式被称为最大间隔假设。即**固定函数间隔为1**（即支持向量的函数间隔为1），使所有点（同样计算方式）要大于函数间隔，从而**保证两类点分别在支持向量的两端**，而不是支持向量的中间。  
	   2. 方法二
	     两类点分布在两端，即所有点（与几何间隔同意计算方式）要大于几何间隔：
	       	     ![](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556098617%281%29.jpg)  
	     **固定函数间隔为1** 后化简为：  
	     ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556012611%281%29.jpg)
   - **最大化几何间隔**  
     1. 推导方法一
     i 几何间隔就等于两个异类支持向量的差在 w 上的投影，即：     ![](https://raw.githubusercontent.com/sfxz035/DL-Learning/master/picture/20180328160123465.png?token=AH7MXQ7BKZKFGDJMFXSVQJC4YAQ7U)
	  ii **固定函数间隔为1** 时，正负支持向量又满足以下公式：  
	 	![enter image description here](https://raw.githubusercontent.com/sfxz035/DL-Learning/master/picture/20180328160552158.png?token=AH7MXQ4PVUBPX5OG2DG2X5K4YAQ6I)
	   推导可得  
	   ![](https://raw.githubusercontent.com/sfxz035/DL-Learning/master/picture/1556095526%281%29.jpg?token=AH7MXQ65BV6JRC6CHU2PP2C4YARJC)
	   2. 推导方法二  
	      i 最大化几何间隔：  
	     $\begin{aligned} &{\color{Red} \underset{w,b}{\max}} \quad\gamma \ & \quad\end{aligned}$  
	     ii **固定函数间隔为1**，最大化 `1/||w||` 等价于最小化`1/2*||w||^2`,所以  
	     ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556100245%281%29.jpg)
	- **综上**
	     - 最大化支持向量的几何间隔的同时，保证所有点的间隔大于等于1（即保证），满足这两个条件。于是就有了获得线性支持向量机的以下约束条件：
		     ![enter image description here](https://raw.githubusercontent.com/sfxz035/DL-Learning/master/picture/1556097977%281%29.jpg?token=AH7MXQY7MZ5BPZDHJKXJE2S4YAWES)
		   以上皆是在固定间隔（例如固定为1），寻找最小的||w||。为什么令`γ^=1`？——比例改变`(ω,b)`，超平面不会改变，但函数间隔`γ^`会成比例改变，因此可以通过等比例改变`(ω,b)`使函数间隔`γ^=1`。
		 - 该约束最优化问题即为**线性支持向量机**的标准问题——这是一个**凸二次优化**问题，可以使用商业 QP 代码完成。理论上，线性 SVM 的问题已经解决了；但在高等数学中，**带约束的最优化问题**还可以用另一种方法求解——**拉格朗日乘子法**。该方法的优点一是更容易求解，而是自然引入**核函数**，进而推广到非线性的情况。
#### 对偶推导  
- 1 构建拉格朗日函数  
  $\begin{aligned} L(w,b,{\color{Red} \alpha})=&\frac{1}{2}w^Tw-\sum_{i=1}^N{\color{Red} \alpha_i}[y_i(w^Tx_i+b)-1]\ &{\color{Red} \alpha_i \geq 0},\quad i=1,2,\cdots,N \end{aligned}$  
- 2 求导进而求 `L` 对 `(w,b)` 的极小  
  $\begin{aligned} \mathrm{set}\quad \frac{\partial L}{\partial w}=0 ;;&\Rightarrow; w-\sum_{i=1}^N {\color{Red} \alpha_i y_i x_i}=0\ &\Rightarrow; w=\sum_{i=1}^N {\color{Red} \alpha_i y_i x_i} \end{aligned}$  
  $\begin{aligned} \mathrm{set}\quad \frac{\partial L}{\partial b}=0 ;;&\Rightarrow; \sum_{i=1}^N {\color{Red} \alpha_i y_i}=0 \end{aligned}$  
  代入L，得：  
  ![](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556186443%281%29.jpg)
  即  
  ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556186536%281%29.jpg)
- 3 求 `L` 对 `α` 的极大，即  
		    ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556186654%281%29.jpg)
    - 该问题的对偶问题  
      ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556186722%281%29.jpg)  
      于是，标准问题最后等价于求解该**对偶问题**  
 - 4 求解：设 `α` 的解为 `α*`，则存在下标`j`使`α_j > 0`，可得标准问题的解为：  
   ![
](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556186771%281%29.jpg) 
   可得分离超平面及分类决策函数为：  
   ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556186319%281%29.jpg)
#### 非线性支持向量机（核函数）  
	通过上述的对偶问题求解方法，可以解决线性可分支持向量机的问题。以下方式是解决非线性支持向量机问题。  
当遇到非线性问题时，线性模型已经无法将样本分开。非线性模型不好求解，我们依然希望通过线性模型来完成分类。所以可以通过将训练样本从原始空间映射到一个更高维的特征空间，使得样本在这个空间中线性可分，如果原始空间维数是有限的，即属性是有限的，那么一定存在一个高维特征空间使样本可分。**令${\phi(x)}$为x的高维特征空间的表示**，于是可得：  
![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/1556355513%281%29.jpg)  

<!--stackedit_data:
eyJoaXN0b3J5IjpbOTU2NjI3Nzc1LDgyNzQ0Nzg4OSwxNzM2OD
UwMDUwLDE2NTgwNzcwMzQsLTEyOTg4OTQ2NjcsMjA0NjIzNDMz
NSwtMzIxMDgzODU4LC0xMTg1NjE1NjA2LC0xMTk4ODU1MTY2LC
05MTMzODgwNiwxNTAxNDI0Mzg4LC0yMTEwMzkyNzIxLDE3NTM4
ODU3ODcsOTc1MTM3Mjg2LDE3MzQ0MDE3NDUsMTAxNTM1NTQ1LC
04NDQ3NjM1OSwtMTQxNzk1Mzk4MywxMTcyMjMxMzUzLDEwMTA4
ODUxMTBdfQ==
-->