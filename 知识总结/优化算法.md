# 优化算法  
机器学习中，有很多优化方法来试图寻找模型的最优解。  
## 梯度下降
##  动量
## 自适应学习率优化算法  
  自适应学习率优化算法针对于机器学习模型的学习率  
  目前的自适应学习率优化算法主要有：**AdaGrad算法**，**RMSProp算法**，**Adam算法**以及**AdaDelta算法**。
  - **AdaGrad**  
    - **思想**  
      - AdaGrad算法，独立地适应所有模型参数的学习率，缩放每个参数反比于其所有梯度历史平均值总和的平方根。具有代价函数最大梯度的参数相应地有个快速下降的学习率，而具有小梯度的参数在学习率上有相对较小的下降。
    - **算法描述**  
      我们设$g_{t,i}$为第t轮第i个参数的梯度，即$g_{t,i}=\triangledown_\Theta J(\Theta_i)$。则Adagrad在每轮训练中对每个参数$θ_i$的学习率进行更新，参数更新公式如下:   
      ![](https://github.com/sfxz035/DL-Learning/raw/master/picture/5005591-0529f9aa9b0bcff0.png)  
      其中，$G_t∈ {R}^{d\times d}$为对角矩阵，每个对角线位置i,i为对应参数$θ_i$从第1轮到第t轮梯度的平方和。ϵ是平滑项，用于避免分母为0，一般取值1e−8。如果不执行均方根操作，算法的性能将会变得很差。
    - **性能**  
      - 从表达式可以看出，对出现比较多的类别数据，Adagrad给予越来越小的学习率，而对于比较少的类别数据，会给予较大的学习率。针对高频特征更新步长较小，而低频特征更新较大。因此该算法适合应用在特征稀疏的场景。
      - Adagrad算法主要的**缺点**在于，其分母梯度平方的累加和。因为每次加入的都是一个正数，随着训练的进行，学习率将会变得无限小，此时算法将不能进行参数的迭代更新。会导致训练困难，甚至提前结束。
      - Adagrad算法的主要**优点**是它避免了手动调整学习率的麻烦，大部分的实现都采用默认值0.01。
  - **RMSProp**  
    Adagrad会累加之前所有的梯度平方，而RMSprop仅仅是计算对应的平均值，因此可缓解Adagrad算法学习率下降较快的问题  
    - 算法描述  
      
      $E[g^2]_t=γE[g^2]_{t−1}+(1-γ)g_t^2$  
      
      $θ_{t+1}=θ_t−\frac{η}{\sqrt{E[g^2]_t+ϵ}g_t$  
      - **RMSProp** 建议的**初始值**：全局学习率 η=1e-3，衰减速率 γ=0.9。ϵ一般取 `1e-8 ~ 1e-10`（Tensorflow 中的默认值为 `1e-10`）
    - 性能  
      - 由于取了个加权平均，避免了学习率越来越低的的问题，而且能自适应地调节学习率。
  - **AdaDelta**  
    Adadelta算法是adagrad算法的改进版，AdaDelta 和 RMSProp 都是为了解决 AdaGrad 对学习率过度衰减的问题而产生的。  
    AdaDelta 进一步解决了 AdaGrad 需要设置一个全局学习率的问题  
    - 算法描述  
        1. $E[g^2]_t=γE[g^2]_{t−1}+(1−γ)g^2_t$  ，$RMS[g]t=\sqrt{E[g^2]t+ϵ}$  
        2. $E[Δθ^2]_t=γE[Δθ^2]_{t−1}+(1−γ)Δθ^2_t$，$RMS[Δθ]_t=\sqrt{E[Δθ^2]_t+ϵ}$
        3. $$
    - 性能
  - 

<!--stackedit_data:
eyJoaXN0b3J5IjpbMTI1NzEwNzAwOCwtNzQ4NTA0Mjk1LC0xOD
ExMzgwMDQ0LC0xMzYwMDc3OTUzLDU1Mzc3MDA3MiwtMTI0MDU1
MTc3NSwxMjQ3MzE0MTMxLC04NjA1MDE0NjQsLTI5NDcwMzgzOC
wyMzIyOTYyODldfQ==
-->