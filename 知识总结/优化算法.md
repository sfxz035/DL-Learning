# 优化算法  
机器学习中，有很多优化方法来试图寻找模型的最优解。  
## 梯度下降
##  动量（Momentum）  算法
### 带动量的 SGD
动量可以加速SGD算法的收敛速度，并且降低SGD算法收敛时的震荡  
- 算法描述  
  $v_t=γv_{t−1}+η∇_θJ(θ)$
  $θ=θ−v_t$
- 性能  
	- 在之前的SGD中，参数的更新方向十分依赖当前的batch，因此不稳定。加入momentum后，能够让参数一定程度上按照之前变化方向进行更新，使参数更稳定的更新
	- 引入**动量**（Momentum）方法一方面是为了解决“峡谷”和“鞍点”问题；一方面也可以用于SGD 加速，特别是针对**高曲率**、小幅但是方向一致的梯度。
	- **动量**方法相当于把纸团换成了**铁球**；不容易受到外力的干扰，轨迹更加稳定；同时因为在鞍点处因为**惯性**的作用，更有可能离开平地。
	- 可能更容易冲出局部最小值。
### NAG  
- 相关
	- 这个“**提前量**”的设计让算法有了对前方环境“**预判**”的能力。Nesterov 动量可以解释为往标准动量方法中添加了一个**修正因子**。
## 自适应学习率优化算法  
  自适应学习率优化算法针对于机器学习模型的学习率  
  目前的自适应学习率优化算法主要有：**AdaGrad算法**，**RMSProp算法**，**Adam算法**以及**AdaDelta算法**。
  ### **AdaGrad**  
   - **思想**  
      - AdaGrad算法，独立地适应所有模型参数的学习率，缩放每个参数反比于其所有梯度历史平均值总和的平方根。具有代价函数最大梯度的参数相应地有个快速下降的学习率，而具有小梯度的参数在学习率上有相对较小的下降。
   - **算法描述**  
      我们设$g_{t,i}$为第t轮第i个参数的梯度，即$g_{t,i}=\triangledown_\Theta J(\Theta_i)$。则Adagrad在每轮训练中对每个参数$θ_i$的学习率进行更新，参数更新公式如下:   
      ![](https://github.com/sfxz035/DL-Learning/raw/master/picture/5005591-0529f9aa9b0bcff0.png)  
      其中，$G_t∈ {R}^{d\times d}$为对角矩阵，每个对角线位置i,i为对应参数$θ_i$从第1轮到第t轮梯度的平方和。ϵ是平滑项，用于避免分母为0，一般取值1e−8。如果不执行均方根操作，算法的性能将会变得很差。
    - **性能**  
      - 从表达式可以看出，对出现比较多的类别数据，Adagrad给予越来越小的学习率，而对于比较少的类别数据，会给予较大的学习率。针对高频特征更新步长较小，而低频特征更新较大。因此该算法适合应用在特征稀疏的场景。
      - Adagrad算法主要的**缺点**在于，其分母梯度平方的累加和。因为每次加入的都是一个正数，随着训练的进行，学习率将会变得无限小，此时算法将不能进行参数的迭代更新。会导致训练困难，甚至提前结束。  
         需要设置初始学习率
      - Adagrad算法的主要**优点**是它避免了手动调整学习率的麻烦，大部分的实现都采用默认值0.01。即不用依赖于全局学习率了。
### **RMSProp**  
   Adagrad会累加之前所有的梯度平方，而RMSprop仅仅是计算对应的平均值，进行了**加权平均**，因此可缓解Adagrad算法学习率下降较快的问题  
   - 算法描述  
      
      $E[g^2]_t=γE[g^2]_{t−1}+(1-γ)g_t^2$  
      
      $θ_{t+1}=θ_t−\frac{η}{\sqrt{E[g^2]_t+ϵ}g_t}$  
      - **RMSProp** 建议的**初始值**：全局学习率 η=1e-3，衰减速率 γ=0.9。ϵ一般取 `1e-8 ~ 1e-10`（Tensorflow 中的默认值为 `1e-10`）
    - 性能  
      - 由于取了个加权平均，避免了学习率越来越低的的问题，而且能自适应地调节学习率。
      - 适合处理非平稳目标
      - 需要设置初始学习率，还需要设置衰减值。

### **AdaDelta**  
 Adadelta算法是adagrad算法的改进版，AdaDelta 和 RMSProp 都是为了解决 AdaGrad 对学习率过度衰减的问题而产生的。  
    AdaDelta 进一步解决了 AdaGrad 需要设置一个全局学习率的问题  
   - **算法描述**  
        1. $E[g^2]_t=γE[g^2]_{t−1}+(1−γ)g^2_t$   
        
            $RMS[g]t=\sqrt{E[g^2]t+ϵ}$  
        2. $E[Δθ^2]_t=γE[Δθ^2]_{t−1}+(1−γ)Δθ^2_t$   
         
            $RMS[Δθ]_t=\sqrt{E[Δθ^2]_t+ϵ}$
        3. 因为$RMS[Δθ]_t$是未知的，故用$RMS[Δθ]_{t−1}来更新$：  
         
	        $Δθ_t=−\frac{RMS[Δθ]_{t−1}}{RMS[g]_t}g_t$  
        5. $θ_{t+1}=θ_t+Δθ_t$  
    - 性能  
	    - 不需要设置学习率，还解决了adagrad算法单调递减学习率的问题 
	    - 训练后期，反复在局部最小值附近抖动
### **Adam**  

Adam 在 RMSProp 方法的基础上更进一步：    
- **算法描述**    
  1. 和Adadelta、RMSProp算法相比，除了加入历史梯度平方的指数衰减平均外，adam算法还保留了**历史梯度的指数衰减平均**（m），相当于**动量**  
       - 计算有偏一阶矩估计： $m_t=β_1m_{t−1}+(1−β_1)g_t$
       - 计算有偏二阶矩估计：  $v_t=β_2v_{t−1}+(1−β_2)g^2_t$  
            >当mt和vt初始化为0向量时，adam的作者发现他们都偏向于0，尤其是在初始化的时候和衰减率很小的时候（例如，beta1和beta2趋近于1时）。通过计算偏差校正的一阶矩和二阶矩估计来抵消偏差（即步骤2）
         
   2. 偏置修正  
        - 修正一阶矩的偏差： $\hat{m_t}=\frac{m_t}{1−β^t_1}$
        - 修正一阶矩的偏差： $\hat{v_t}=\frac{v_t}{1−β^t_2}$
  3. 更新参数  
           $θ_{t+1}=θ_t−\frac{η}{\sqrt{\hat{v_t}}+ϵ}\hat{m_t}$  
        - 建议初始值  
             β1=0.9 , β2=0.999, and $ϵ=10^{−8}$
 - **偏差修正**
	-   注意到，`s`  和  `r`  需要初始化为  `0`；且  `ρ1`  和  `ρ2`  推荐的初始值都很接近  `1`（`0.9`  和  `0.999`）
	-   这将导致在训练初期  `m`  和  `v`  都很小（偏向于 0），从而训练缓慢。
	-   因此，Adam 通过修正偏差来抵消这个倾向。
 - **性能**：  
	- adam和Adadelta、RMSProp算法相比
	     - 除了加入历史梯度平方的指数衰减平均外，还保留了**历史梯度的指数衰减平均**（m），相当于**动量**
         - 加入了**偏置修正**，相比于缺少修正因子导致二阶矩估计可能在训练初期具有很高偏置的RMSProp，Adam修正了从原点初始化的一阶矩（动量项）和（非中心的）二阶矩估计。
         - Adam 行为就像一个带有摩擦力的小球，在误差面上倾向于平坦的极小值。
     - 优点  
       1. 经过偏置校正后，每一次迭代学习率都有个确定范围，使得参数比较平稳。  
       2. 结合了Adagrad善于处理稀疏梯度和RMSprop善于处理非平稳目标的优点
       3. 为不同的参数计算不同的自适应学习率

<!--stackedit_data:
eyJoaXN0b3J5IjpbLTE0MTIyMzYwMTYsMTIxODQxMzE2MSwtMT
I4NTk5NjQwOSwtMTY4NTkyMzA2Miw5Nzc5OTExNTEsMjA4ODY0
OTIzMSwtMTgyMzgwODUzNywtMTUzNjc4NTI3MywtNzQ4NTA0Mj
k1LC0xODExMzgwMDQ0LC0xMzYwMDc3OTUzLDU1Mzc3MDA3Miwt
MTI0MDU1MTc3NSwxMjQ3MzE0MTMxLC04NjA1MDE0NjQsLTI5ND
cwMzgzOCwyMzIyOTYyODldfQ==
-->