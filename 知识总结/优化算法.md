# 优化算法  
机器学习中，有很多优化方法来试图寻找模型的最优解。  
## 梯度下降
##  动量
## 自适应学习率优化算法  
  自适应学习率优化算法针对于机器学习模型的学习率  
  目前的自适应学习率优化算法主要有：**AdaGrad算法**，**RMSProp算法**，**Adam算法**以及**AdaDelta算法**。
  - **AdaGrad**  
    - 思想  
      - AdaGrad算法，独立地适应所有模型参数的学习率，缩放每个参数反比于其所有梯度历史平均值总和的平方根。具有代价函数最大梯度的参数相应地有个快速下降的学习率，而具有小梯度的参数在学习率上有相对较小的下降。
      - 针对高频特征更新步长较小，而低频特征更新较大。因此该算法适合应用在特征稀疏的场景。
    - 算法描述
<!--stackedit_data:
eyJoaXN0b3J5IjpbMTI0NzMxNDEzMSwtODYwNTAxNDY0LC0yOT
Q3MDM4MzgsMjMyMjk2Mjg5XX0=
-->