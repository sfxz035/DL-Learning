# 优化算法  
机器学习中，有很多优化方法来试图寻找模型的最优解。  
## 梯度下降  
### Batch gradient descent  
每次更新我们需要计算整个数据集的梯度
- 算法公式  
  $θ=θ−η⋅∇_θJ(θ)$
### SGD(Stochastic gradient descent)
每读入一个数据，便立刻计算cost fuction的梯度来更新参数
- 算法公式    
  $θ=θ−η⋅∇_θJ(θ;x^{(i)};y^{(i)})$
### Min-batch gradient descent  
mini-batch Gradient Descent的方法是在上述两个方法中取折衷, 每次从所有训练数据中取一个子集（mini-batch） 用于计算梯度：
- 算法公式    
  $θ=θ−η⋅∇_θJ(θ;x^{(i:i+n)};y^{(i:i+n)})$
### “批”的大小对优化效果的影响
-   **较大的批能得到更精确的梯度估计**，但回报是小于线性的。
-   **较小的批能带来更好的泛化误差**，泛化误差通常在批大小为 1 时最好。
    -   原因可能是由于小批量在学习过程中带来了**噪声**，使产生了一些正则化效果 (Wilson and Martinez, 2003)
    -   但是，因为梯度估计的高方差，小批量训练需要**较小的学习率**以保持稳定性，这意味着**更长的训练时间**。
-   当批的大小为  **2 的幂**时能充分利用矩阵运算操作，所以批的大小一般取 32、64、128、256 等。
### 随机梯度下降存在的问题

-   随机梯度下降（SGD）放弃了**梯度的准确性**，仅采用一部分样本来估计当前的梯度；因此 SGD 对梯度的估计常常出现偏差，造成目标函数收敛不稳定，甚至不收敛的情况。
-   无论是经典的梯度下降还是随机梯度下降，都可能陷入**局部极值点**；除此之外，SGD 还可能遇到“**峡谷**”和“**鞍点**”两种情况
    -   **峡谷**类似一个带有**坡度**的狭长小道，左右两侧是“**峭壁**”；在**峡谷**中，准确的梯度方向应该沿着坡的方向向下，但粗糙的梯度估计使其稍有偏离就撞向两侧的峭壁，然后在两个峭壁间来回**震荡**。
    -   **鞍点**的形状类似一个马鞍，一个方向两头翘，一个方向两头垂，而**中间区域近似平地**；一旦优化的过程中不慎落入鞍点，优化很可能就会停滞下来。
##  动量（Momentum）  算法
### 带动量的 SGD
动量可以加速SGD算法的收敛速度，并且降低SGD算法收敛时的震荡  
- 算法描述  
  $v_t=γv_{t−1}+η∇_θJ(θ)$  
  $θ=θ−v_t$
- 性能  
	- 在之前的SGD中，参数的更新方向十分依赖当前的batch，因此不稳定。加入momentum后，能够让参数一定程度上按照之前变化方向进行更新，使参数更稳定的更新
	- 引入**动量**（Momentum）方法一方面是为了解决“峡谷”和“鞍点”问题；一方面也可以用于SGD 加速，特别是针对**高曲率**、小幅但是方向一致的梯度。
	- **动量**方法相当于把纸团换成了**铁球**；不容易受到外力的干扰，轨迹更加稳定；同时因为在鞍点处因为**惯性**的作用，更有可能离开平地。
	- 可能更容易冲出局部最小值。
### NAG  
我们希望有一个智能的雪球，它能够预知运动的方向，以至于当它再次遇到斜坡的时候会减慢速度。我们可以通过计算来渐进估计下一个位置的参数（梯度并不是完全更新）
- 算法描述  
  $v_t=γv_{t−1}+η∇θJ(θ−γv_{t−1})$  
  $θ=θ−v_t$
- 相关
	- 这个“**提前量**”的设计让算法有了对前方环境“**预判**”的能力。Nesterov 动量可以解释为往标准动量方法中添加了一个**修正因子**。
	- 使我们避免过快地前进，并提高了算法地响应能力
## 自适应学习率优化算法  
  自适应学习率优化算法针对于机器学习模型的学习率  
  目前的自适应学习率优化算法主要有：**AdaGrad算法**，**RMSProp算法**，**Adam算法**以及**AdaDelta算法**。
  ### **AdaGrad**   
   - **思想**  
      - AdaGrad算法，独立地适应所有模型参数的学习率，缩放每个参数反比于其所有梯度历史平均值总和的平方根。具有代价函数最大梯度的参数相应地有个快速下降的学习率，而具有小梯度的参数在学习率上有相对较小的下降。
   - **算法描述**  
      我们设$g_{t,i}$为第t轮第i个参数的梯度，即$g_{t,i}=\triangledown_\Theta J(\Theta_i)$。则Adagrad在每轮训练中对每个参数$θ_i$的学习率进行更新，参数更新公式如下:   
      ![enter image description here](https://lh3.googleusercontent.com/YriSbZlaAbTHddtftWNZtf3yQjvyALOjHNlcAWZgeQOuxG7JyqhdwoC-OVtgovj5bY8yuOLdy31s)  
      其中，$G_t∈ {R}^{d\times d}$为对角矩阵，每个对角线位置i,i为对应参数$θ_i$从第1轮到第t轮梯度的平方和。ϵ是平滑项，用于避免分母为0，一般取值1e−8。如果不执行均方根操作，算法的性能将会变得很差。
 - **性能**  
      - 从表达式可以看出，对出现比较多的类别数据，Adagrad给予越来越小的学习率，而对于比较少的类别数据，会给予较大的学习率。针对高频特征更新步长较小，而低频特征更新较大。因此该算法适合应用在特征稀疏或者分布不平衡的数据集。
      - Adagrad算法主要的**缺点**在于，其分母梯度平方的累加和。因为每次加入的都是一个正数，随着训练的进行，学习率将会变得无限小，此时算法将不能进行参数的迭代更新。会导致训练困难，甚至提前结束。  
         需要设置初始学习率
      - Adagrad算法的主要**优点**是它避免了手动调整学习率的麻烦，大部分的实现都采用默认值0.01。即不用依赖于全局学习率了。
### **RMSProp**  
   Adagrad会累加之前所有的梯度平方，而RMSprop仅仅是计算对应的平均值，进行了**加权平均**，因此可缓解Adagrad算法学习率下降较快的问题  
   - 算法描述  
      
      $E[g^2]_t=\gamma E[g^2]_{t−1}+(1-\gamma)g_t^2$  
      
      $θ_{t+1}=θ_t−\frac{η}{\sqrt{E[g^2]_t+ϵ}}g_t$  
      - **RMSProp** 建议的**初始值**：全局学习率 η=1e-3，衰减速率 γ=0.9。ϵ一般取 `1e-8 ~ 1e-10`（Tensorflow 中的默认值为 `1e-10`）
    - 性能  
      - 由于取了个加权平均，避免了学习率越来越低的的问题，而且能自适应地调节学习率。
      - 适合处理非平稳目标
      - 需要设置初始学习率，还需要设置衰减值。 

### **AdaDelta**  
 Adadelta算法是adagrad算法的改进版，AdaDelta 和 RMSProp 都是为了解决 AdaGrad 对学习率过度衰减的问题而产生的。  
    AdaDelta 进一步解决了 AdaGrad 需要设置一个全局学习率的问题  
   - **算法描述**  
        1.  $E[g^2]_t=γE[g^2]_{t−1}+(1−γ)g^2_t$   
        
            $RMS[g]t=\sqrt{E[g^2]t+ϵ}$  
        2. $E[Δθ^2]_t=γE[Δθ^2]_{t−1}+(1−γ)Δθ^2_t$   
         
            $RMS[Δθ]_t=\sqrt{E[Δθ^2]_t+ϵ}$
        3. 因为$RMS[Δθ]_t$是未知的，故用$RMS[Δθ]_{t−1}来更新$：  
         
	        $Δθ_t=−\frac{RMS[Δθ]_{t−1}}{RMS[g]_t}g_t$  
        5. $θ_{t+1}=θ_t+Δθ_t$  
    - 性能  
	    - 不需要设置学习率，还解决了adagrad算法单调递减学习率的问题 
	    - 训练后期，反复在局部最小值附近抖动
### **Adam**  

Adam 在 RMSProp 方法的基础上更进一步：    
- **算法描述**    
  1. 和Adadelta、RMSProp算法相比，除了加入历史梯度平方的指数衰减平均外，adam算法还保留了**历史梯度的指数衰减平均**（m），相当于**动量**  
       - 计算有偏一阶矩估计： $m_t=β_1m_{t−1}+(1−β_1)g_t$
       - 计算有偏二阶矩估计：  $v_t=β_2v_{t−1}+(1−β_2)g^2_t$  
            >当mt和vt初始化为0向量时，adam的作者发现他们都偏向于0，尤其是在初始化的时候和衰减率很小的时候（例如，beta1和beta2趋近于1时）。通过计算偏差校正的一阶矩和二阶矩估计来抵消偏差（即步骤2）
         
   2. 偏置修正  
        - 修正一阶矩的偏差： $\hat{m_t}=\frac{m_t}{1−β^t_1}$
        - 修正一阶矩的偏差： $\hat{v_t}=\frac{v_t}{1−β^t_2}$
  3. 更新参数  
           $θ_{t+1}=θ_t−\frac{η}{\sqrt{\hat{v_t}}+ϵ}\hat{m_t}$  
        - 建议初始值  
             β1=0.9 , β2=0.999, and $ϵ=10^{−8}$
 - **偏差修正**
	-   注意到，`s`  和  `r`  需要初始化为  `0`；且  `ρ1`  和  `ρ2`  推荐的初始值都很接近  `1`（`0.9`  和  `0.999`）
	-   这将导致在训练初期  `m`  和  `v`  都很小（偏向于 0），从而训练缓慢。
	-   因此，Adam 通过修正偏差来抵消这个倾向。
 - **性能**：  
	- adam和Adadelta、RMSProp算法相比
	     - 除了加入历史梯度平方的指数衰减平均外，还保留了**历史梯度的指数衰减平均**（m），相当于**动量**
         - 加入了**偏置修正**，相比于缺少修正因子导致二阶矩估计可能在训练初期具有很高偏置的RMSProp，Adam修正了从原点初始化的一阶矩（动量项）和（非中心的）二阶矩估计。
         - Adam 行为就像一个带有摩擦力的小球，在误差面上倾向于平坦的极小值。
     - 优点  
       1. 经过偏置校正后，每一次迭代学习率都有个确定范围，使得参数比较平稳。  
       2. 结合了Adagrad善于处理稀疏梯度和RMSprop善于处理非平稳目标的优点
       3. 为不同的参数计算不同的自适应学习率



<!--stackedit_data:
eyJoaXN0b3J5IjpbLTM3MDk4MzkwMyw2MzIyOTczNjEsNjQ3ND
Q0MDUxLDYzNDYxMDAwOSwxNzk4NzYzNjc3LC0xODkyNzMzNjYz
LC00NTEyNDcwMDAsLTQ1MTI0NzAwMCwtNTMxNzM2MTQ1LC05MD
M1ODExMDEsLTE0Nzc3MDY0NDMsLTE0MTIyMzYwMTYsMTIxODQx
MzE2MSwtMTI4NTk5NjQwOSwtMTY4NTkyMzA2Miw5Nzc5OTExNT
EsMjA4ODY0OTIzMSwtMTgyMzgwODUzNywtMTUzNjc4NTI3Mywt
NzQ4NTA0Mjk1XX0=
-->