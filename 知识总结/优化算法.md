# 优化算法  
机器学习中，有很多优化方法来试图寻找模型的最优解。  
## 梯度下降
##  动量
## 自适应学习率优化算法  
  自适应学习率优化算法针对于机器学习模型的学习率  
  目前的自适应学习率优化算法主要有：**AdaGrad算法**，**RMSProp算法**，**Adam算法**以及**AdaDelta算法**。
  - **AdaGrad**  
    - 思想  
      - AdaGrad算法，独立地适应所有模型参数的学习率，缩放每个参数反比于其所有梯度历史平均值总和的平方根。具有代价函数最大梯度的参数相应地有个快速下降的学习率，而具有小梯度的参数在学习率上有相对较小的下降。
      - 针对高频特征更新步长较小，而低频特征更新较大。因此该算法适合应用在特征稀疏的场景。
    - 算法描述  
      我们设gt,i g_{t,i}g 
t,i
​	
 为第t轮第i个参数的梯度，即gt,i=▽ΘJ(Θi) g_{t,i}=\triangledown_\Theta J(\Theta_i)g 
t,i
​	
 =▽ 
Θ
​	
 J(Θ 
i
​	
 )
--------------------- 
作者：Joe-Han 
来源：CSDN 
原文：https://blog.csdn.net/u010089444/article/details/76725843 
版权声明：本文为博主原创文章，转载请附上博文链接！Adagrad在每轮训练中对每个参数θi θ_iθi的学习率进行更新，参数更新公式如下：

<!--stackedit_data:
eyJoaXN0b3J5IjpbNjE4NjM3MTQ5LDEyNDczMTQxMzEsLTg2MD
UwMTQ2NCwtMjk0NzAzODM4LDIzMjI5NjI4OV19
-->