# 优化算法  
机器学习中，有很多优化方法来试图寻找模型的最优解。  
## 梯度下降
##  动量
## 自适应学习率优化算法  
  自适应学习率优化算法针对于机器学习模型的学习率  
  目前的自适应学习率优化算法主要有：**AdaGrad算法**，**RMSProp算法**，**Adam算法**以及**AdaDelta算法**。
  - **AdaGrad**  
    - 思想  
      - AdaGrad算法，独立地适应所有模型参数的学习率，缩放每个参数反比于其所有梯度历史平均值总和的平方根。具有代价函数最大梯度的参数相应地有个快速下降的学习率，而具有小梯度的参数在学习率上有相对较小的下降。
      - 针对高频特征更新步长较小，而低频特征更新较大。因此该算法适合应用在特征稀疏的场景。
    - 算法描述  
      我们设$g_{t,i}$为第t轮第i个参数的梯度，即$g_{t,i}=\triangledown_\Theta J(\Theta_i)$。则Adagrad在每轮训练中对每个参数$θ_i$的学习率进行更新，参数更新公式如下:   
      $Θt+1,i​=Θt,i​−Gt,ii​+ϵ​α​⋅gt,i​$


<!--stackedit_data:
eyJoaXN0b3J5IjpbLTEyNDA1NTE3NzUsMTI0NzMxNDEzMSwtOD
YwNTAxNDY0LC0yOTQ3MDM4MzgsMjMyMjk2Mjg5XX0=
-->