### **神经网络：**

神经网络技术起源于上世纪五、六十年代，当时叫感知机（perceptron），拥有输入层、输出层和（即但隐藏层的bp神经网络）。上世纪八十年代，发明的多层感知机。多层感知机，顾名思义，就是有多个隐含层的感知机。
 - **感知机**
感知机（perceptron）是二分类的线性分类模型，它的基本结构如图1所示，其输入为实例的特征向量，输出为实例的类别，取+1和-1二值，即使多个输入，一个输出。由输入空间到输出空间的如下函数 f(x)=sign(w·x+b)称为感知机。其中![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190318202610.png)。
![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/%E6%84%9F%E7%9F%A5%E6%9C%BA.png)

	感知机模型选择的是采用随机梯度下降
 - **多层感知机(multi-layer perceptron)/神经网络(neural network)**
	
	多层感知机由感知机推广而来，最主要的特点是有多个神经元层，因此也叫深度神经网络(DNN: Deep Neural Networks)。
	多层感知机的一个重要特点就是多层，我们将第一层称之为输入层，最后一层称之有输出层，中间的层称之为隐层。MLP并没有规定隐层的数量，且对于输出层神经元的个数也没有限制。
![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/11.png)

	1. 相比于感知器，引入了隐层(hidden layer)概念；隐层, 不包括输入层和输出层，在输入层和输出层中间的所有N层神经元就称作隐层！通常输入层不算作神经网络的一部分，对于有一层隐层的神经网络，叫做单隐层神经网络或二层感知机；对于第L个隐层，通常有以下一些特性：

		a) L层的每一个神经元与 L-1 层的每一个神经元的输出相连；

		b) L层的每一个神经元互相没有连接；

	2. 引入了新的非线性激活函数(sigmoid/tanh等)

	3. 反向传播算法(back propagation)

	4. 优化算法(梯度下降，随机梯度下降，mini-batch)-暂不确定

 - **卷积神经网络（CNN）**

	卷积神经网络的层级结构

		• 数据输入层/ Input layer
		• 卷积计算层/ CONV layer
		• ReLU激励层 / ReLU layer
		• 池化层 / Pooling layer
		• 全连接层 / FC layer

	小结：bp神经网络与卷积神经网络区别

### GoogleNet  
#### inception V1
- **网络结构**  
  - **inception结构**  
  ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/3770010-28b5366b692d25d4.png)    
  其中，(a)是原版，(b)是改良版。   
  - **整体网络结构**  
    ![](https://github.com/sfxz035/DL-Learning/raw/master/picture/20181123211341548.png)  
    - 上图结构说明  
      1. 为了避免梯度消失，网络额外增加了2个辅助的softmax用于向前传导梯度（辅助分类器）。辅助分类器是将中间某一层的输出用作分类，并按一个较小的权重（0.3）加到最终分类结果中，这样相当于做了模型融合，同时给网络增加了反向传播的梯度信号，也提供了额外的正则化，对于整个网络的训练很有裨益。而在实际测试的时候，这两个额外的softmax会被去掉，也就是这两个辅助分类器只有在训练时用到。  
      2. GoogLeNet采用了模块化的结构（Inception结构），方便增添和修改；  
      3. 网络最后采用了average pooling（平均池化）来代替全连接层，最后阶段的full connection layer不是必须的，global average pooling可代替FC层减少over fitting risk，事实证明这样可以将准确率提高0.6%。但是，实际在最后还是加了一个全连接层，主要是为了方便对输出进行灵活调整  
      4. 虽然移除了全连接，但是网络中依然使用了Dropout ;
- **特点分析**  
  - **思想**  
    - 该结构将CNN中常用的卷积（1x1，3x3，5x5）、池化操作（3x3）堆叠在一起（卷积、池化后的尺寸相同，将通道相加），一方面增加了网络的宽度，另一方面也增加了网络对尺度的适应性。
    - 5x5的滤波器也能够覆盖大部分接受层的的输入。还可以进行一个池化操作，以减少空间大小，降低过度拟合。在这些层之上，在**每一个卷积层后都要做一个ReLU操作**，以增加网络的非线性特征。
    - 原始版本，所有的卷积核都在上一层的所有输出上来做，而那个5x5的卷积核所需的参数量就太大了，造成了特征图的厚度很大，为了避免这种情况，在3x3前、5x5前、max pooling后分别加上了1x1的卷积核，以起到了降低特征图厚度的作用，由此形成了改良版(b)。
  - **1x1的卷积核有什么用呢？**  
    1x1卷积的主要目的是为了减少维度，还用于修正线性激活（ReLU）  

 - 参考：[https://blog.csdn.net/u010712012/article/details/84404457](https://blog.csdn.net/u010712012/article/details/84404457)
#### inception V2  
- **网络结构**    
  用2个连续的3x3卷积层(stride=1)组成的小网络来代替单个的5x5卷积层，保持感受野范围的同时又减少了参数量
  - **inception结构**   
    ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/4038437-94fd5890b77f8c8a.png)   
    
    3x3替代5x5感受野示意图如下：   
    ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/4038437-ebc6be0e3ce65f9a.png)
  - **改进**  
    1. 大尺寸的卷积核可以带来更大的感受野，也意味着更多的参数，比如5x5卷积核参数是3x3卷积核的25/9=2.78倍。作者借鉴VGG提出可以**用2个连续的3x3卷积层(stride=1)组成的小网络来代替单个的5x5卷积层**，两个3x3的卷积核可以达到5x5的卷积核的感受野效果，而且还减少了参数，增加了非线性。这便是Inception V2结构。
    2. 提出**加入了batchnorm**。
  - **3x3卷积之后还要再加激活吗？** 作者也做了对比试验，表明添加非线性激活会提高性能。
  - 参考：[https://blog.csdn.net/loveliuzz/article/details/79135583]  
#### inception V3  
- **网络结构**    
   使用非对称卷积。将nxn的卷积分解成1xn和nx1卷积的串联，例如n=3，分解后就能节省33%的计算量（图１右侧）。作者通过测试发现非对称卷积用在中度大小的feature map上使用效果才会更好（特别是feature map的大小在12x12~20x20之间时）  
   减轻参数，加速运算，拆分成1xn和nx1卷积的串联，同时增加一层非线性激活层提高表达能力。    
   - 第二级inception  
        ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/20180521125938914.png)   
   - 第三级inception  
   ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/20180521150445129.png)  
   使 Inception 模块变得更宽。这种类型等同于前面展示的模块。高维表示更容易处理，更有利于训练  
 - **特点分析**  
	 - 非对称卷积  
	   论文指出，这种非对称卷积结构拆分，比对称的地拆分为几个相同的小卷积效果更好，可以处理更丰富更多的空间特征，增加特征多样性。
- **pool**  
	- **要防止出现特征描述的瓶颈（representational bottleneck）。**所谓特征描述的瓶颈就是中间某层对特征在空间维度进行较大比例的压缩（比如使用pooling时），导致很多特征丢失。  
	  **为了避免representation瓶颈，在应用maximum或者average pooling之前需要将activation的维度进行增加**（可以通过对pool前的特征图**进行通道数加倍，来抵消尺寸上pool而造成的信息损失**，但是会造成计算量加大）。  
	  ![](https://github.com/sfxz035/DL-Learning/raw/master/picture/20180521143822325.png)   
	  左图缺点：带来了一个representation瓶颈；  右图缺点：计算量增加三倍  
	  ![](https://github.com/sfxz035/DL-Learning/raw/master/picture/20180521143856160.png)   
	  作者新提出的降低feature map的size的方法：用一个并行（conv + pooling两个path的stride都为2）来实现。它很cheap并且避免了representation瓶颈（左图为操作角度上，右图为网络角度上）。
	- 
### SE block
 - 思想  
   通过控制scale的大小来控制特征通道中的特征，把重要的特征增强，不重要的特征减弱，从而让提取的特征指向性更强。   
   - 卷积通常被看做是在局部感受野上，将空间（spatial）的信息和特征维度上（channel-wise）的信息进行聚合的信息聚合体。最近很多工作被提出来从**空间维度**层面来提升网络的性能，如Inception结构中嵌入了多尺度信息，聚合多种不同感受野上的特征来获得性能增益；在Inside-Outside网络中考虑了空间中的上下文信息；还有将Attention机制引入到空间维度上等等  
   - SE block是从特征维度层面上来提升网络性能。通过学习的方式来自动获取到每个特征通道的重要程度，然后依照这个重要程度去提升有用的特征并抑制对当前任务用处不大的特征。
- 结构原理    
  - 结构    
  
    ![enter image description here](https://lh3.googleusercontent.com/Fsf5aUu8bAUBWr_qIyYhzf2t-etkeVVNZPy7IKe4Ys52Im9LxEa7JQO-iaGGveN8hBvYhL2mM7DY)   
    
    给定一个输入x，其特征通道数为c_1，通过一系列卷积等一般变换后得到一个特征通道数为c_2的特征。  
    接下来通过三个操作来重标定前面得到的特征：  
    1. Squeeze操作  
       - 我们顺着空间维度来进行特征压缩，将每个二维的特征通道变成一个实数，可以得到$C_2$个特征维度。每个实数某种程度上具有全局的感受野，它表征着在特征通道上响应的全局分布，而且使得靠近输入的层也可以获得全局的感受野。  
       - 具体操作：对U做一个Global Average Pooling操作，输出的1x1xC数据。  
    2. Excitation操作    
       - 通过参数来为每个特征通道生成权重，其中参数 被学习用来显式地建模特征通道间的相关性。     
       - 具体操作：对pooling输出的1x1xC数据再经过两级全连接，最后用sigmoid限制到[0，1]的范围  
    3. Reweight操作  
      - 将Excitation的输出的权重看做是进过特征选择后的每个特征通道的重要性，然后通过乘法逐通道加权到先前的特征上，完成在通道维度上的对原始特征的重标定。   
      - 具体操作：把得到的scale值乘到U的C个通道上，作为下一次输入。  
   - 实例结构   
     ![enter image description here](https://lh3.googleusercontent.com/J4SqbkZYWKmMg4zzW8_UgCcu98Ke2NjTUNy7HOm1a7Um7QpQxFPQmQdK3z_QCCXyoH7J5oBx548A)    
     - 上左图是将SE模块嵌入到Inception结构的一个示例。方框旁边的维度信息代表该层的输出。这里我们使用global average pooling作为Squeeze操作。紧接着两个Fully Connected 层组成一个Bottleneck结构去建模通道间的相关性，并输出和输入特征同样数目的权重。首先将特征维度降低到输入的1/16，然后经过ReLu激活后再通过一个Fully Connected 层升回到原来的维度。这样做比直接用一个Fully Connected层的好处在于：1）具有更多的非线性，可以更好地拟合通道间复杂的相关性；2）极大地减少了参数量和计算量。然后通过一个Sigmoid的门获得0~1之间归一化的权重，最后通过一个Scale的操作来将归一化后的权重加权到每个通道的特征上。  
     - 上右图是将SE嵌入到 ResNet模块中的一个例子，操作过程基本和SE-Inception一样，只不过是在Addition前对分支上Residual的特征进行了特征重标定。如果对Addition后主支上的特征进行重标定，由于在主干上存在0~1的scale操作，在网络较深BP优化时就会在靠近输入层容易出现梯度消散的情况，导致模型难以优化。   
 - **为什么Non-local和SENet有效**   
    卷积也能拆分成空间依赖、通道依赖、特征融合三个步骤，Non-local和SENet有效主要是因为context modeling，卷积只能对局部区域进行context modeling，导致感受野受限制，而Non-local和SENet实际上是对整个输入feature进行context modeling，感受野可以覆盖到整个输入feature上，这对于网络来说是一个有益的语义信息补充。另外，网络仅仅通过卷积堆叠来提取特征，其实可以认为是用同一个形式的函数来拟合输入，导致网络提取特征缺乏多样性，而Non-local和SENet正好增加了提取特征的多样性，弥补了多样性的不足。   
### GCNet
### **ResNet网络**

结构：

优点：

为什么有效：

### **UNet网络**

结构：

优点：

为什么有效：


**Min-batch:**



<!--stackedit_data:
eyJoaXN0b3J5IjpbLTU4MDgwMDIwNCwtMjE0NTY0MjUxMSwyMT
I5NTQxMjk5LDMyODg5NDM3OSwxOTUwODY1ODIzLC0xNzU4MDYw
NTcyLDYwNTIzMzE2NSwxNzQ3NTQwNTgzLC01NDM4NjY2NTIsLT
E4MDQ4NjM1OTYsNzY4NzEwNjUxLC0xNTE2Mzk4MzIzLC0xOTU2
OTM4ODg2LC01MzQ5MTE5NjYsMTA0NDY1MTIzNCwyMzU5Mjk2Mj
gsLTQ1MjU4NDg2MiwtNDEyOTI1NzU3LC0xMzk5NzExNTMyLDg0
MzA5NDc2NV19
-->