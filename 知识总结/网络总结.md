**神经网络：**

神经网络技术起源于上世纪五、六十年代，当时叫感知机（perceptron），拥有输入层、输出层和（即但隐藏层的bp神经网络）。上世纪八十年代，发明的多层感知机。多层感知机，顾名思义，就是有多个隐含层的感知机。
 - **感知机**
感知机（perceptron）是二分类的线性分类模型，它的基本结构如图1所示，其输入为实例的特征向量，输出为实例的类别，取+1和-1二值，即使多个输入，一个输出。由输入空间到输出空间的如下函数 f(x)=sign(w·x+b)称为感知机。其中![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190318202610.png)。
![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/%E6%84%9F%E7%9F%A5%E6%9C%BA.png)

	感知机模型选择的是采用随机梯度下降
 - **多层感知机(multi-layer perceptron)/神经网络(neural network)**
	
	多层感知机由感知机推广而来，最主要的特点是有多个神经元层，因此也叫深度神经网络(DNN: Deep Neural Networks)。
	多层感知机的一个重要特点就是多层，我们将第一层称之为输入层，最后一层称之有输出层，中间的层称之为隐层。MLP并没有规定隐层的数量，且对于输出层神经元的个数也没有限制。
![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/11.png)

	1. 相比于感知器，引入了隐层(hidden layer)概念；隐层, 不包括输入层和输出层，在输入层和输出层中间的所有N层神经元就称作隐层！通常输入层不算作神经网络的一部分，对于有一层隐层的神经网络，叫做单隐层神经网络或二层感知机；对于第L个隐层，通常有以下一些特性：

		a) L层的每一个神经元与 L-1 层的每一个神经元的输出相连；

		b) L层的每一个神经元互相没有连接；

	2. 引入了新的非线性激活函数(sigmoid/tanh等)

	3. 反向传播算法(back propagation)

	4. 优化算法(梯度下降，随机梯度下降，mini-batch)-暂不确定

 - **卷积神经网络（CNN）**

	卷积神经网络的层级结构

		• 数据输入层/ Input layer
		• 卷积计算层/ CONV layer
		• ReLU激励层 / ReLU layer
		• 池化层 / Pooling layer
		• 全连接层 / FC layer

	小结：bp神经网络与卷积神经网络区别

### GoogleNet  
#### inception V1
- **网络结构**  
  - **inception结构**  
  ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/3770010-28b5366b692d25d4.png)    
  其中，(a)是原版，(b)是改良版。   
  - **整体网络结构**  
    ![](https://github.com/sfxz035/DL-Learning/raw/master/picture/20181123211341548.png)  
    - 上图结构说明  
      1. 为了避免梯度消失，网络额外增加了2个辅助的softmax用于向前传导梯度（辅助分类器）。辅助分类器是将中间某一层的输出用作分类，并按一个较小的权重（0.3）加到最终分类结果中，这样相当于做了模型融合，同时给网络增加了反向传播的梯度信号，也提供了额外的正则化，对于整个网络的训练很有裨益。而在实际测试的时候，这两个额外的softmax会被去掉，也就是这两个辅助分类器只有在训练时用到。  
      2. GoogLeNet采用了模块化的结构（Inception结构），方便增添和修改；  
      3. 网络最后采用了average pooling（平均池化）来代替全连接层，该想法来自NIN（Network in Network），事实证明这样可以将准确率提高0.6%。但是，实际在最后还是加了一个全连接层，主要是为了方便对输出进行灵活调整  
      4. 虽然移除了全连接，但是网络中依然使用了Dropout ;
- **特点分析**  
  - 思想  
    - 该结构将CNN中常用的卷积（1x1，3x3，5x5）、池化操作（3x3）堆叠在一起（卷积、池化后的尺寸相同，将通道相加），一方面增加了网络的宽度，另一方面也增加了网络对尺度的适应性。
    - 5x5的滤波器也能够覆盖大部分接受层的的输入。还可以进行一个池化操作，以减少空间大小，降低过度拟合。在这些层之上，在**每一个卷积层后都要做一个ReLU操作**，以增加网络的非线性特征。
    - 原始版本，所有的卷积核都在上一层的所有输出上来做，而那个5x5的卷积核所需的计算量就太大了，造成了特征图的厚度很大，为了避免这种情况，在3x3前、5x5前、max pooling后分别加上了1x1的卷积核，以起到了降低特征图厚度的作用，由此形成了改良版(b)。
  - **1x1的卷积核有什么用呢？**  
    1x1卷积的主要目的是为了减少维度，还用于修正线性激活（ReLU）

**ResNet网络**

结构：

优点：

为什么有效：

**UNet网络**

结构：

优点：

为什么有效：

**Min-batch:**

**SGD,ADAM区别**
<!--stackedit_data:
eyJoaXN0b3J5IjpbMTg4OTM0NjQwMywtMTI3NDA0NzE3MiwxMT
M5NjM1NzgzLC0xNDY0NjEwMjEyLC0xMjQ2OTkwNDUsLTE3ODQ1
MjAyNTYsLTE3MDkzMjgxMzQsLTQ5NDE1OTEyNSwtMTUwMjM5ND
Q0NF19
-->