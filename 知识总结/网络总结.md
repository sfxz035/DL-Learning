### **神经网络：**
![enter image description here](https://lh3.googleusercontent.com/ZtYzWIyANBz1I7ndcTe_9Pmkp0RFDqQDlSwnXIt3aNTbdKjvGatgfIZmBHjBQ1ERsLHJ2_6G1YJT)
神经网络技术起源于上世纪五、六十年代，当时叫感知机（perceptron），拥有输入层、输出层和（即但隐藏层的bp神经网络）。上世纪八十年代，发明的多层感知机。多层感知机，顾名思义，就是有多个隐含层的感知机。
 - **感知机**
感知机（perceptron）是二分类的线性分类模型，它的基本结构如图1所示，其输入为实例的特征向量，输出为实例的类别，取+1和-1二值，即使多个输入，一个输出。由输入空间到输出空间的如下函数 f(x)=sign(w·x+b)称为感知机。其中![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190318202610.png)。
![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/%E6%84%9F%E7%9F%A5%E6%9C%BA.png)

	感知机模型选择的是采用随机梯度下降
 - **多层感知机(multi-layer perceptron)/神经网络(neural network)**
	
	多层感知机由感知机推广而来，最主要的特点是有多个神经元层，因此也叫深度神经网络(DNN: Deep Neural Networks)。
	多层感知机的一个重要特点就是多层，我们将第一层称之为输入层，最后一层称之有输出层，中间的层称之为隐层。MLP并没有规定隐层的数量，且对于输出层神经元的个数也没有限制。
![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/11.png)

	1. 相比于感知器，引入了隐层(hidden layer)概念；隐层, 不包括输入层和输出层，在输入层和输出层中间的所有N层神经元就称作隐层！通常输入层不算作神经网络的一部分，对于有一层隐层的神经网络，叫做单隐层神经网络或二层感知机；对于第L个隐层，通常有以下一些特性：

		a) L层的每一个神经元与 L-1 层的每一个神经元的输出相连；

		b) L层的每一个神经元互相没有连接；

	2. 引入了新的非线性激活函数(sigmoid/tanh等)

	3. 反向传播算法(back propagation)

	4. 优化算法(梯度下降，随机梯度下降，mini-batch)-暂不确定

  **卷积神经网络（CNN）**

- 卷积神经网络的层级结构

	- 数据输入层/ Input layer
	- 卷积计算层/ CONV layer
	- ReLU激励层 / ReLU layer
	- 池化层 / Pooling layer
	- 全连接层 / FC layer

- **bp神经网络与卷积神经网络区别**   
  DNN的输入是向量形式，并未考虑到平面的结构信息，在图像和NLP领域这一结构信息尤为重要   
  CNN的卷积有重叠操作，对于边缘部分处理的更平滑

- **Relu 对比sigmoid主要变化：**   
  1. 单侧抑制  
  2. 相对宽阔的兴奋边界  
  3. 稀疏激活性     
-  **CNN为什么适用于图像**   
	CNN模型限制参数了个数并挖掘了局部结构
- **小结：**CNN与传统SVM等机器学习算法相比，减轻了使用传统算法SVM时必须的大量重复繁琐的数据预处理工作。CNN对缩放，平移，旋转等畸变具有不变性，有着很强的泛化性。CNN最大的特点在于**权重共享，减少网络参数，防止过拟合。池化层用于压缩数据量，减少过拟合。**
- **CNN的三个优点：**   
  sparse interaction(稀疏的交互)，parameter sharing(参数共享)，equivalent respresentation(等价表示)   


- **卷积神经网络之典型CNN。**   
   - LeNet，这是最早用于数字识别的CNN

	- AlexNet， 2012 ILSVRC比赛远超第2名的CNN，比LeNet更深，用多层小卷积层叠加替换单大卷积层。

	- ZF Net， 2013 ILSVRC比赛冠军

	- GoogLeNet， 2014 ILSVRC比赛冠军

	- VGGNet， 2014 ILSVRC比赛中的模型，图像识别略差于GoogLeNet，但是在很多图像转化学习问题(比如object detection)上效果奇好
### **Alexnet**   
-  **网络结构**   
   ![enter image description here](https://lh3.googleusercontent.com/pFauc7991RbcOu99eQIWN7Ztwi9v2w221QMSFjUNwgkuV_zejNfSxiK_-If7xdS3hpN3G3RIA2ud)    
   ![enter image description here](https://lh3.googleusercontent.com/4LMMHn_VZTfb_pED5dWj4spTmsG_TYpWX5J46KR4V_4OGL9NDMNsEWwQ7BRF-UCcNWjcjNHXe-7Q)    
   1.  AlexNet为8层结构，其中前5层为卷积层，后面3层为全连接层   
   2.  AlexNet在两个GPU上运行   
   3. AlexNet在第2,4,5层均是前一层在自己的GPU内连接，第3层是与前面两层合并后输入（我理解是concat），全连接是2个GPU全连接（即将两个GPU的输入连接后再分别输入连个GPUT）；   
   4. LRN层和池化层pool只在前两层卷积层后有，之后的卷积层不再携有。   
  - **特点分析**    
	  - **Local Response Normalization(局部响应归一化)**   
	    ReLU之后进行标准化     
	    ![enter image description here](https://lh3.googleusercontent.com/dwzpwuoZDiVuXQcUl203AlHVVElV--_CfGj8j-6vXuG9FnXOcQ0BGVQ-3204LFpzv7bhRFUaekKd)
	    a代表在feature map中第i个卷积核(x,y)坐标经过了ReLU激活函数的输出，n表示相邻的几个卷积核。N表示这一层总的卷积核数量。k, n, α和β是hyper-parameters，他们的值是在验证集上实验得到的，其中k = 2，n = 5，α = 0.0001，β = 0.75。   
	    **其实对某个对应值以及其相邻的几个feature map（即通道）上的值进行标准化。**这种归一化操作实现了某种形式的横向抑制。    
	 - **Overlapping Pooling**   
	   一般的池化层因为没有重叠，所以pool_size 和 stride一般是相等的。这里的池化是重叠的，即每个pooling单元可能受到相邻单元的影响。在训练阶段有避免过拟合的作用。   
### VGG   
- **网络结构**   
    ![enter image description here](https://lh3.googleusercontent.com/wfk0uepa3qE9E9yrl29vIFl7cVVahGo18MbmgWBhAEQMKoGX2Yc2zqxTwxjeDalJfN2spwL5rjdc)   
    - **VGG16**    
       ![enter image description here](https://lh3.googleusercontent.com/3JKbaK37JuuxmGCNclSlH9ptgn0B5etKni_sMR-mZDdKgiyUY3j7Q4En62NudFWfunFgHmyrSuJL)      
       
 - 特点   
   VGG16用多个3*3卷积代替Alexnet中的11*11卷积好处是，在达到了相同感受野的前提下减少了参数量，并且有更多的非线性变换（前者可以使用三次ReLU激活函数，而后者只有一次），学习能力更强。   
   1*1卷积的意义主要在于线性变换，然后通过relu非线性处理，增强非线性表达能力。而输入通道数和输出通道数不变，没有发生降维。
### GoogleNet  
#### inception V1
- **网络结构**  
  - **inception结构**  
  ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/3770010-28b5366b692d25d4.png)    
  其中，(a)是原版，(b)是改良版。   
  - **整体网络结构**  
    ![](https://github.com/sfxz035/DL-Learning/raw/master/picture/20181123211341548.png)  
    - 上图结构说明  
      1. 为了避免梯度消失，网络额外增加了2个辅助的softmax用于向前传导梯度（辅助分类器）。辅助分类器是将中间某一层的输出用作分类，并按一个较小的权重（0.3）加到最终分类结果中，这样相当于做了模型融合，同时给网络增加了反向传播的梯度信号，也提供了额外的正则化，对于整个网络的训练很有裨益。而在实际测试的时候，这两个额外的softmax会被去掉，也就是这两个辅助分类器只有在训练时用到。  
      2. GoogLeNet采用了模块化的结构（Inception结构），方便增添和修改；  
      3. 网络最后采用了average pooling（平均池化）来代替全连接层，最后阶段的full connection layer不是必须的，global average pooling可代替FC层减少over fitting risk，事实证明这样可以将准确率提高0.6%。但是，实际在最后还是加了一个全连接层，主要是为了方便对输出进行灵活调整  
      4. 虽然移除了全连接，但是网络中依然使用了Dropout ;    
     
- **特点分析**  
  - **思想**  
    - 该结构将CNN中常用的卷积（1x1，3x3，5x5）、池化操作（3x3）堆叠在一起（卷积、池化后的尺寸相同，将通道相加），一方面增加了网络的宽度，另一方面也增加了网络对尺度的适应性。
    - 5x5的滤波器也能够覆盖大部分接受层的的输入。还可以进行一个池化操作，以减少空间大小，降低过度拟合。在这些层之上，在**每一个卷积层后都要做一个ReLU操作**，以增加网络的非线性特征。
    - 原始版本，所有的卷积核都在上一层的所有输出上来做，而那个5x5的卷积核所需的参数量就太大了，造成了特征图的厚度很大，为了避免这种情况，在3x3前、5x5前、max pooling后分别加上了1x1的卷积核，以起到了降低特征图厚度的作用，由此形成了改良版(b)。
  - **1x1的卷积核有什么用呢？**  
    1x1卷积的主要目的是为了减少维度，还用于修正线性激活（ReLU）  

 - 参考：[https://blog.csdn.net/u010712012/article/details/84404457](https://blog.csdn.net/u010712012/article/details/84404457)
#### inception V2  
- **网络结构**    
  用2个连续的3x3卷积层(stride=1)组成的小网络来代替单个的5x5卷积层，保持感受野范围的同时又减少了参数量
  - **inception结构**   
    ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/4038437-94fd5890b77f8c8a.png)   
    
    3x3替代5x5感受野示意图如下：   
    ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/4038437-ebc6be0e3ce65f9a.png)
  - **改进**  
    1. 大尺寸的卷积核可以带来更大的感受野，也意味着更多的参数，比如5x5卷积核参数是3x3卷积核的25/9=2.78倍。作者借鉴VGG提出可以**用2个连续的3x3卷积层(stride=1)组成的小网络来代替单个的5x5卷积层**，两个3x3的卷积核可以达到5x5的卷积核的感受野效果，而且还减少了参数，增加了非线性。这便是Inception V2结构。
    2. **提出加入了batchnorm**。
  - **3x3卷积之后还要再加激活吗？** 作者也做了对比试验，表明添加非线性激活会提高性能。
  - 参考：[https://blog.csdn.net/loveliuzz/article/details/79135583]  
#### inception V3  
- **网络结构**    
   使用非对称卷积。将nxn的卷积分解成1xn和nx1卷积的串联，例如n=3，分解后就能节省33%的计算量（图１右侧）。作者通过测试发现非对称卷积用在中度大小的feature map上使用效果才会更好（特别是feature map的大小在12x12~20x20之间时）  
   减轻参数，加速运算，拆分成1xn和nx1卷积的串联，同时增加一层非线性激活层提高表达能力。    
   - 第二级inception  
        ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/20180521125938914.png)   
   - 第三级inception  
   ![enter image description here](https://github.com/sfxz035/DL-Learning/raw/master/picture/20180521150445129.png)  
   使 Inception 模块变得更宽。这种类型等同于前面展示的模块。高维表示更容易处理，更有利于训练  
 - **特点分析**  
	 - 非对称卷积  
	   论文指出，这种非对称卷积结构拆分，比对称的地拆分为几个相同的小卷积效果更好，可以处理更丰富更多的空间特征，增加特征多样性。
- **使用场景**     
   用1个$1\times n$的卷积和1个$n\times 1$的卷积代替1个$n \times n$的卷积，论文给出了使用场景，最好在中间大小的feature map上使用    
   >In practice, we have found that employing this factorization does not work well on early layers, but it gives very good results on medium grid-sizes (On m×m feature maps, where m ranges between 12 and 20). On that level, very good results can be achieved by using 1 × 7 convolutions followed by 7 × 1 convolutions
- **pool**   
	- **要防止出现特征描述的瓶颈（representational bottleneck）**。所谓特征描述的瓶颈就是中间某层对特征在空间维度进行较大比例的压缩（比如使用pooling时），导致很多特征丢失。  
	  **为了避免representation瓶颈，在应用maximum或者average pooling之前需要将activation的维度进行增加**（可以通过对pool前的特征图**进行通道数加倍，来抵消尺寸上pool而造成的信息损失**，但是会造成计算量加大）。  
	  ![](https://github.com/sfxz035/DL-Learning/raw/master/picture/20180521143822325.png)   
	  左图缺点：带来了一个representation瓶颈；  右图缺点：计算量增加三倍  
	  ![](https://github.com/sfxz035/DL-Learning/raw/master/picture/20180521143856160.png)   
	  作者新提出的降低feature map的size的方法：用一个并行（conv + pooling两个path的stride都为2）来实现。它很cheap并且避免了representation瓶颈（左图为操作角度上，右图为网络角度上）。
	- 
### SE block
 - 思想  
   通过控制scale的大小来控制特征通道中的特征，把重要的特征增强，不重要的特征减弱，从而让提取的特征指向性更强。   
   - 卷积通常被看做是在局部感受野上，将空间（spatial）的信息和特征维度上（channel-wise）的信息进行聚合的信息聚合体。最近很多工作被提出来从**空间维度**层面来提升网络的性能，如Inception结构中嵌入了多尺度信息，聚合多种不同感受野上的特征来获得性能增益；在Inside-Outside网络中考虑了空间中的上下文信息；还有将Attention机制引入到空间维度上等等  
   - SE block是从特征维度层面上来提升网络性能。通过学习的方式来自动获取到每个特征通道的重要程度，然后依照这个重要程度去提升有用的特征并抑制对当前任务用处不大的特征。
- 结构原理    
  - 结构    
  
    ![enter image description here](https://lh3.googleusercontent.com/Fsf5aUu8bAUBWr_qIyYhzf2t-etkeVVNZPy7IKe4Ys52Im9LxEa7JQO-iaGGveN8hBvYhL2mM7DY)   
    
    给定一个输入x，其特征通道数为c_1，通过一系列卷积等一般变换后得到一个特征通道数为c_2的特征。  
    接下来通过三个操作来重标定前面得到的特征：  
    1. Squeeze操作  
       - 我们顺着空间维度来进行特征压缩，将每个二维的特征通道变成一个实数，可以得到$C_2$个特征维度。每个实数某种程度上具有全局的感受野，它表征着在特征通道上响应的全局分布，而且使得靠近输入的层也可以获得全局的感受野。  
       - 具体操作：对U做一个Global Average Pooling操作，输出的1x1xC数据。  
    2. Excitation操作    
       - 通过参数来为每个特征通道生成权重，其中参数 被学习用来显式地建模特征通道间的相关性。     
       - 具体操作：对pooling输出的1x1xC数据再经过两级全连接，最后用sigmoid限制到[0，1]的范围  
    3. Reweight操作  
      - 将Excitation的输出的权重看做是进过特征选择后的每个特征通道的重要性，然后通过乘法逐通道加权到先前的特征上，完成在通道维度上的对原始特征的重标定。   
      - 具体操作：把得到的scale值乘到U的C个通道上，作为下一次输入。  
   - 实例结构   
     ![enter image description here](https://lh3.googleusercontent.com/J4SqbkZYWKmMg4zzW8_UgCcu98Ke2NjTUNy7HOm1a7Um7QpQxFPQmQdK3z_QCCXyoH7J5oBx548A)     
     - 上左图是将SE模块嵌入到Inception结构的一个示例。方框旁边的维度信息代表该层的输出。这里我们使用global average pooling作为Squeeze操作。紧接着两个Fully Connected 层组成一个Bottleneck结构去建模通道间的相关性，并输出和输入特征同样数目的权重。首先将特征维度降低到输入的1/16，然后经过ReLu激活后再通过一个Fully Connected 层升回到原来的维度。这样做比直接用一个Fully Connected层的好处在于：1）具有更多的非线性，可以更好地拟合通道间复杂的相关性；2）极大地减少了参数量和计算量。然后通过一个Sigmoid的门获得0~1之间归一化的权重，最后通过一个Scale的操作来将归一化后的权重加权到每个通道的特征上。  
     - 上右图是将SE嵌入到 ResNet模块中的一个例子，操作过程基本和SE-Inception一样，只不过是在Addition前对分支上Residual的特征进行了特征重标定。如果对Addition后主支上的特征进行重标定，由于在主干上存在0~1的scale操作，在网络较深BP优化时就会在靠近输入层容易出现梯度消散的情况，导致模型难以优化。   
 - **为什么Non-local和SENet有效**   
    卷积也能拆分成空间依赖、通道依赖、特征融合三个步骤，Non-local和SENet有效主要是因为context  modeling，卷积只能对局部区域进行context modeling，导致感受野受限制，而Non-local和SENet实际上是对整个输入feature进行context modeling，感受野可以覆盖到整个输入feature上，这对于网络来说是一个有益的语义信息补充。另外，网络仅仅通过卷积堆叠来提取特征，其实可以认为是用同一个形式的函数来拟合输入，导致网络提取特征缺乏多样性，而Non-local和SENet正好增加了提取特征的多样性，弥补了多样性的不足。   
### NonLocal Net   
- **主要思想**
	文章主要受到NL-Means在图像去噪应用中的启发，考虑所有的特征点来进行加权计算，克服了CNN网络过于关注局部特征的缺点。      
	
	NLnet的网络结构采用的是自注意力机制来建模像素对关系。利用像素对之间的关系提取某处特征时利用其周围点的信息，这个“周围”既可以是时间维度的，也可以是空间维度的。从而通过其他位置聚集的信息来增强当前位置的特征。

	卷积层的堆叠可以增大感受野，但是如果看特定层的卷积核在原图上的感受野，它毕竟是有限的，而且不断重复局部计算有几个限制。首先，计算效率低下。其次，会产生一些优化问题，需要仔细解决。这是local运算不能避免的。然而有些任务，它们可能需要原图上更多的信息，比如attention。如果在某些层能够引入全局的信息，就能很好地解决local操作无法看清全局的情况，为后面的层带去更丰富的信息。  
- **主要内容**    
    - **借鉴非局部平均去噪**   
      处理这些全局动作信息，文章借鉴NL-Means中利用整幅图去噪的思想。前面讲到 NL-Means利用了整幅图像来进行去噪，以图像块为单位在图像中寻找相似区域，再对这些区域求平均，它的滤波过程可以用下面公式来表示：    
    ![enter image description here](https://lh3.googleusercontent.com/2AqNyRatBWIv-_4nhlWXYJgpbDWYlXgLwzgGckggPhNVVEZCKSfwv61NddH9JdhVJr0kR20KjcvI)    
    在这个公式中，w(x,y)是一个权重，表示在原始图像中，像素 x和像素 y 的相似度。这个权重要大于0，同时，权重的和为1。  
   - **本文方法**      
      - 理论
	      类似的，该文章定义了一个用于处理当前位置点与全局所有信息关系的函数：   
	      
	      $$y_i=\frac{1}{C(x)}\sum_{\forall j}f(x_i,x_j)g(x_j)$$   
	       
	      上面的公式中，输入是x，输出是y，i和j分别代表输入的某个空间位置，i代表时间空间上的输出位置索引，j代表全图中所有可能位置的枚举索引，$x_i$是一个向量，维数跟x的channel数一样，f是一个计算任意两点相似关系的函数，即相当于计算位置i和j的权重，g是一个映射函数，将一个点映射成一个向量，可以看成是计算一个点的特征。   
	      
	      文中给出了具体的几种$f(x_i,x_j)$函数的实现形式：   
	      ![enter image description here](https://lh3.googleusercontent.com/yeEG1j08rqCiZ1CM5A_y1a1y22nDwGhhta_-oQqpLLoVY50-uv3ytU6BpSL_sRNyj__rrYfYhf9G)    
      - 具体实现   
        文章中还定义了Non-local Block，也就是把前面的这种Non-local操作封装起来作为一个模块可以很方便的用在现有的框架中。  
        
        $$z_i=W_zy_i+x_i$$   
        
        >“+x_i”表示残差连接。这个残差连接使得我们可以将这个Non-local Block很方便的插入已有的预训练模型中，而不会破坏模型原有的操作。     
        
        ![enter image description here](https://lh3.googleusercontent.com/Ib3Wwy3sVBQWsCJhBkdhKOPE9HovjlyDO6UyApg3AT3_cqVpidNA6Apuo7nIVIoyZ_twv8wN-H1V)   
            
        softmax及之前的分支部分实现的就是f()操作，可以看出前面介绍的Embedded Gaussian就是该部分的完整操作（因为softmax操作会有一个分母，也就是C(x)，该部分中的softmax操作就完成了f()函数的幂计算和除以C(x)计算）。Gaussian就是对应截图中去掉θ和φ的结果；Dot product对应截图中将softmax换成1/N。    
        
- **使用非局部运算有几大好处**：（a）与递归和卷积运算的渐进的操作相比，非本局部运算直接通过计算任意两个位置之间的交互来获取长时记忆，可以不用管其间的距离；（b）正如他们在实验中所显示的那样，非局部运算效率很高，即使只有几层（比如实验中的5层）也能达到最好的效果；（c）最后，他们的非局部运算能够维持可变输入的大小，并且能很方便地与其他运算（比如实验中使用的卷积运算）相组合。   
- **和全连接的联系**   
  我们知道，non-local block利用两个点的相似性对每个位置的特征做加权，而全连接层则是利用position-related的weight对每个位置做加权。于是，全连接层可以看成non-local block的一个特例：    
    - 任意两点的相似性仅跟两点的位置有关，而与两点的具体坐标无关，即$f(x_i,x_j)=w_{i,j}$   
    - g是identity函数，$g(x_i)=x_i$   
    - 归一化系数为1。归一化系数跟输入无关，全连接层不能处理任意尺寸的输入。
- **跟Self-attention的联系**    
   与《Attention Is All You Need》这篇文章的联系。  
   这部分在原文中也提到了。Embedding的1*1卷积操作可以看成矩阵乘法：  
   
   $$\theta(x_i)=W_\theta \cdot x_i,\phi(x_j)=W_\phi \cdot x_j \Rightarrow \theta(x)=W_\theta \cdot x，\phi(x)=W_\phi \cdot x $$   
   
   于是：   
   $y=softmax(x^T \cdot W_\theta^T \cdot W_\theta \cdot x)g(x)$
### NonLocal 和 SENet     
SENet用全局上下文对不同通道进行权值重标定，来调整通道依赖。然而，采用权值重标定的特征融合，不能充分利用全局上下文。
 
通过严格的实验分析，作者发现non-local network的全局上下文在不同位置几乎是相同的，这表明学习到了无位置依赖的全局上下文，造成了大量的计算浪费。**虽然non-local block想要计算出每一个位置特定的全局上下文，但是经过训练之后，全局上下文是不受位置依赖的。**(所以其实只用计算某一个位置的关于其他位置的相似关系即可)    
>这里的位置指的就是像素点的位置，即不同像素点的全局上下文计算结果是大致相同的
### GCNet    
**GCNet，既能够像NLNet一样有效的对全局上下文建模，又能够像SENet一样轻量。**     
- **NLNet**     
  - **Revisiting the Non-local Block**    
  ![enter image description here](https://lh3.googleusercontent.com/dYI-9q2r1zruKDtRmXnYjem_-FSj75eRN0MDJPz8EEWAwOQHzoFjYcNdrHoSQkMGqcua0R8Yfr-6)
  non-local block想要计算出每一个位置特定的全局上下文，但是经过训练之后，全局上下文是不受位置依赖的。即每一个位置所计算出来的attention map几乎是相同的，作者通过分析不同位置全局上下文的距离，进一步证明了这一点。  
  所以只用计算某一个位置的全局上下文信息即可，就可将non-local block简化为Simplifying the Non-local Block  
   
  - **Simplifying the Non-local Block**    
    ![enter image description here](https://lh3.googleusercontent.com/XCwkB_BFygTSldAIyG5GoshZaMAEg1vYzF4TjPjzxeOUQ7w3Oss2hsvd5Xz1cxV8od5UTKePC6YN)    
    通过计算一个位置的全局的attention map来简化non-local block，并且对所有位置共享这个全局attention。 map
### CCNet   

### **ResNet网络**

结构：

优点：

为什么有效：

### **UNet网络**

- 结构：   
   ![enter image description here](https://lh3.googleusercontent.com/IEAjoDFvhrXtW8xKX_KdvoV5d3CBywHO4bPUfVc8nnfTqMp-vyJ4TlGD-Tn4ZNx-Thaet5V-t4mT)     
   - 具体分析   
     U-Net的特点是:1.完全对称，2.采用了skip-connection操作，3.用了一个比较经典的思路，就是编码解码结构。   
     >码和解码（encoder-decoder），早在2006年就被Hinton大神提出来发表在了nature上，当时这个结构提出的主要作用并不是分割，而是压缩图像和去噪声。后来用到了图像分割中，该结构不仅简介而且非常好用。   
     
- 优点：   
  - 一个收缩路径(contracting path)来获取上下文信息以及一个对称的扩张路径(expanding path)用以精确定位    
  - skip-conection它考虑到了来自多个层的特性。这样在获得空间上下文的信息同时也可以更好的获得良好的定位。     
  - 在上采样部分依然有大量的特征通道，这使得网络可以将空间上下文信息向更高的分辨率层传播。
  - 支持少量的数据训练模型    
  - 通过对每个像素点进行分类，获得更高的分割准确率
- 总结  
  Unet利用了下采样的收缩路径扩大感受野从而获得更好的空间上下文信息，在上采样的过程中存在着大量的特征通道，从而保证空间上下文信息可以更好的传递到更高分辨率层传播，同时skip-connection的上采样级联，保证了更好的精确定位。skip-connection较好的保留了图像的底层信息特征，上采样级联，较好的融合了浅层特征和深层特征。  
	> 在图像生成类应用中，skip可以更好的保留图像底层信息特征。 较好的保留的边界，角点等高频的内容，U-Net对提升细节的效果非常明显。

- 为什么有效（特别是医学图像）：   
  - 因为医学图像边界模糊、梯度复杂，需要较多的高分辨率信息。高分辨率用于精准分割。  
  - 人体内部结构相对固定，分割目标在人体图像中的分布很具有规律，语义简单明确，低分辨率信息能够提供这一信息，用于目标物体的识别。     

   UNet结合了**低分辨率信息**（提供物体类别识别依据）和**高分辨率信息**（提供精准分割定位依据），完美适用于医学图像分割。

unet的下采样过程，是从高分辨率（浅层特征）到低分辨率（深层特征）的过程。
unet的特点就是通过上采样过程中的级联，使得浅层特征和深层特征结合起来。对于医学图像来说，unet能用深层特征用于定位，浅层特征用于精确分割，这就是为什么unet常见于很多图像分割任务。其实即便不是医学图像，unet在平常图像的分割效果也有不错的表现。




<!--stackedit_data:
eyJoaXN0b3J5IjpbLTEyNTQwNDkwMzgsMjQ2MjYzNzY1LDkzMj
k4NzIyNCwtMTQ1Njg1MTM5NSw2ODExNjUzNTgsLTEzMzI0MzI5
NDMsLTE2Nzk1OTM5MzEsMjEwNDg1MDM5NCwtMzQ2MTU4ODM0LD
IyNTM5MjkyLC0xMjA4ODAxNjc2LDgyNDA2ODI3MiwtMTAwMzU3
NDUyMSwxNzA5Njc3NDMzLDQ3NzIzNjQ4LC0xOTg2NTc2NTY3LC
0xNzkxNzY0NzI0LDk1NjE2NzU0Myw5NzYxNTYzMzUsMTY1NDUw
MTE3NF19
-->